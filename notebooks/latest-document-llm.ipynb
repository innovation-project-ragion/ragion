{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b435988-49bf-43a0-8313-3c0ce9f8b13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# section1 Import required libraries\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "import logging\n",
    "from typing import List, Dict, Any\n",
    "from docx import Document\n",
    "from tqdm.notebook import tqdm\n",
    "from pymilvus import connections, Collection, utility, CollectionSchema, FieldSchema, DataType\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def verify_section(section_number: int, verification_func) -> bool:\n",
    "    \"\"\"Verify if a section was executed successfully.\"\"\"\n",
    "    try:\n",
    "        result = verification_func()\n",
    "        print_status(f\"Section {section_number} Verification\", True, \"Successfully executed\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print_status(f\"Section {section_number} Verification\", False, f\"Error: {str(e)}\")\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2286f7b6-c817-4136-a027-6f1a4b5d908c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCESS | Test\n",
      "  └─ Test message\n",
      "\n",
      "✅ SUCCESS | Section Status Function Verification\n",
      "  └─ Successfully executed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## section 2 \n",
    "def print_status(section_name: str, status: bool, message: str = \"\"):\n",
    "    \"\"\"Print status of a section with colored output.\"\"\"\n",
    "    status_str = \"✅ SUCCESS\" if status else \"❌ FAILED\"\n",
    "    print(f\"\\n{status_str} | {section_name}\")\n",
    "    if message:\n",
    "        print(f\"  └─ {message}\")\n",
    "# Add at the end of section 2\n",
    "def verify_section2():\n",
    "    # Test the print_status function\n",
    "    print_status(\"Test\", True, \"Test message\")\n",
    "    return True\n",
    "\n",
    "verify_section(\"Status Function\", verify_section2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "045a0853-562a-4aea-a61b-f823dcbe3c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCESS | Embedding Dimension\n",
      "  └─ Using dimension: 768\n"
     ]
    }
   ],
   "source": [
    "def get_model_embedding_dim(model_name: str = \"TurkuNLP/bert-base-finnish-cased-v1\") -> int:\n",
    "    \"\"\"Get embedding dimension from model config.\"\"\"\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    return model.config.hidden_size\n",
    "\n",
    "# Get the embedding dimension from the model\n",
    "EMBEDDING_DIM = get_model_embedding_dim()\n",
    "print_status(\"Embedding Dimension\", True, f\"Using dimension: {EMBEDDING_DIM}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf312fb4-af02-4358-a41e-c3c0958235c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCESS | CUDA Check\n",
      "  └─ Using GPU: NVIDIA GeForce RTX 2070 SUPER\n",
      "\n",
      "✅ SUCCESS | Section CUDA Setup Verification\n",
      "  └─ Successfully executed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## section or cell 3\n",
    "def check_cuda():\n",
    "    \"\"\"Check CUDA availability and print status.\"\"\"\n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            device_name = torch.cuda.get_device_name(0)\n",
    "            print_status(\"CUDA Check\", True, f\"Using GPU: {device_name}\")\n",
    "            return True\n",
    "        else:\n",
    "            print_status(\"CUDA Check\", True, \"Using CPU\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print_status(\"CUDA Check\", False, str(e))\n",
    "        return False\n",
    "def verify_section3():\n",
    "    return check_cuda()\n",
    "\n",
    "verify_section(\"CUDA Setup\", verify_section3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca53ff65-91df-4c99-b4e1-d331fff01d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCESS | NLTK Setup\n",
      "  └─ Downloaded finnish stopwords\n",
      "\n",
      "✅ SUCCESS | Section NLTK and Milvus Settings Verification\n",
      "  └─ Successfully executed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## section or cell 4\n",
    "def ensure_stopwords_downloaded(language='finnish'):\n",
    "    \"\"\"Download NLTK stopwords and print status.\"\"\"\n",
    "    try:\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "        print_status(\"NLTK Setup\", True, f\"Downloaded {language} stopwords\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print_status(\"NLTK Setup\", False, str(e))\n",
    "        return False\n",
    "\n",
    "# Milvus Connection Settings\n",
    "MILVUS_HOST = \"milvus-standalone\"\n",
    "MILVUS_PORT = \"19530\"\n",
    "MILVUS_ALIAS = \"default\"\n",
    "# Add at the end of section 4\n",
    "def verify_section4():\n",
    "    result = ensure_stopwords_downloaded()\n",
    "    if not result:\n",
    "        raise Exception(\"Failed to download stopwords\")\n",
    "    if not all([MILVUS_HOST, MILVUS_PORT, MILVUS_ALIAS, EMBEDDING_DIM]):\n",
    "        raise Exception(\"Milvus settings not properly defined\")\n",
    "    return True\n",
    "\n",
    "verify_section(\"NLTK and Milvus Settings\", verify_section4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d9700b9-5631-4c13-9c38-dbf24a12632d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCESS | Document Processor\n",
      "  └─ Initialized successfully\n",
      "\n",
      "✅ SUCCESS | Section DocumentProcessor Verification\n",
      "  └─ Successfully executed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## section or cell 5\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, chunk_size=400, chunk_overlap=50):\n",
    "        try:\n",
    "            self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "                separators=[\"\\n\\n\", \"\\n\", \". \", \", \", \" \"],\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                length_function=len,\n",
    "                keep_separator=True,\n",
    "                add_start_index=True\n",
    "            )\n",
    "            print_status(\"Document Processor\", True, \"Initialized successfully\")\n",
    "        except Exception as e:\n",
    "            print_status(\"Document Processor\", False, str(e))\n",
    "            raise\n",
    "        \n",
    "    def extract_metadata_from_filename(self, filename: str) -> tuple:\n",
    "        \"\"\"Extract metadata from filename.\"\"\"\n",
    "        title = os.path.splitext(filename)[0]\n",
    "        match = re.match(r'([A-Za-z]+)\\s+(\\d{1,3})v\\s+([A-Za-z0-9\\-]+)', title)\n",
    "        if match:\n",
    "            return match.group(1), int(match.group(2)), match.group(3)\n",
    "        return None, None, None\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize Finnish text.\"\"\"\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'[^\\w\\s\\.\\,\\?\\!\\-\\:\\;äöåÄÖÅ]', '', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def process_document(self, file_path: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process a single document and return chunks with metadata.\"\"\"\n",
    "        try:\n",
    "            # Read document\n",
    "            doc = Document(file_path)\n",
    "            text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "            \n",
    "            # Extract metadata\n",
    "            filename = os.path.basename(file_path)\n",
    "            name, age, doc_id = self.extract_metadata_from_filename(filename)\n",
    "            \n",
    "            # Preprocess and split text\n",
    "            clean_text = self.preprocess_text(text)\n",
    "            chunks = self.text_splitter.split_text(clean_text)\n",
    "            \n",
    "            # Create chunks with metadata\n",
    "            processed_chunks = []\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                processed_chunks.append({\n",
    "                    \"text\": chunk,\n",
    "                    \"metadata\": {\n",
    "                        \"source\": filename,\n",
    "                        \"person_name\": name,\n",
    "                        \"person_age\": age,\n",
    "                        \"document_id\": doc_id,\n",
    "                        \"chunk_index\": i\n",
    "                    }\n",
    "                })\n",
    "            \n",
    "            print_status(\"Document Processing\", True, f\"Processed {filename}\")\n",
    "            return processed_chunks\n",
    "        except Exception as e:\n",
    "            print_status(\"Document Processing\", False, f\"Error processing {file_path}: {str(e)}\")\n",
    "            raise\n",
    "# Add at the end of section 5\n",
    "def verify_section5():\n",
    "    processor = DocumentProcessor()\n",
    "    # Test basic functionality\n",
    "    test_text = \"Test document content\"\n",
    "    processed = processor.preprocess_text(test_text)\n",
    "    if not processed:\n",
    "        raise Exception(\"Document processor initialization failed\")\n",
    "    return True\n",
    "\n",
    "verify_section(\"DocumentProcessor\", verify_section5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c5ac1a4-e2bb-4fd4-af80-6f94b1eaea20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCESS | Milvus Connection\n",
      "  └─ Cleaned up existing connection\n",
      "\n",
      "✅ SUCCESS | Milvus Connection\n",
      "  └─ Connected to milvus-standalone:19530\n",
      "\n",
      "✅ SUCCESS | Milvus Collection\n",
      "  └─ Dropped existing collection: test_collection\n",
      "\n",
      "✅ SUCCESS | Index Creation\n",
      "  └─ Created IVF_FLAT index\n",
      "\n",
      "✅ SUCCESS | Collection Load\n",
      "  └─ Loaded collection into memory\n",
      "\n",
      "✅ SUCCESS | Milvus Collection\n",
      "  └─ Created new collection: test_collection with dim=768\n",
      "\n",
      "✅ SUCCESS | Collection Inspection\n",
      "  └─ Successfully inspected collection with 0 records\n",
      "\n",
      "Collection Schema:\n",
      "{'auto_id': True, 'description': 'Document embeddings collection', 'fields': [{'name': 'id', 'description': '', 'type': <DataType.INT64: 5>, 'is_primary': True, 'auto_id': True}, {'name': 'text', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 65535}}, {'name': 'embedding', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 768}}, {'name': 'person_name', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 100}}, {'name': 'person_age', 'description': '', 'type': <DataType.INT64: 5>}, {'name': 'document_id', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 100}}, {'name': 'chunk_index', 'description': '', 'type': <DataType.INT64: 5>}], 'enable_dynamic_field': False}\n",
      "\n",
      "Sample Records:\n",
      "\n",
      "Collection Statistics:\n",
      "Total entities: 0\n",
      "\n",
      "Index Information:\n",
      "Index Type: {'metric_type': 'IP', 'index_type': 'IVF_FLAT', 'params': {'nlist': 1024}}\n",
      "\n",
      "✅ SUCCESS | Test Cleanup\n",
      "  └─ Dropped test collection: test_collection\n",
      "\n",
      "✅ SUCCESS | MilvusManager Verification\n",
      "  └─ All tests passed\n",
      "\n",
      "✅ SUCCESS | Section MilvusManager Verification\n",
      "  └─ Successfully executed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## section or cell 6\n",
    "class MilvusManager:\n",
    "    def __init__(self, host: str = \"milvus-standalone\", port: str = \"19530\", alias: str = \"default\"):\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.alias = alias\n",
    "        self.connected = False\n",
    "        self.connect()\n",
    "        \n",
    "    def connect(self):\n",
    "        \"\"\"Establish connection to Milvus.\"\"\"\n",
    "        try:\n",
    "            # First, check if there's an existing connection and remove it\n",
    "            try:\n",
    "                connections.remove_connection(alias=self.alias)\n",
    "                print_status(\"Milvus Connection\", True, \"Cleaned up existing connection\")\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Attempt to establish new connection\n",
    "            connections.connect(\n",
    "                alias=self.alias,\n",
    "                host=self.host,\n",
    "                port=self.port,\n",
    "                timeout=10.0  # Add timeout parameter\n",
    "            )\n",
    "            \n",
    "            # Verify connection is working\n",
    "            try:\n",
    "                utility.get_server_version()\n",
    "                self.connected = True\n",
    "                print_status(\"Milvus Connection\", True, f\"Connected to {self.host}:{self.port}\")\n",
    "            except Exception as ve:\n",
    "                raise Exception(f\"Connection verification failed: {str(ve)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.connected = False\n",
    "            print_status(\"Milvus Connection\", False, str(e))\n",
    "            raise\n",
    "        \n",
    "\n",
    "    def create_collection(self, collection_name: str = \"document_embeddings\"):\n",
    "        \"\"\"Create Milvus collection with appropriate schema.\"\"\"\n",
    "        try:\n",
    "            # First, check if collection exists and drop it to ensure correct dimension\n",
    "            if utility.has_collection(collection_name):\n",
    "                Collection(name=collection_name).drop()\n",
    "                print_status(\"Milvus Collection\", True, f\"Dropped existing collection: {collection_name}\")\n",
    "                \n",
    "            fields = [\n",
    "                FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "                FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "                FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=EMBEDDING_DIM),\n",
    "                FieldSchema(name=\"person_name\", dtype=DataType.VARCHAR, max_length=100),\n",
    "                FieldSchema(name=\"person_age\", dtype=DataType.INT64),\n",
    "                FieldSchema(name=\"document_id\", dtype=DataType.VARCHAR, max_length=100),\n",
    "                FieldSchema(name=\"chunk_index\", dtype=DataType.INT64)\n",
    "            ]\n",
    "            \n",
    "            schema = CollectionSchema(\n",
    "                fields=fields,\n",
    "                description=\"Document embeddings collection\",\n",
    "                enable_dynamic_field=False\n",
    "            )\n",
    "            collection = Collection(name=collection_name, schema=schema)\n",
    "            \n",
    "            # Create index and load collection\n",
    "            self.create_and_load_index(collection)\n",
    "            \n",
    "            print_status(\"Milvus Collection\", True, f\"Created new collection: {collection_name} with dim={EMBEDDING_DIM}\")\n",
    "            return collection\n",
    "        except Exception as e:\n",
    "            print_status(\"Milvus Collection\", False, str(e))\n",
    "            raise\n",
    "\n",
    "    def create_and_load_index(self, collection):\n",
    "        \"\"\"Create index and load collection into memory.\"\"\"\n",
    "        try:\n",
    "            # Create IVF_FLAT index\n",
    "            index_params = {\n",
    "                \"metric_type\": \"IP\",\n",
    "                \"index_type\": \"IVF_FLAT\",\n",
    "                \"params\": {\"nlist\": 1024}\n",
    "            }\n",
    "            collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "            print_status(\"Index Creation\", True, \"Created IVF_FLAT index\")\n",
    "            \n",
    "            # Load collection into memory\n",
    "            collection.load()\n",
    "            print_status(\"Collection Load\", True, \"Loaded collection into memory\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print_status(\"Index Creation\", False, str(e))\n",
    "            raise\n",
    "\n",
    "    def reload_collection(self, collection_name: str = \"document_embeddings\"):\n",
    "        \"\"\"Reload collection into memory if it exists.\"\"\"\n",
    "        try:\n",
    "            if utility.has_collection(collection_name):\n",
    "                collection = Collection(name=collection_name)\n",
    "                collection.load()\n",
    "                print_status(\"Collection Reload\", True, f\"Reloaded collection: {collection_name}\")\n",
    "                return collection\n",
    "            else:\n",
    "                raise Exception(f\"Collection {collection_name} does not exist\")\n",
    "        except Exception as e:\n",
    "            print_status(\"Collection Reload\", False, str(e))\n",
    "            raise\n",
    "    def get_collection_info(self, collection_name: str = \"document_embeddings\"):\n",
    "        \"\"\"Get detailed collection information as a dictionary.\"\"\"\n",
    "        try:\n",
    "            collection = Collection(collection_name)\n",
    "            \n",
    "            # Get collection stats\n",
    "            stats = {\n",
    "                \"name\": collection_name,\n",
    "                \"schema\": collection.schema,\n",
    "                \"row_count\": collection.num_entities,\n",
    "            }\n",
    "            \n",
    "            # Get index information\n",
    "            try:\n",
    "                index_info = collection.index().params if collection.has_index() else None\n",
    "                stats[\"index_info\"] = index_info\n",
    "            except Exception as e:\n",
    "                stats[\"index_info\"] = None\n",
    "                print(f\"Error getting index info: {e}\")\n",
    "                \n",
    "            # Check load state using utility function instead\n",
    "            try:\n",
    "                load_status = utility.load_state(collection_name)\n",
    "                stats[\"loaded\"] = str(load_status) == \"Loaded\"\n",
    "            except Exception as e:\n",
    "                stats[\"loaded\"] = None\n",
    "                print(f\"Error getting load state: {e}\")\n",
    "                \n",
    "            return stats\n",
    "        except Exception as e:\n",
    "            print_status(\"Get Collection Info\", False, str(e))\n",
    "            return None\n",
    "\n",
    "    def inspect_collection(self, collection_name: str = \"document_embeddings\"):\n",
    "        \"\"\"Inspect collection contents and statistics.\"\"\"\n",
    "        try:\n",
    "            if not utility.has_collection(collection_name):\n",
    "                raise Exception(f\"Collection {collection_name} does not exist\")\n",
    "                    \n",
    "            # Get collection\n",
    "            collection = Collection(collection_name)\n",
    "            \n",
    "            # Print schema\n",
    "            print(\"\\nCollection Schema:\")\n",
    "            print(collection.schema)\n",
    "            \n",
    "            # Get sample records using search instead of query\n",
    "            try:\n",
    "                # Make sure collection is loaded\n",
    "                collection.load()\n",
    "                \n",
    "                print(\"\\nSample Records:\")\n",
    "                # Search with empty vector to get random samples\n",
    "                results = collection.search(\n",
    "                    data=[[0]*EMBEDDING_DIM],  # dummy vector\n",
    "                    anns_field=\"embedding\",\n",
    "                    param={\"metric_type\": \"IP\", \"params\": {\"nprobe\": 10}},\n",
    "                    limit=5,\n",
    "                    output_fields=[\"text\", \"person_name\", \"document_id\", \"chunk_index\"]\n",
    "                )\n",
    "                \n",
    "                if results and results[0]:\n",
    "                    for i, hit in enumerate(results[0]):\n",
    "                        print(f\"\\nRecord {i+1}:\")\n",
    "                        print(f\"Text: {hit.entity.get('text')[:200]}...\")\n",
    "                        print(f\"Person: {hit.entity.get('person_name')}\")\n",
    "                        print(f\"Document: {hit.entity.get('document_id')}\")\n",
    "                        print(f\"Chunk Index: {hit.entity.get('chunk_index')}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error getting sample records: {e}\")\n",
    "            \n",
    "            # Get collection statistics\n",
    "            try:\n",
    "                num_entities = collection.num_entities\n",
    "                print(\"\\nCollection Statistics:\")\n",
    "                print(f\"Total entities: {num_entities}\")\n",
    "                \n",
    "                # Get index information\n",
    "                if collection.has_index():\n",
    "                    index_info = collection.index().params\n",
    "                    print(\"\\nIndex Information:\")\n",
    "                    print(f\"Index Type: {index_info}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error getting statistics: {e}\")\n",
    "            \n",
    "            return collection\n",
    "                \n",
    "        except Exception as e:\n",
    "            print_status(\"Collection Inspection\", False, str(e))\n",
    "            return None\n",
    "\n",
    "# Add at the end of section 6 - Enhanced verification\n",
    "def verify_section6():\n",
    "    try:\n",
    "        collection_name = \"test_collection\"  # Use consistent collection name\n",
    "        \n",
    "        # Test basic connection\n",
    "        manager = MilvusManager()\n",
    "        if not manager.connected:\n",
    "            raise Exception(\"Failed to connect to Milvus\")\n",
    "            \n",
    "        # Test collection creation\n",
    "        collection = manager.create_collection(collection_name)\n",
    "        if collection is None:\n",
    "            raise Exception(\"Failed to create test collection\")\n",
    "        \n",
    "        # Test collection inspection\n",
    "        collection_info = manager.get_collection_info(collection_name)\n",
    "        if collection_info:\n",
    "            print_status(\"Collection Inspection\", True, \n",
    "                        f\"Successfully inspected collection with {collection_info['row_count']} records\")\n",
    "            \n",
    "        # Optional: Print detailed inspection\n",
    "        manager.inspect_collection(collection_name)\n",
    "        \n",
    "        # Clean up - optionally drop the test collection\n",
    "        if utility.has_collection(collection_name):\n",
    "            Collection(name=collection_name).drop()\n",
    "            print_status(\"Test Cleanup\", True, f\"Dropped test collection: {collection_name}\")\n",
    "        \n",
    "        print_status(\"MilvusManager Verification\", True, \"All tests passed\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print_status(\"MilvusManager Verification\", False, str(e))\n",
    "        raise Exception(f\"MilvusManager verification failed: {str(e)}\")\n",
    "\n",
    "# Run verification\n",
    "verify_section(\"MilvusManager\", verify_section6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b505974f-ea81-4124-81ec-18c7f950a133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c635161bb4fb439188812d60fa4fb436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/56.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b6a78c27704c7a919eea0564b30930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/424k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba1fd67cb0b4b079bf7ce19bac0620a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/816k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCESS | Embedding Model\n",
      "  └─ Loaded TurkuNLP/bert-base-finnish-cased-v1 (dim=768)\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 768\n",
      "\n",
      "✅ SUCCESS | Embedding Verification\n",
      "  └─ Generated embeddings with shape (1, 768), dimension=768\n",
      "\n",
      "✅ SUCCESS | Section EmbeddingGenerator Verification\n",
      "  └─ Successfully executed\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingGenerator:\n",
    "    def __init__(self, model_name: str = \"TurkuNLP/bert-base-finnish-cased-v1\"):\n",
    "        try:\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "            # Get the embedding dimension from the model config\n",
    "            self.embedding_dim = self.model.config.hidden_size\n",
    "            print_status(\"Embedding Model\", True, f\"Loaded {model_name} (dim={self.embedding_dim})\")\n",
    "        except Exception as e:\n",
    "            print_status(\"Embedding Model\", False, str(e))\n",
    "            raise\n",
    "        \n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        \n",
    "    def generate(self, texts: List[str], batch_size: int = 32) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings for a list of texts.\"\"\"\n",
    "        try:\n",
    "            all_embeddings = []\n",
    "            \n",
    "            # Process in batches\n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                batch_texts = texts[i:i + batch_size]\n",
    "                \n",
    "                # Tokenize texts\n",
    "                encoded_input = self.tokenizer(\n",
    "                    batch_texts,\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=512,\n",
    "                    return_tensors='pt'\n",
    "                ).to(self.device)\n",
    "                \n",
    "                # Compute token embeddings\n",
    "                with torch.no_grad():\n",
    "                    model_output = self.model(**encoded_input)\n",
    "                \n",
    "                # Perform pooling\n",
    "                sentence_embeddings = self.mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "                \n",
    "                # Normalize embeddings\n",
    "                sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "                \n",
    "                all_embeddings.append(sentence_embeddings.cpu().numpy())\n",
    "            \n",
    "            result = np.concatenate(all_embeddings)\n",
    "            \n",
    "            # Fixed indentation here\n",
    "            if result.shape[1] != EMBEDDING_DIM:\n",
    "                raise ValueError(f\"Embedding dimension mismatch. Expected {EMBEDDING_DIM}, got {result.shape[1]}\")\n",
    "            \n",
    "            print_status(\"Embedding Generation\", True, \n",
    "                    f\"Generated {len(texts)} embeddings with dimension {result.shape[1]}\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print_status(\"Embedding Generation\", False, str(e))\n",
    "            raise\n",
    "\n",
    "    def get_embedding_dim(self) -> int:\n",
    "        \"\"\"Return the embedding dimension.\"\"\"\n",
    "        return self.embedding_dim\n",
    "\n",
    "# Add at the end of section 7\n",
    "def verify_section7():\n",
    "    try:\n",
    "        # Initialize the generator\n",
    "        generator = EmbeddingGenerator()\n",
    "        \n",
    "        # Get the embedding dimension\n",
    "        embedding_dim = generator.get_embedding_dim()\n",
    "        \n",
    "        # Test with a simple text\n",
    "        test_text = [\"Test sentence for embedding generation\"]\n",
    "        embeddings = generator.generate(test_text)\n",
    "        \n",
    "        # Verify embedding shape\n",
    "        if embeddings is None:\n",
    "            raise Exception(\"Embeddings are None\")\n",
    "            \n",
    "        if len(embeddings.shape) != 2:\n",
    "            raise Exception(f\"Expected 2D array, got shape {embeddings.shape}\")\n",
    "            \n",
    "        if embeddings.shape[0] != len(test_text):\n",
    "            raise Exception(f\"Expected {len(test_text)} embeddings, got {embeddings.shape[0]}\")\n",
    "            \n",
    "        if embeddings.shape[1] != embedding_dim:\n",
    "            raise Exception(f\"Expected dimension {embedding_dim}, got {embeddings.shape[1]}\")\n",
    "        \n",
    "        # Verify embedding values\n",
    "        if not np.all(np.isfinite(embeddings)):\n",
    "            raise Exception(\"Embeddings contain invalid values\")\n",
    "            \n",
    "        if not np.allclose(np.linalg.norm(embeddings, axis=1), 1.0, atol=1e-6):\n",
    "            raise Exception(\"Embeddings are not properly normalized\")\n",
    "        \n",
    "        print_status(\"Embedding Verification\", True, \n",
    "                    f\"Generated embeddings with shape {embeddings.shape}, dimension={embedding_dim}\")\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print_status(\"Embedding Verification\", False, str(e))\n",
    "        raise Exception(f\"EmbeddingGenerator verification failed: {str(e)}\")\n",
    "\n",
    "# Run verification\n",
    "verify_section(\"EmbeddingGenerator\", verify_section7)\n",
    "\n",
    "# You can also add a specific test function\n",
    "def test_embedding_generation():\n",
    "    try:\n",
    "        generator = EmbeddingGenerator()\n",
    "        test_texts = [\n",
    "            \"Tämä on testilause.\",\n",
    "            \"Toinen testilause suomeksi.\"\n",
    "        ]\n",
    "        embeddings = generator.generate(test_texts)\n",
    "        print(f\"Generated embeddings shape: {embeddings.shape}\")\n",
    "        print(f\"Embedding dimension: {generator.get_embedding_dim()}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Test failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Run test if needed\n",
    "# test_embedding_generation()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b4a337a-6d17-41df-ba2e-234530064064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2859415056c748a8ae9e6f649401dbab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07e6b81a9984e9fae6879b6d9fe8c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff387a86e8c3442abc1f7f5e499bc8ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247b5145d6a145acb8502fcfe249f47b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7bdec65d7fe4cfe910b79b8c0f0e484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca2802fdc124c19b1263879f41e2595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.04G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b14f52dc5e54b498ad29f499ce3b8bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a56aa6ace64e2299a3b9958298eb80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ecdf3c0262a47fca49f184a4000cb69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.89k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6cfdfa2b7ad4865b0d007d16c4bbfdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "053e6b72ae044a5280447666f7f692be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/554 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCESS | LLM Pipeline\n",
      "  └─ Pipeline configured successfully\n",
      "\n",
      "✅ SUCCESS | LLM Setup\n",
      "  └─ Loaded Finnish-NLP/llama-7b-finnish-instruct-v0.2\n",
      "\n",
      "✅ SUCCESS | Document Processor\n",
      "  └─ Initialized successfully\n",
      "\n",
      "✅ SUCCESS | Embedding Model\n",
      "  └─ Loaded TurkuNLP/bert-base-finnish-cased-v1 (dim=768)\n",
      "\n",
      "✅ SUCCESS | Milvus Connection\n",
      "  └─ Cleaned up existing connection\n",
      "\n",
      "✅ SUCCESS | Milvus Connection\n",
      "  └─ Connected to milvus-standalone:19530\n",
      "\n",
      "✅ SUCCESS | Milvus Collection\n",
      "  └─ Dropped existing collection: document_embeddings\n",
      "\n",
      "✅ SUCCESS | Index Creation\n",
      "  └─ Created IVF_FLAT index\n",
      "\n",
      "✅ SUCCESS | Collection Load\n",
      "  └─ Loaded collection into memory\n",
      "\n",
      "✅ SUCCESS | Milvus Collection\n",
      "  └─ Created new collection: document_embeddings with dim=768\n",
      "\n",
      "✅ SUCCESS | RAG Pipeline\n",
      "  └─ All components initialized\n",
      "\n",
      "✅ SUCCESS | Section RAGPipeline Verification\n",
      "  └─ Successfully executed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## section or cell 8\n",
    "class RAGPipeline:\n",
    "    def __init__(self, model_id: str = \"Finnish-NLP/llama-7b-finnish-instruct-v0.2\"):\n",
    "        try:\n",
    "            self.setup_llm(model_id)\n",
    "            print_status(\"LLM Setup\", True, f\"Loaded {model_id}\")\n",
    "            \n",
    "            self.doc_processor = DocumentProcessor()\n",
    "            self.embedding_generator = EmbeddingGenerator()\n",
    "            \n",
    "            # Verify embedding dimension matches\n",
    "            if self.embedding_generator.get_embedding_dim() != EMBEDDING_DIM:\n",
    "                raise ValueError(f\"Embedding dimension mismatch. Global: {EMBEDDING_DIM}, \" \n",
    "                               f\"Generator: {self.embedding_generator.get_embedding_dim()}\")\n",
    "            \n",
    "            self.milvus_manager = MilvusManager(\n",
    "                host=MILVUS_HOST,\n",
    "                port=MILVUS_PORT,\n",
    "                alias=MILVUS_ALIAS\n",
    "            )\n",
    "            self.collection = self.milvus_manager.create_collection()\n",
    "            print_status(\"RAG Pipeline\", True, \"All components initialized\")\n",
    "        except Exception as e:\n",
    "            print_status(\"RAG Pipeline\", False, str(e))\n",
    "            raise\n",
    "        \n",
    "    def setup_llm(self, model_id: str):\n",
    "        \"\"\"Initialize the LLM with optimized settings.\"\"\"\n",
    "        try:\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\"\n",
    "            )\n",
    "            \n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                quantization_config=bnb_config,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                max_memory={0: \"6GiB\"},\n",
    "                offload_folder=\"offload\"\n",
    "            )\n",
    "            \n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "            self.pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                max_new_tokens=512,\n",
    "                do_sample=True,\n",
    "                temperature=0.3,\n",
    "                top_p=0.95,\n",
    "                repetition_penalty=1.15\n",
    "            )\n",
    "            print_status(\"LLM Pipeline\", True, \"Pipeline configured successfully\")\n",
    "        except Exception as e:\n",
    "            print_status(\"LLM Pipeline\", False, str(e))\n",
    "            raise\n",
    "    def process_documents(self, folder_path: str):\n",
    "        \"\"\"Process all documents in the specified folder.\"\"\"\n",
    "        try:\n",
    "            file_paths = [f for f in os.listdir(folder_path) if f.endswith('.docx')]\n",
    "            all_chunks = []\n",
    "            \n",
    "            for file in tqdm(file_paths, desc=\"Processing documents\"):\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                chunks = self.doc_processor.process_document(file_path)\n",
    "                all_chunks.extend(chunks)\n",
    "                \n",
    "            # Generate embeddings\n",
    "            texts = [chunk[\"text\"] for chunk in all_chunks]\n",
    "            embeddings = self.embedding_generator.generate(texts)\n",
    "            \n",
    "            # Prepare data for insertion - convert to list of dictionaries\n",
    "            entities = []\n",
    "            for i, (text, embedding, chunk) in enumerate(zip(texts, embeddings, all_chunks)):\n",
    "                entity = {\n",
    "                    \"text\": text,  # Single string, not a list\n",
    "                    \"embedding\": embedding.tolist(),  # Single embedding vector\n",
    "                    \"person_name\": chunk[\"metadata\"][\"person_name\"],\n",
    "                    \"person_age\": chunk[\"metadata\"][\"person_age\"],\n",
    "                    \"document_id\": chunk[\"metadata\"][\"document_id\"],\n",
    "                    \"chunk_index\": chunk[\"metadata\"][\"chunk_index\"]\n",
    "                }\n",
    "                entities.append(entity)\n",
    "            \n",
    "            # Insert entities one by one or in small batches\n",
    "            batch_size = 100\n",
    "            for i in range(0, len(entities), batch_size):\n",
    "                batch = entities[i:i + batch_size]\n",
    "                self.collection.insert(batch)\n",
    "                \n",
    "            self.collection.flush()\n",
    "            self.milvus_manager.create_and_load_index(self.collection)\n",
    "            print_status(\"Document Processing\", True, f\"Inserted {len(texts)} chunks into Milvus\")\n",
    "        except Exception as e:\n",
    "            print_status(\"Document Processing\", False, str(e))\n",
    "            raise   \n",
    "        \n",
    "    def query(self, question: str, top_k: int = 3):\n",
    "        \"\"\"Query the system with a question.\"\"\"\n",
    "        try:\n",
    "            # Ensure collection is loaded\n",
    "            self.collection = self.milvus_manager.reload_collection()\n",
    "            \n",
    "            # Generate question embedding\n",
    "            question_embedding = self.embedding_generator.generate([question])[0]\n",
    "            \n",
    "            # Search in Milvus\n",
    "            search_params = {\"metric_type\": \"IP\", \"params\": {\"nprobe\": 10}}\n",
    "            results = self.collection.search(\n",
    "                data=[question_embedding.tolist()],\n",
    "                anns_field=\"embedding\",\n",
    "                param=search_params,\n",
    "                limit=top_k,\n",
    "                output_fields=[\"text\", \"person_name\", \"document_id\"]\n",
    "            )\n",
    "            \n",
    "            # Format context with clear document separation\n",
    "            context_parts = []\n",
    "            for i, hit in enumerate(results[0]):\n",
    "                text = hit.entity.get('text')\n",
    "                doc_id = hit.entity.get('document_id')\n",
    "                person = hit.entity.get('person_name')\n",
    "                context_parts.append(f\"[Dokumentti {i+1}]\\nID: {doc_id}\\nHenkilö: {person}\\nTeksti: {text}\\n\")\n",
    "                \n",
    "            context = \"\\n\".join(context_parts)\n",
    "            \n",
    "            # Enhanced prompt with strict instruction to return exact quotes\n",
    "            prompt = f\"\"\"Kysymys: {question}\n",
    "    \n",
    "            Konteksti:\n",
    "            {context}\n",
    "            \n",
    "            Ohje: Etsi suora vastaus annetusta kontekstista. Käytä tarkkoja lainauksia.\n",
    "            \n",
    "            Jos löydät suoran vastauksen:\n",
    "            1. Kerro ensin mistä dokumentista löysit vastauksen\n",
    "            2. Lainaa tekstiä sanatarkasti\n",
    "            \n",
    "            Jos et löydä vastausta:\n",
    "            - Vastaa vain: \"En löydä suoraa vastausta annetusta kontekstista\"\n",
    "            \n",
    "            Vastaus:\"\"\"\n",
    "    \n",
    "            response = self.pipeline(prompt)[0][\"generated_text\"].split(\"Vastaus:\")[-1].strip()\n",
    "            \n",
    "            # Clean up any extra content after the answer\n",
    "            if \"*END*\" in response:\n",
    "                response = response.split(\"*END*\")[0].strip()\n",
    "                \n",
    "            if \"Ohje:\" in response:\n",
    "                response = response.split(\"Ohje:\")[0].strip()\n",
    "                \n",
    "            print_status(\"Query\", True, \"Generated response successfully\")\n",
    "            return {\n",
    "                \"answer\": response,\n",
    "                \"sources\": [\n",
    "                    {\n",
    "                        \"text\": hit.entity.get('text'),\n",
    "                        \"person_name\": hit.entity.get('person_name'),\n",
    "                        \"document_id\": hit.entity.get('document_id')\n",
    "                    }\n",
    "                    for hit in results[0]\n",
    "                ]\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print_status(\"Query\", False, str(e))\n",
    "            raise\n",
    "# Add at the end of section 8\n",
    "def verify_section8():\n",
    "    pipeline = RAGPipeline()\n",
    "    if not hasattr(pipeline, 'pipeline') or not pipeline.collection:\n",
    "        raise Exception(\"RAG Pipeline initialization failed\")\n",
    "    return True\n",
    "\n",
    "verify_section(\"RAGPipeline\", verify_section8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39740d04-6e0e-443b-ac74-79269ba073f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCESS | Milvus Connection\n",
      "  └─ Cleaned up existing connection\n",
      "\n",
      "✅ SUCCESS | Milvus Connection\n",
      "  └─ Connected to milvus-standalone:19530\n",
      "Available collections:\n",
      "['document_embeddings']\n",
      "\n",
      "==================================================\n",
      "Inspecting collection: document_embeddings\n",
      "==================================================\n",
      "\n",
      "Collection Info:\n",
      "Name: document_embeddings\n",
      "Row Count: 0\n",
      "Loaded: True\n",
      "Index Info: {'metric_type': 'IP', 'index_type': 'IVF_FLAT', 'params': {'nlist': 1024}}\n",
      "\n",
      "Detailed Inspection:\n",
      "\n",
      "Collection Schema:\n",
      "{'auto_id': True, 'description': 'Document embeddings collection', 'fields': [{'name': 'id', 'description': '', 'type': <DataType.INT64: 5>, 'is_primary': True, 'auto_id': True}, {'name': 'text', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 65535}}, {'name': 'embedding', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 768}}, {'name': 'person_name', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 100}}, {'name': 'person_age', 'description': '', 'type': <DataType.INT64: 5>}, {'name': 'document_id', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 100}}, {'name': 'chunk_index', 'description': '', 'type': <DataType.INT64: 5>}], 'enable_dynamic_field': False}\n",
      "\n",
      "Sample Records:\n",
      "\n",
      "Collection Statistics:\n",
      "Total entities: 0\n",
      "\n",
      "Index Information:\n",
      "Index Type: {'metric_type': 'IP', 'index_type': 'IVF_FLAT', 'params': {'nlist': 1024}}\n"
     ]
    }
   ],
   "source": [
    "# Test cell for data inspection\n",
    "def test_data_inspection():\n",
    "    try:\n",
    "        manager = MilvusManager()\n",
    "        \n",
    "        # List all collections\n",
    "        print(\"Available collections:\")\n",
    "        collections = utility.list_collections()\n",
    "        print(collections)\n",
    "        \n",
    "        if collections:\n",
    "            # Inspect each collection\n",
    "            for collection_name in collections:\n",
    "                print(f\"\\n{'='*50}\")\n",
    "                print(f\"Inspecting collection: {collection_name}\")\n",
    "                print(f\"{'='*50}\")\n",
    "                \n",
    "                # Get collection info\n",
    "                info = manager.get_collection_info(collection_name)\n",
    "                if info:\n",
    "                    print(\"\\nCollection Info:\")\n",
    "                    print(f\"Name: {info['name']}\")\n",
    "                    print(f\"Row Count: {info['row_count']}\")\n",
    "                    print(f\"Loaded: {info['loaded']}\")\n",
    "                    if info['index_info']:\n",
    "                        print(f\"Index Info: {info['index_info']}\")\n",
    "                \n",
    "                # Detailed inspection\n",
    "                print(\"\\nDetailed Inspection:\")\n",
    "                manager.inspect_collection(collection_name)\n",
    "                \n",
    "        else:\n",
    "            print(\"No collections found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during inspection: {e}\")\n",
    "\n",
    "# Run the test\n",
    "test_data_inspection()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31dd20bd-654e-4593-bf1a-bc49c756b0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCESS | Section Main Function Verification\n",
      "  └─ Successfully executed\n",
      "\n",
      "✅ SUCCESS | CUDA Check\n",
      "  └─ Using GPU: NVIDIA GeForce RTX 2070 SUPER\n",
      "\n",
      "✅ SUCCESS | NLTK Setup\n",
      "  └─ Downloaded finnish stopwords\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be9515b392e4a9c9ed4e3d4b9e73953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCESS | LLM Pipeline\n",
      "  └─ Pipeline configured successfully\n",
      "\n",
      "✅ SUCCESS | LLM Setup\n",
      "  └─ Loaded Finnish-NLP/llama-7b-finnish-instruct-v0.2\n",
      "\n",
      "✅ SUCCESS | Document Processor\n",
      "  └─ Initialized successfully\n",
      "\n",
      "✅ SUCCESS | Embedding Model\n",
      "  └─ Loaded TurkuNLP/bert-base-finnish-cased-v1 (dim=768)\n",
      "\n",
      "✅ SUCCESS | Milvus Connection\n",
      "  └─ Cleaned up existing connection\n",
      "\n",
      "✅ SUCCESS | Milvus Connection\n",
      "  └─ Connected to milvus-standalone:19530\n",
      "\n",
      "✅ SUCCESS | Milvus Collection\n",
      "  └─ Dropped existing collection: document_embeddings\n",
      "\n",
      "✅ SUCCESS | Index Creation\n",
      "  └─ Created IVF_FLAT index\n",
      "\n",
      "✅ SUCCESS | Collection Load\n",
      "  └─ Loaded collection into memory\n",
      "\n",
      "✅ SUCCESS | Milvus Collection\n",
      "  └─ Created new collection: document_embeddings with dim=768\n",
      "\n",
      "✅ SUCCESS | RAG Pipeline\n",
      "  └─ All components initialized\n",
      "\n",
      "✅ SUCCESS | Document Path\n",
      "  └─ Using folder: /home/jovyan/work/notebooks/data/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8728b38f1ec34ac8bda0686c6e3b4e7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing documents:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Eila 81v SH-4.docx\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Sulo 75v C5-50.docx\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 8 embeddings with dimension 768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmp5ctf405t\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmp5ctf405t/_remote_module_non_scriptable.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCESS | Index Creation\n",
      "  └─ Created IVF_FLAT index\n",
      "\n",
      "✅ SUCCESS | Collection Load\n",
      "  └─ Loaded collection into memory\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Inserted 8 chunks into Milvus\n",
      "\n",
      "Inspecting processed data:\n",
      "\n",
      "✅ SUCCESS | Milvus Connection\n",
      "  └─ Cleaned up existing connection\n",
      "\n",
      "✅ SUCCESS | Milvus Connection\n",
      "  └─ Connected to milvus-standalone:19530\n",
      "Available collections:\n",
      "['document_embeddings']\n",
      "\n",
      "==================================================\n",
      "Inspecting collection: document_embeddings\n",
      "==================================================\n",
      "\n",
      "Collection Info:\n",
      "Name: document_embeddings\n",
      "Row Count: 8\n",
      "Loaded: True\n",
      "Index Info: {'index_type': 'IVF_FLAT', 'params': {'nlist': 1024}, 'metric_type': 'IP'}\n",
      "\n",
      "Detailed Inspection:\n",
      "\n",
      "Collection Schema:\n",
      "{'auto_id': True, 'description': 'Document embeddings collection', 'fields': [{'name': 'id', 'description': '', 'type': <DataType.INT64: 5>, 'is_primary': True, 'auto_id': True}, {'name': 'text', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 65535}}, {'name': 'embedding', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 768}}, {'name': 'person_name', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 100}}, {'name': 'person_age', 'description': '', 'type': <DataType.INT64: 5>}, {'name': 'document_id', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 100}}, {'name': 'chunk_index', 'description': '', 'type': <DataType.INT64: 5>}], 'enable_dynamic_field': False}\n",
      "\n",
      "Sample Records:\n",
      "\n",
      "Record 1:\n",
      "Text: . Alkoholia en ole käyttänyt muuta kuin suurissa juhlissa juhlan kunniaksi....\n",
      "Person: Eila\n",
      "Document: SH-4\n",
      "Chunk Index: 4\n",
      "\n",
      "Record 2:\n",
      "Text: . Jos ei ystävälle sovi lähteä kanssani ulos ja on tärkeää mennä, lähden yksin. Tärkeitä henkilöitä on ystäväni Marjatta ja tietysti aviomies. Tärkeimmät omaiset. Harrastan milloin mitäkin. Mielelläni...\n",
      "Person: Eila\n",
      "Document: SH-4\n",
      "Chunk Index: 3\n",
      "\n",
      "Record 3:\n",
      "Text: . Olen muutaman vuoden asunut tässä ryhmäkodissa. Olen tyytyväinen kotiini, en ole ajatellut muuttaa. Täältä saan kaiken, mitä tarvitsen. Joskus käyn kävelyllä, rollaattorilla. Silloin kun huvittaa, n...\n",
      "Person: Eila\n",
      "Document: SH-4\n",
      "Chunk Index: 2\n",
      "\n",
      "Record 4:\n",
      "Text: . Marjatta on minun ystäväni. En ole terveydessä huomannut ongelmia, mitä nyt flunssaa on ollut. Musiikista pidän ja kahvista. On mukavaa, kun kysytään kävelylle tai kauppaan mukaan. Raha-asiat huolet...\n",
      "Person: Eila\n",
      "Document: SH-4\n",
      "Chunk Index: 1\n",
      "\n",
      "Record 5:\n",
      "Text: Syntymävuosi: 1942 81v. Sukupuoli: nainen Kansalaisuus: suomi Maakunta: uusimaa Talotyyppi: ryhmäkoti Omannäköinen arki Arjessa teen milloin mitäkin. Arjen kohokohtia ovat tapaamiset omaisten kanssa. ...\n",
      "Person: Eila\n",
      "Document: SH-4\n",
      "Chunk Index: 0\n",
      "\n",
      "Collection Statistics:\n",
      "Total entities: 8\n",
      "\n",
      "Index Information:\n",
      "Index Type: {'index_type': 'IVF_FLAT', 'params': {'nlist': 1024}, 'metric_type': 'IP'}\n",
      "\n",
      "Processing Query 1/3\n",
      "\n",
      "✅ SUCCESS | Collection Reload\n",
      "  └─ Reloaded collection: document_embeddings\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 768\n",
      "\n",
      "✅ SUCCESS | Query\n",
      "  └─ Generated response successfully\n",
      "\n",
      "✅ SUCCESS | Query 1\n",
      "  └─ Question: Onko Marjatta Eilan ystävä?\n",
      "Answer: [Dokumentti 1] ID: SH-4 Henkilö: Eila Teksti: \"Se on helppo käyttää.\" Luottamus: 85% Selitys: Konteksti osoittaa, että Eila käyttää helppokäyttöistä järjestelmää, mikä viittaa siihen, että hän asuu todennäköisesti asumisyksikössä, jossa käytetään helppokäyttöisiä järjestelmiä. [Dokumentti 2] ID: C5-50 Henkilö: Sulo Teksti: \"Se on helppo käyttää.\"  Luottamus: 95% Selitys: Tekstissä mainitaan, että Sulun on helppo käyttää, mikä on osoitus siitä, että hän on käyttänyt järjestelmää aiemmin. [Dokumentti 3] ID: SH-4 Henkilö: Eila Teksti: \"Ystävyys on kaikkein tärkeintä.\" Luottamus: 95% Selitys: Eila on maininnut ystävyyden olevan hänelle tärkeintä elämässä. Tämä viittaa siihen, että hänellä on läheisiä ihmissuhteita ja että hänellä on ystäviä tai perheenjäseniä, jotka ovat hänelle tärkeitä. Muista, että on tärkeää olla tietoinen asiayhteydestä ja etsiä suoria vastauksia annettujen tietojen perusteella. Huomaa, että jos sinulla on muita kysymyksiä, kysy rohkeasti!  Voinko auttaa teitä vielä jossakin muussa asiassa?  Ystävällisin terveisin, [Nimesi] [Sähköpostiosoitteesi] [Puhelinnumerosi] [Tarvittava apu/kysymys] [Päiväys] Pidä hauskaa! [Avulias avustaja]\n",
      "\n",
      "Sources:\n",
      "- SH-4: . Marjatta on minun ystäväni. En ole terveydessä huomannut ongelmia, mitä nyt flunssaa on ollut. Mus...\n",
      "- C5-50: . Se on helppo käyttää. Asuminen Olen asunut tässä asumisyksikössä useamman vuoden. Elämän käännekoh...\n",
      "- SH-4: . Jos ei ystävälle sovi lähteä kanssani ulos ja on tärkeää mennä, lähden yksin. Tärkeitä henkilöitä ...\n",
      "\n",
      "Processing Query 2/3\n",
      "\n",
      "✅ SUCCESS | Collection Reload\n",
      "  └─ Reloaded collection: document_embeddings\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 768\n",
      "\n",
      "✅ SUCCESS | Query\n",
      "  └─ Generated response successfully\n",
      "\n",
      "✅ SUCCESS | Query 2\n",
      "  └─ Question: Miten Sulo kokee sosiaalisen kanssakäymisen merkityksen?\n",
      "Answer: \"En löydä suoraa vastausta annetusta kontekstista.\"  Luottamus: 90%\n",
      "\n",
      "Sources:\n",
      "- C5-50: . Se on helppo käyttää. Asuminen Olen asunut tässä asumisyksikössä useamman vuoden. Elämän käännekoh...\n",
      "- SH-4: . Jos ei ystävälle sovi lähteä kanssani ulos ja on tärkeää mennä, lähden yksin. Tärkeitä henkilöitä ...\n",
      "- C5-50: . Minulla on pieniä vaikeuksia puhua, mutta tykkään jutella muiden asukkaiden kanssa. Ruokailut tapa...\n",
      "\n",
      "Processing Query 3/3\n",
      "\n",
      "✅ SUCCESS | Collection Reload\n",
      "  └─ Reloaded collection: document_embeddings\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 768\n",
      "\n",
      "✅ SUCCESS | Query\n",
      "  └─ Generated response successfully\n",
      "\n",
      "✅ SUCCESS | Query 3\n",
      "  └─ Question: Montako sisarusta Sulolla on?\n",
      "Answer: En löydä suoraa vastausta annetusta kontekstista.  Luottamus: 90%  Selitys: Kontekstissa ei ole suoraa vastausta kysymykseen \"Montako sisarusta Sulolla on?\". Luottamus: 90% Selitys: Konteksti ei anna suoraa vastausta kysymykseen \"Montako sisarusta Sulolla on?\". Luottamus: 90%  Selitys: Kontekstin mukaan Sulolla on 4 sisarusta. Luottamus: 90%  Selitys: Kontekstin mukaan Sulolla on 4 sisarusta. Luottamus: 90%  Selitys: Konteksti on selkeä ja antaa vastauksen kysymykseen.  Luottamus: 90%  Selitys: Konteksti on selkeä ja antaa vastauksen kysymykseen. Luottamus: 90%  Selitys: Konteksti on selkeä ja antaa vastauksen kysymykseen. Luottamus: 90%  Selitys: Konteksti on selkeä ja antaa vastauksen kysymykseen. Luottamus: 90%  Selitys: Konteksti on selkeä ja antaa vastauksen kysymykseen. Luottamus: 90%  Selitys: Konteksti on selkeä ja antaa vastauksen kysymykseen. Luottamus: 90%  Selitys: Konteksti on selkeä ja antaa vastauksen kysymykseen. Luottamus: 90%  Selitys: Konteksti on selkeä ja antaa vastauksen kysymykseen. Luottamus: 90%  Selitys: Konteksti on selkeä ja antaa vastauksen kysymykseen. Luottamus: 90%  Selitys: Konteksti on selkeä ja antaa vastauksen kysymykseen. Luottamus: 90%  Selitys: Konteksti on selkeä ja antaa vastauksen kysymykseen. Luottamus: 90%  Selitys: Konteksti on selkeä ja antaa vastauksen kysymykseen. Luottamus: 90%  Selitys: Konteksti on selkeä ja antaa vastauksen kysymykseen. Luottamus: 90%  Selitys: Konteksti on selkeä ja antaa vastauksen kysymykseen. Luottamus: 90%  Selitys: Konteksti on selkeä ja antaa vastauksen kysymykseen. Luottamus: 90%  Selitys: Konteksti on selkeä ja antaa vastauksen kysymykseen. Luottamus: 90%  Selitys: Konteksti on selkeä ja antaa vastauksen kysymykseen. Luottamus: 90%  Selitys: Konteksti on selkeä ja antaa vastauksen kysymykseen. Luottamus: 90%  Selitys: Konteksti on selkeä ja antaa vastauksen kysymykseen. Luottamus: 90%  Selitys: Konteksti on selkeä ja antaa vastauksen kysymykseen. Luo\n",
      "\n",
      "Sources:\n",
      "- SH-4: . Marjatta on minun ystäväni. En ole terveydessä huomannut ongelmia, mitä nyt flunssaa on ollut. Mus...\n",
      "- C5-50: . Minulla on pieniä vaikeuksia puhua, mutta tykkään jutella muiden asukkaiden kanssa. Ruokailut tapa...\n",
      "- SH-4: Syntymävuosi: 1942 81v. Sukupuoli: nainen Kansalaisuus: suomi Maakunta: uusimaa Talotyyppi: ryhmäkot...\n",
      "\n",
      "✅ SUCCESS | Main Execution\n",
      "  └─ All operations completed successfully\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    try:\n",
    "        # Check CUDA\n",
    "        check_cuda()\n",
    "        \n",
    "        # Download stopwords\n",
    "        ensure_stopwords_downloaded()\n",
    "        \n",
    "        # Initialize the RAG pipeline\n",
    "        rag = RAGPipeline()\n",
    "        \n",
    "        # Process documents\n",
    "        folder_path = '/home/jovyan/work/notebooks/data/'  # Update this path\n",
    "        print_status(\"Document Path\", True, f\"Using folder: {folder_path}\")\n",
    "        rag.process_documents(folder_path)\n",
    "        \n",
    "        # Run data inspection after processing documents\n",
    "        print(\"\\nInspecting processed data:\")\n",
    "        test_data_inspection()\n",
    "        \n",
    "        # Example queries\n",
    "        questions = [\n",
    "            \"Onko Marjatta Eilan ystävä?\",\n",
    "            \"Miten Sulo kokee sosiaalisen kanssakäymisen merkityksen?\",\n",
    "            \"Montako sisarusta Sulolla on?\"\n",
    "        ]\n",
    "        \n",
    "        for i, question in enumerate(questions, 1):\n",
    "            print(f\"\\nProcessing Query {i}/{len(questions)}\")\n",
    "            try:\n",
    "                result = rag.query(question)\n",
    "                print_status(f\"Query {i}\", True, f\"Question: {question}\")\n",
    "                print(f\"Answer: {result['answer']}\")\n",
    "                print(\"\\nSources:\")\n",
    "                for source in result['sources']:\n",
    "                    print(f\"- {source['document_id']}: {source['text'][:100]}...\")\n",
    "            except Exception as e:\n",
    "                print_status(f\"Query {i}\", False, f\"Failed to process question: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        print_status(\"Main Execution\", True, \"All operations completed successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print_status(\"Main Execution\", False, str(e))\n",
    "        raise\n",
    "\n",
    "# Add at the end of section 9\n",
    "def verify_section9():\n",
    "    # Test if main components are accessible\n",
    "    if 'main' not in globals() or not callable(main):\n",
    "        raise Exception(\"Main function not properly defined\")\n",
    "    return True\n",
    "\n",
    "verify_section(\"Main Function\", verify_section9)\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c6cda4-73d4-4c02-9e7c-23ae8cab17ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
