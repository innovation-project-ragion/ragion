{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6076b32b-7828-476a-91df-4dcade6e6559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/opt/conda/lib/python3.11/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCESS | CUDA Check\n",
      "  └─ Using GPU: NVIDIA GeForce RTX 2070 SUPER\n",
      "\n",
      "✅ SUCCESS | NLTK Setup\n",
      "  └─ Downloaded finnish stopwords\n",
      "\n",
      "✅ SUCCESS | Embedding Dimension\n",
      "  └─ Using dimension: 1536\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "import logging\n",
    "from typing import List, Dict, Any\n",
    "from docx import Document\n",
    "from tqdm.notebook import tqdm\n",
    "from pymilvus import connections, Collection, utility, CollectionSchema, FieldSchema, DataType\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "## Env \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "\n",
    "# Utility Functions\n",
    "def print_status(section_name: str, status: bool, message: str = \"\"):\n",
    "    \"\"\"Print status of a section with colored output.\"\"\"\n",
    "    status_str = \"✅ SUCCESS\" if status else \"❌ FAILED\"\n",
    "    print(f\"\\n{status_str} | {section_name}\")\n",
    "    if message:\n",
    "        print(f\"  └─ {message}\")\n",
    "\n",
    "def verify_section(section_number: int, verification_func) -> bool:\n",
    "    \"\"\"Verify if a section was executed successfully.\"\"\"\n",
    "    try:\n",
    "        result = verification_func()\n",
    "        print_status(f\"Section {section_number} Verification\", True, \"Successfully executed\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print_status(f\"Section {section_number} Verification\", False, f\"Error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def get_model_embedding_dim(model_name: str = \"TurkuNLP/bert-base-finnish-cased-v1\") -> int:\n",
    "    \"\"\"Get embedding dimension from model config.\"\"\"\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    return model.config.hidden_size\n",
    "\n",
    "def check_cuda():\n",
    "    \"\"\"Check CUDA availability and print status.\"\"\"\n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            device_name = torch.cuda.get_device_name(0)\n",
    "            print_status(\"CUDA Check\", True, f\"Using GPU: {device_name}\")\n",
    "            return True\n",
    "        else:\n",
    "            print_status(\"CUDA Check\", True, \"Using CPU\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print_status(\"CUDA Check\", False, str(e))\n",
    "        return False\n",
    "\n",
    "def ensure_stopwords_downloaded(language='finnish'):\n",
    "    \"\"\"Download NLTK stopwords and print status.\"\"\"\n",
    "    try:\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "        print_status(\"NLTK Setup\", True, f\"Downloaded {language} stopwords\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print_status(\"NLTK Setup\", False, str(e))\n",
    "        return False\n",
    "\n",
    "# Global Constants\n",
    "#EMBEDDING_DIM = get_model_embedding_dim()\n",
    "EMBEDDING_DIM = 1536\n",
    "MILVUS_HOST = \"milvus-standalone\"\n",
    "MILVUS_PORT = \"19530\"\n",
    "MILVUS_ALIAS = \"default\"\n",
    "\n",
    "# Initial Setup Verification\n",
    "def verify_initial_setup():\n",
    "    check_cuda()\n",
    "    ensure_stopwords_downloaded()\n",
    "    print_status(\"Embedding Dimension\", True, f\"Using dimension: {EMBEDDING_DIM}\")\n",
    "    \n",
    "# Run initial verification\n",
    "verify_initial_setup()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e32ba9a-93cc-4bf9-a81b-2cede41cd7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCESS | Document Processor\n",
      "  └─ Initialized with Finnish-optimized settings\n",
      "\n",
      "✅ SUCCESS | Milvus Connection\n",
      "  └─ Cleaned up existing connection\n",
      "\n",
      "✅ SUCCESS | Milvus Connection\n",
      "  └─ Connected to milvus-standalone:19530\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda8ee9447a74e8bb382156141951bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/218 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1481ee6c90544c818710aab7541bc23a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/6.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b63c8c96a2884b509674cf36864bac10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/96.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40fff89603e8401ab80f83227063efe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/562 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b69c2fcc01fa42aca7122e69a9a7e781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/3.53G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCESS | Embedding Model\n",
      "  └─ Loaded TurkuNLP/gpt3-finnish-large (dim=1536)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e7938cc9144110a4c79524de33725e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5884751739934055beb2209110bbfebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.53G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab70de99589f4963af41acbeabeed26b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34d2dad41d634365ac46a337b29b0eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a99a533feb441d684fed5aab213f0ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7fb6df65b045cfb33dd0a3b66bd851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aadc09f484e4da6a4c3d26830796e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.04G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d6f4ba8c6024dd0be7d7f00bf9c1b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927ee7ff98d24b3693e096c4b519ec2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339b390ce17c40aa86f129bc50fcac19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.89k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31afa5fcdb7742bd8d85b54820966931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3186754a1ab2417d8557826ddb22c7a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/554 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCESS | LLM Pipeline\n",
      "  └─ Pipeline configured successfully\n",
      "\n",
      "✅ SUCCESS | LLM Setup\n",
      "  └─ Loaded Finnish-NLP/llama-7b-finnish-instruct-v0.2\n",
      "\n",
      "✅ SUCCESS | Document Processor\n",
      "  └─ Initialized with Finnish-optimized settings\n",
      "\n",
      "✅ SUCCESS | Embedding Model\n",
      "  └─ Loaded TurkuNLP/gpt3-finnish-large (dim=1536)\n",
      "\n",
      "✅ SUCCESS | Milvus Connection\n",
      "  └─ Cleaned up existing connection\n",
      "\n",
      "✅ SUCCESS | Milvus Connection\n",
      "  └─ Connected to milvus-standalone:19530\n",
      "\n",
      "✅ SUCCESS | Milvus Collection\n",
      "  └─ Dropped existing collection: document_embeddings\n",
      "\n",
      "✅ SUCCESS | Index Creation\n",
      "  └─ Created IVF_FLAT index\n",
      "\n",
      "✅ SUCCESS | Collection Load\n",
      "  └─ Loaded collection into memory\n",
      "\n",
      "✅ SUCCESS | Milvus Collection\n",
      "  └─ Created new collection: document_embeddings with dim=1536\n",
      "\n",
      "✅ SUCCESS | RAG Pipeline\n",
      "  └─ All components initialized\n",
      "\n",
      "✅ SUCCESS | Section Pipeline Components Verification\n",
      "  └─ Successfully executed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Core Pipeline Components\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, chunk_size=400, chunk_overlap=80):\n",
    "        try:\n",
    "            self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "                separators=[\n",
    "                    \"\\n\\n\",  # First split on double newlines\n",
    "                    \"\\n\",    # Then single newlines\n",
    "                    \".\",     # Then sentence endings\n",
    "                    \":\",     # Then colons (common in Finnish formatting)\n",
    "                    \";\",     # Then semicolons\n",
    "                    \",\",     # Then commas\n",
    "                    \" \",     # Finally, split on spaces if needed\n",
    "                    \"\"\n",
    "                ],\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                length_function=len,\n",
    "                keep_separator=True,\n",
    "                add_start_index=True\n",
    "            )\n",
    "            \n",
    "            # Add Finnish-specific cleaning patterns\n",
    "            self.clean_patterns = [\n",
    "                (r'\\s+', ' '),  # Normalize whitespace\n",
    "                (r'[\\(\\{\\[\\]\\}\\)]', ''),  # Remove brackets\n",
    "                (r'[^\\w\\s\\.\\,\\?\\!\\-\\:\\;äöåÄÖÅ]', ''),  # Keep Finnish characters\n",
    "                (r'\\s+\\.', '.'),  # Fix spacing around periods\n",
    "                (r'\\.+', '.'),  # Normalize multiple periods\n",
    "            ]\n",
    "            print_status(\"Document Processor\", True, \"Initialized with Finnish-optimized settings\")\n",
    "        except Exception as e:\n",
    "            print_status(\"Document Processor\", False, str(e))\n",
    "            raise\n",
    "\n",
    "    def extract_metadata_from_filename(self, filename: str) -> tuple:\n",
    "        \"\"\"Extract metadata from filename.\"\"\"\n",
    "        title = os.path.splitext(filename)[0]\n",
    "        match = re.match(r'([A-Za-z]+)\\s+(\\d{1,3})v\\s+([A-Za-z0-9\\-]+)', title)\n",
    "        if match:\n",
    "            return match.group(1), int(match.group(2)), match.group(3)\n",
    "        return None, None, None\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize Finnish text with improved handling.\"\"\"\n",
    "        try:\n",
    "            # Apply all cleaning patterns\n",
    "            for pattern, replacement in self.clean_patterns:\n",
    "                text = re.sub(pattern, replacement, text)\n",
    "            \n",
    "            # Ensure proper sentence boundaries\n",
    "            text = re.sub(r'([.!?])\\s*([A-ZÄÖÅ])', r'\\1\\n\\2', text)\n",
    "            \n",
    "            # Remove extra whitespace while preserving paragraph breaks\n",
    "            text = '\\n'.join(line.strip() for line in text.split('\\n'))\n",
    "            return text.strip()\n",
    "        except Exception as e:\n",
    "            print_status(\"Text Preprocessing\", False, str(e))\n",
    "            raise\n",
    "\n",
    "    def process_document(self, file_path: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process document with improved metadata and chunking.\"\"\"\n",
    "        try:\n",
    "            # Read document\n",
    "            doc = Document(file_path)\n",
    "            text = \"\\n\".join([para.text for para in doc.paragraphs if para.text.strip()])\n",
    "            \n",
    "            # Extract metadata\n",
    "            filename = os.path.basename(file_path)\n",
    "            name, age, doc_id = self.extract_metadata_from_filename(filename)\n",
    "            \n",
    "            # Preprocess and split text\n",
    "            clean_text = self.preprocess_text(text)\n",
    "            chunks = self.text_splitter.split_text(clean_text)\n",
    "            \n",
    "            # Create chunks with enhanced metadata\n",
    "            processed_chunks = []\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                # Calculate semantic importance score\n",
    "                importance_score = self._calculate_chunk_importance(chunk)\n",
    "                \n",
    "                processed_chunks.append({\n",
    "                    \"text\": chunk,\n",
    "                    \"metadata\": {\n",
    "                        \"source\": filename,\n",
    "                        \"person_name\": name,\n",
    "                        \"person_age\": age,\n",
    "                        \"document_id\": doc_id,\n",
    "                        \"chunk_index\": i,\n",
    "                        \"importance_score\": importance_score,\n",
    "                        \"chunk_length\": len(chunk),\n",
    "                        \"contains_question\": \"?\" in chunk,\n",
    "                    }\n",
    "                })\n",
    "            \n",
    "            print_status(\"Document Processing\", True, \n",
    "                        f\"Processed {filename} into {len(processed_chunks)} chunks\")\n",
    "            return processed_chunks\n",
    "            \n",
    "        except Exception as e:\n",
    "            print_status(\"Document Processing\", False, f\"Error processing {file_path}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _calculate_chunk_importance(self, chunk: str) -> float:\n",
    "        \"\"\"Calculate importance score for chunk based on Finnish text patterns.\"\"\"\n",
    "        score = 1.0\n",
    "        \n",
    "        # Key phrase indicators (common in Finnish documentation)\n",
    "        key_phrases = [\n",
    "            \"tärkeä\", \"merkittävä\", \"olennainen\", \"keskeinen\",\n",
    "            \"huomattava\", \"erityinen\", \"tärkein\", \"ensisijainen\"\n",
    "        ]\n",
    "        \n",
    "        # Increase score for chunks with key phrases\n",
    "        if any(phrase in chunk.lower() for phrase in key_phrases):\n",
    "            score *= 1.2\n",
    "            \n",
    "        # Prefer chunks with complete sentences\n",
    "        if chunk.count('.') > 0:\n",
    "            score *= 1.1\n",
    "            \n",
    "        # Prefer chunks with personal pronouns (common in Finnish personal documents)\n",
    "        if any(pronoun in chunk.lower() for pronoun in [\"minä\", \"minun\", \"minua\", \"minulla\"]):\n",
    "            score *= 1.15\n",
    "            \n",
    "        return score\n",
    "\n",
    "\n",
    "class MilvusManager:\n",
    "    def __init__(self, host: str = \"milvus-standalone\", port: str = \"19530\", alias: str = \"default\"):\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.alias = alias\n",
    "        self.connected = False\n",
    "        self.connect()\n",
    "        \n",
    "    def connect(self):\n",
    "        \"\"\"Establish connection to Milvus.\"\"\"\n",
    "        try:\n",
    "            try:\n",
    "                connections.remove_connection(alias=self.alias)\n",
    "                print_status(\"Milvus Connection\", True, \"Cleaned up existing connection\")\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            connections.connect(\n",
    "                alias=self.alias,\n",
    "                host=self.host,\n",
    "                port=self.port,\n",
    "                timeout=10.0\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                utility.get_server_version()\n",
    "                self.connected = True\n",
    "                print_status(\"Milvus Connection\", True, f\"Connected to {self.host}:{self.port}\")\n",
    "            except Exception as ve:\n",
    "                raise Exception(f\"Connection verification failed: {str(ve)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.connected = False\n",
    "            print_status(\"Milvus Connection\", False, str(e))\n",
    "            raise\n",
    "\n",
    "    def create_collection(self, collection_name: str = \"document_embeddings\"):\n",
    "        \"\"\"Create Milvus collection with appropriate schema.\"\"\"\n",
    "        try:\n",
    "            if utility.has_collection(collection_name):\n",
    "                Collection(name=collection_name).drop()\n",
    "                print_status(\"Milvus Collection\", True, f\"Dropped existing collection: {collection_name}\")\n",
    "                \n",
    "            fields = [\n",
    "                FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "                FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "                FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=EMBEDDING_DIM),\n",
    "                FieldSchema(name=\"person_name\", dtype=DataType.VARCHAR, max_length=100),\n",
    "                FieldSchema(name=\"person_age\", dtype=DataType.INT64),\n",
    "                FieldSchema(name=\"document_id\", dtype=DataType.VARCHAR, max_length=100),\n",
    "                FieldSchema(name=\"chunk_index\", dtype=DataType.INT64)\n",
    "            ]\n",
    "            \n",
    "            schema = CollectionSchema(\n",
    "                fields=fields,\n",
    "                description=\"Document embeddings collection\",\n",
    "                enable_dynamic_field=False\n",
    "            )\n",
    "            collection = Collection(name=collection_name, schema=schema)\n",
    "            \n",
    "            self.create_and_load_index(collection)\n",
    "            \n",
    "            print_status(\"Milvus Collection\", True, f\"Created new collection: {collection_name} with dim={EMBEDDING_DIM}\")\n",
    "            return collection\n",
    "        except Exception as e:\n",
    "            print_status(\"Milvus Collection\", False, str(e))\n",
    "            raise\n",
    "\n",
    "    def create_and_load_index(self, collection):\n",
    "        try:\n",
    "            index_params = {\n",
    "                \"metric_type\": \"IP\",\n",
    "                \"index_type\": \"IVF_FLAT\",\n",
    "                \"params\": {\"nlist\": 1024}\n",
    "            }\n",
    "            collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "            print_status(\"Index Creation\", True, \"Created IVF_FLAT index\")\n",
    "            \n",
    "            collection.load()\n",
    "            print_status(\"Collection Load\", True, \"Loaded collection into memory\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print_status(\"Index Creation\", False, str(e))\n",
    "            raise\n",
    "\n",
    "    def reload_collection(self, collection_name: str = \"document_embeddings\"):\n",
    "        try:\n",
    "            if utility.has_collection(collection_name):\n",
    "                collection = Collection(name=collection_name)\n",
    "                collection.load()\n",
    "                print_status(\"Collection Reload\", True, f\"Reloaded collection: {collection_name}\")\n",
    "                return collection\n",
    "            else:\n",
    "                raise Exception(f\"Collection {collection_name} does not exist\")\n",
    "        except Exception as e:\n",
    "            print_status(\"Collection Reload\", False, str(e))\n",
    "            raise\n",
    "class EmbeddingGenerator:\n",
    "    def __init__(self, model_name: str = \"TurkuNLP/gpt3-finnish-large\"):\n",
    "        try:\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "            self.embedding_dim = self.model.config.hidden_size\n",
    "            print_status(\"Embedding Model\", True, f\"Loaded {model_name} (dim={self.embedding_dim})\")\n",
    "        except Exception as e:\n",
    "            print_status(\"Embedding Model\", False, str(e))\n",
    "            raise\n",
    "        \n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        \n",
    "    def generate(self, texts: List[str], batch_size: int = 8) -> np.ndarray:\n",
    "        try:\n",
    "            all_embeddings = []\n",
    "            \n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                batch_texts = texts[i:i + batch_size]\n",
    "                \n",
    "                encoded_input = self.tokenizer(\n",
    "                    batch_texts,\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=512,\n",
    "                    return_tensors='pt'\n",
    "                ).to(self.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    model_output = self.model(**encoded_input)\n",
    "                \n",
    "                sentence_embeddings = self.mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "                sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "                \n",
    "                all_embeddings.append(sentence_embeddings.cpu().numpy())\n",
    "            \n",
    "            result = np.concatenate(all_embeddings)\n",
    "            \n",
    "            if result.shape[1] != EMBEDDING_DIM:\n",
    "                raise ValueError(f\"Embedding dimension mismatch. Expected {EMBEDDING_DIM}, got {result.shape[1]}\")\n",
    "            \n",
    "            print_status(\"Embedding Generation\", True, \n",
    "                    f\"Generated {len(texts)} embeddings with dimension {result.shape[1]}\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print_status(\"Embedding Generation\", False, str(e))\n",
    "            raise\n",
    "\n",
    "    def get_embedding_dim(self) -> int:\n",
    "        return self.embedding_dim\n",
    "\n",
    "\n",
    "class RAGPipeline:\n",
    "    def __init__(self, model_id: str = \"Finnish-NLP/llama-7b-finnish-instruct-v0.2\"):\n",
    "        try:\n",
    "            self.setup_llm(model_id)\n",
    "            print_status(\"LLM Setup\", True, f\"Loaded {model_id}\")\n",
    "            \n",
    "            self.doc_processor = DocumentProcessor()\n",
    "            self.embedding_generator = EmbeddingGenerator()\n",
    "            \n",
    "            if self.embedding_generator.get_embedding_dim() != EMBEDDING_DIM:\n",
    "                raise ValueError(f\"Embedding dimension mismatch. Global: {EMBEDDING_DIM}, \" \n",
    "                               f\"Generator: {self.embedding_generator.get_embedding_dim()}\")\n",
    "            \n",
    "            self.milvus_manager = MilvusManager(\n",
    "                host=MILVUS_HOST,\n",
    "                port=MILVUS_PORT,\n",
    "                alias=MILVUS_ALIAS\n",
    "            )\n",
    "            self.collection = self.milvus_manager.create_collection()\n",
    "            print_status(\"RAG Pipeline\", True, \"All components initialized\")\n",
    "        except Exception as e:\n",
    "            print_status(\"RAG Pipeline\", False, str(e))\n",
    "            raise\n",
    "    def process_documents(self, folder_path: str):\n",
    "        \"\"\"Process all documents in the specified folder.\"\"\"\n",
    "        try:\n",
    "            # Get all .docx files in the folder\n",
    "            file_paths = [f for f in os.listdir(folder_path) if f.endswith('.docx')]\n",
    "            if not file_paths:\n",
    "                raise ValueError(f\"No .docx files found in {folder_path}\")\n",
    "                \n",
    "            print_status(\"Document Loading\", True, f\"Found {len(file_paths)} documents\")\n",
    "            all_chunks = []\n",
    "            \n",
    "            # Process each document\n",
    "            for file in tqdm(file_paths, desc=\"Processing documents\"):\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                chunks = self.doc_processor.process_document(file_path)\n",
    "                all_chunks.extend(chunks)\n",
    "                \n",
    "            # Generate embeddings for all chunks\n",
    "            texts = [chunk[\"text\"] for chunk in all_chunks]\n",
    "            embeddings = self.embedding_generator.generate(texts)\n",
    "            \n",
    "            # Prepare entities for Milvus\n",
    "            entities = []\n",
    "            for i, (text, embedding, chunk) in enumerate(zip(texts, embeddings, all_chunks)):\n",
    "                entity = {\n",
    "                    \"text\": text,\n",
    "                    \"embedding\": embedding.tolist(),\n",
    "                    \"person_name\": chunk[\"metadata\"][\"person_name\"],\n",
    "                    \"person_age\": chunk[\"metadata\"][\"person_age\"],\n",
    "                    \"document_id\": chunk[\"metadata\"][\"document_id\"],\n",
    "                    \"chunk_index\": chunk[\"metadata\"][\"chunk_index\"]\n",
    "                }\n",
    "                entities.append(entity)\n",
    "            \n",
    "            # Insert into Milvus in batches\n",
    "            batch_size = 100\n",
    "            for i in range(0, len(entities), batch_size):\n",
    "                batch = entities[i:i + batch_size]\n",
    "                self.collection.insert(batch)\n",
    "            \n",
    "            # Ensure data is persisted\n",
    "            self.collection.flush()\n",
    "            \n",
    "            # Create index and load collection\n",
    "            self.milvus_manager.create_and_load_index(self.collection)\n",
    "            \n",
    "            print_status(\"Document Processing\", True, \n",
    "                        f\"Processed {len(texts)} chunks from {len(file_paths)} documents\")\n",
    "            return all_chunks\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print_status(\"Document Processing\", False, f\"Error: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "    def setup_llm(self, model_id: str):\n",
    "        try:\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\"\n",
    "            )\n",
    "            \n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                quantization_config=bnb_config,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                max_memory={0: \"6GiB\"},\n",
    "                offload_folder=\"offload\"\n",
    "            )\n",
    "            \n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "            self.pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                max_new_tokens=512,\n",
    "                do_sample=True,\n",
    "                temperature=0.3,\n",
    "                top_p=0.95,\n",
    "                repetition_penalty=1.15\n",
    "            )\n",
    "            print_status(\"LLM Pipeline\", True, \"Pipeline configured successfully\")\n",
    "        except Exception as e:\n",
    "            print_status(\"LLM Pipeline\", False, str(e))\n",
    "            raise\n",
    "\n",
    "    def query(self, question: str, top_k: int = 10):\n",
    "        try:\n",
    "            self.collection = self.milvus_manager.reload_collection()\n",
    "            \n",
    "            # Preprocess question\n",
    "            question = question.strip()\n",
    "            if not question.endswith('?'):\n",
    "                question += '?'\n",
    "            \n",
    "            # Generate question embedding and search\n",
    "            question_embedding = self.embedding_generator.generate([question])[0]\n",
    "            \n",
    "            search_params = {\n",
    "                \"metric_type\": \"IP\",\n",
    "                \"params\": {\"nprobe\": 50}\n",
    "            }\n",
    "            \n",
    "            # Get search results\n",
    "            search_results = self.collection.search(\n",
    "                data=[question_embedding.tolist()],\n",
    "                anns_field=\"embedding\",\n",
    "                param=search_params,\n",
    "                limit=top_k * 2,\n",
    "                output_fields=[\"text\", \"person_name\", \"document_id\", \"chunk_index\"]\n",
    "            )\n",
    "    \n",
    "            # Convert search results to a format we can work with\n",
    "            initial_results = []\n",
    "            for hit in search_results[0]:  # Fix: Properly handle Milvus Hit objects\n",
    "                initial_results.append({\n",
    "                    'text': hit.entity.get('text'),\n",
    "                    'person_name': hit.entity.get('person_name'),\n",
    "                    'document_id': hit.entity.get('document_id'),\n",
    "                    'chunk_index': hit.entity.get('chunk_index'),\n",
    "                    'score': float(hit.score)\n",
    "                })\n",
    "            \n",
    "            # Rerank results\n",
    "            reranked_results = self._rerank_results(question, initial_results)\n",
    "            \n",
    "            # Take top_k after reranking\n",
    "            results = reranked_results[:top_k]\n",
    "            \n",
    "            context_parts = []\n",
    "            for i, hit in enumerate(results):\n",
    "                context_parts.append(\n",
    "                    f\"[Dokumentti {i+1}]\\n\"\n",
    "                    f\"Lähde: {hit['document_id']}\\n\"\n",
    "                    f\"Henkilö: {hit['person_name']}\\n\"\n",
    "                    f\"Luotettavuus: {hit['score']:.2%}\\n\"\n",
    "                    f\"Tekstikatkelma:\\n{hit['text']}\\n\"\n",
    "                    f\"{'-' * 40}\\n\"\n",
    "                )\n",
    "            \n",
    "            context = \"\\n\".join(context_parts)\n",
    "            \n",
    "            prompt = f\"\"\"Tehtävä: Etsi tarkka vastaus annettuun kysymykseen käyttäen vain alla olevaa kontekstia.\n",
    "    \n",
    "            Kysymys: {question}\n",
    "            \n",
    "            Konteksti:\n",
    "            {context}\n",
    "            \n",
    "            Tärkeät ohjeet:\n",
    "            1. Jos löydät suoran vastauksen:\n",
    "               - Mainitse AINA ensin dokumentti, josta vastaus löytyy (esim. \"Dokumentti 1:\")\n",
    "               - Lainaa TARKASTI alkuperäistä tekstiä käyttäen lainausmerkkejä\n",
    "               - Perustele vastauksen luotettavuus yhdellä lyhyellä lauseella\n",
    "            \n",
    "            2. Jos löydät vain osittaisen vastauksen:\n",
    "               - Kerro selkeästi mikä osa vastauksesta löytyi ja mikä puuttuu\n",
    "               - Käytä silti suoria lainauksia löytyneestä osasta\n",
    "            \n",
    "            3. Jos et löydä minkäänlaista vastausta:\n",
    "               - Vastaa vain: \"En löydä suoraa vastausta annetusta kontekstista\"\n",
    "            \n",
    "            Vastaus:\"\"\"\n",
    "            \n",
    "            response = self.pipeline(\n",
    "                prompt,\n",
    "                max_new_tokens=300,\n",
    "                do_sample=True,\n",
    "                temperature=0.1,\n",
    "                top_p=0.85,\n",
    "                repetition_penalty=1.2\n",
    "            )[0][\"generated_text\"]\n",
    "    \n",
    "            response = self._clean_response(response)\n",
    "            \n",
    "            return {\n",
    "                \"answer\": response,\n",
    "                \"sources\": results,\n",
    "                \"metadata\": {\n",
    "                    \"question\": question,\n",
    "                    \"num_chunks_retrieved\": len(results),\n",
    "                    \"max_similarity_score\": max(hit['score'] for hit in results)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print_status(\"Query\", False, str(e))\n",
    "            raise\n",
    "    \n",
    "    def _rerank_results(self, question: str, results: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Rerank results using both semantic similarity and context relevance.\"\"\"\n",
    "        reranked = []\n",
    "        question_embedding = self.embedding_generator.generate([question])[0]\n",
    "        \n",
    "        for hit in results:\n",
    "            text = hit['text']\n",
    "            score = hit['score']\n",
    "            \n",
    "            # Compute semantic similarity\n",
    "            chunk_embedding = self.embedding_generator.generate([text])[0]\n",
    "            semantic_similarity = np.dot(question_embedding, chunk_embedding)\n",
    "            \n",
    "            # Adjust score based on semantic similarity\n",
    "            score *= (1 + semantic_similarity)\n",
    "            \n",
    "            # Further adjustments based on context relevance\n",
    "            if '?' in text:\n",
    "                score *= 1.05  # Slight boost if chunk contains a question\n",
    "            if any(entity in text.lower() for entity in self._extract_entities(question)):\n",
    "                score *= 1.1  # Boost if entities match\n",
    "            \n",
    "            hit['score'] = score\n",
    "            reranked.append(hit)\n",
    "        \n",
    "        # Sort the reranked results\n",
    "        reranked.sort(key=lambda x: x['score'], reverse=True)\n",
    "        return reranked\n",
    "    \n",
    "    def _extract_entities(self, text: str) -> List[str]:\n",
    "        \"\"\"Simple entity extraction based on capitalized words.\"\"\"\n",
    "        return re.findall(r'\\b[A-Z][a-zäöå]*\\b', text)\n",
    "\n",
    "    def _clean_response(self, response: str) -> str:\n",
    "        response = re.sub(r'(\\[Dokumentti \\d+\\])\\s*\\1', r'\\1', response)\n",
    "        response = re.sub(r'Luottamus: \\d+%\\s*Selitys:', '', response)\n",
    "        if len(response) > 500:\n",
    "            response = response[:497] + '...'\n",
    "        return response.strip()\n",
    "\n",
    "    def _create_prompt(self, question: str, context: str) -> str:\n",
    "        return f\"\"\"Tehtävä: Vastaa annettuun kysymykseen käyttäen vain alla olevaa kontekstia.\n",
    "\n",
    "        Kysymys: {question}\n",
    "        \n",
    "        Konteksti:\n",
    "        {context}\n",
    "        \n",
    "        Vastausohjeet:\n",
    "        1. Jos löydät suoran vastauksen:\n",
    "           - Mainitse ensin dokumentti, josta vastaus löytyy\n",
    "           - Lainaa tekstiä tarkasti käyttäen lainausmerkkejä\n",
    "           - Mainitse vastauksen luotettavuus prosentteina\n",
    "        \n",
    "        2. Jos löydät osittaisen vastauksen:\n",
    "           - Kerro, mitä tietoa löysit ja mistä\n",
    "           - Mainitse selkeästi, mitä tietoa puuttuu\n",
    "        \n",
    "        3. Jos et löydä vastausta:\n",
    "           - Vastaa vain: \"En löydä suoraa vastausta annetusta kontekstista\"\n",
    "        \n",
    "        Vastaus:\"\"\"\n",
    "\n",
    "## validating \n",
    "def verify_pipeline_components():\n",
    "    doc_processor = DocumentProcessor()\n",
    "    milvus_manager = MilvusManager()\n",
    "    embedding_generator = EmbeddingGenerator()\n",
    "    pipeline = RAGPipeline()\n",
    "    return all([doc_processor, milvus_manager, embedding_generator, pipeline])\n",
    "\n",
    "verify_section(\"Pipeline Components\", verify_pipeline_components)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccd03e30-aaa8-47a0-8494-fcea122cf666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb14225347914f4290ddd7168be9c660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCESS | LLM Pipeline\n",
      "  └─ Pipeline configured successfully\n",
      "\n",
      "✅ SUCCESS | LLM Setup\n",
      "  └─ Loaded Finnish-NLP/llama-7b-finnish-instruct-v0.2\n",
      "\n",
      "✅ SUCCESS | Document Processor\n",
      "  └─ Initialized with Finnish-optimized settings\n",
      "\n",
      "✅ SUCCESS | Embedding Model\n",
      "  └─ Loaded TurkuNLP/gpt3-finnish-large (dim=1536)\n",
      "\n",
      "✅ SUCCESS | Milvus Connection\n",
      "  └─ Cleaned up existing connection\n",
      "\n",
      "✅ SUCCESS | Milvus Connection\n",
      "  └─ Connected to milvus-standalone:19530\n",
      "\n",
      "✅ SUCCESS | Milvus Collection\n",
      "  └─ Dropped existing collection: document_embeddings\n",
      "\n",
      "✅ SUCCESS | Index Creation\n",
      "  └─ Created IVF_FLAT index\n",
      "\n",
      "✅ SUCCESS | Collection Load\n",
      "  └─ Loaded collection into memory\n",
      "\n",
      "✅ SUCCESS | Milvus Collection\n",
      "  └─ Created new collection: document_embeddings with dim=1536\n",
      "\n",
      "✅ SUCCESS | RAG Pipeline\n",
      "  └─ All components initialized\n",
      "\n",
      "❌ FAILED | Neo4j Components\n",
      "  └─ Error: name 'GraphDatabase' is not defined\n",
      "\n",
      "✅ SUCCESS | Section Neo4j Components Verification\n",
      "  └─ Successfully executed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Neo4jDocumentManager:\n",
    "    def __init__(self, uri: str = \"neo4j://neo4j:7687\", \n",
    "                 username: str = \"neo4j\", \n",
    "                 password: str = \"test\"):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "        self.setup_constraints()\n",
    "        self.person_names = set() \n",
    "\n",
    "    def check_connection(self) -> bool:\n",
    "        \"\"\"Check if Neo4j connection is working.\"\"\"\n",
    "        try:\n",
    "            with self.driver.session() as session:\n",
    "                result = session.run(\"RETURN 1 as test\")\n",
    "                assert result.single()[\"test\"] == 1\n",
    "                print(\"Neo4j connection test successful\")\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            print(f\"Neo4j connection test failed: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def setup_constraints(self):\n",
    "        \"\"\"Setup necessary constraints for the graph.\"\"\"\n",
    "        try:\n",
    "            with self.driver.session() as session:\n",
    "                # Neo4j 5+ syntax for constraints\n",
    "                constraints = [\n",
    "                    \"\"\"CREATE CONSTRAINT person_name_unique IF NOT EXISTS \n",
    "                       FOR (p:Person) REQUIRE p.name IS UNIQUE\"\"\",\n",
    "                    \"\"\"CREATE CONSTRAINT document_id_unique IF NOT EXISTS \n",
    "                       FOR (d:Document) REQUIRE d.id IS UNIQUE\"\"\"\n",
    "                ]\n",
    "                \n",
    "                for constraint in constraints:\n",
    "                    try:\n",
    "                        session.run(constraint)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Could not create constraint: {str(e)}\")\n",
    "                        # Try older Neo4j syntax if newer one fails\n",
    "                        try:\n",
    "                            if \"person_name\" in constraint:\n",
    "                                session.run(\"\"\"CREATE CONSTRAINT ON (p:Person) \n",
    "                                             ASSERT p.name IS UNIQUE\"\"\")\n",
    "                            elif \"document_id\" in constraint:\n",
    "                                session.run(\"\"\"CREATE CONSTRAINT ON (d:Document) \n",
    "                                             ASSERT d.id IS UNIQUE\"\"\")\n",
    "                        except Exception as e2:\n",
    "                            print(f\"Warning: Could not create constraint with old syntax: {str(e2)}\")\n",
    "                            \n",
    "                print(\"Neo4j constraints setup completed\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error during constraint setup: {str(e)}\")\n",
    "            # Continue even if constraints fail - they're helpful but not crit\n",
    "            pass\n",
    "        \n",
    "    def _extract_person_names(self, text: str) -> set:\n",
    "        \"\"\"Extract potential person names from text using Finnish patterns.\"\"\"\n",
    "        names = set()\n",
    "        \n",
    "        # Finnish name patterns\n",
    "        patterns = [\n",
    "            r'\\b[A-ZÄÖÅ][a-zäöå]+\\b',  # Basic capitalized word\n",
    "            r'(?:herra|rouva|neiti)\\s+[A-ZÄÖÅ][a-zäöå]+',  # Titles\n",
    "            r'(?:ystävä|naapuri)\\s+[A-ZÄÖÅ][a-zäöå]+',  # Relationships\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            matches = re.finditer(pattern, text)\n",
    "            for match in matches:\n",
    "                name = match.group().split()[-1]  # Get the actual name part\n",
    "                if len(name) > 2:  # Filter out very short words\n",
    "                    names.add(name)\n",
    "        \n",
    "        return names\n",
    "\n",
    "    def create_document_graph(self, processed_chunks: List[Dict]):\n",
    "        \"\"\"Create graph structure from processed document chunks.\"\"\"\n",
    "        # First pass: collect all person names\n",
    "        for chunk in processed_chunks:\n",
    "            # Add names from metadata\n",
    "            if chunk[\"metadata\"].get(\"person_name\"):\n",
    "                self.person_names.add(chunk[\"metadata\"][\"person_name\"])\n",
    "            \n",
    "            # Add names found in content\n",
    "            self.person_names.update(self._extract_person_names(chunk[\"text\"]))\n",
    "        \n",
    "        with self.driver.session() as session:\n",
    "            for chunk in processed_chunks:\n",
    "                # Create Document and Person nodes\n",
    "                self._create_document_and_person_nodes(session, chunk)\n",
    "                \n",
    "                # Extract and create relationships\n",
    "                self._process_relationships(session, chunk)\n",
    "    \n",
    "    def _create_document_and_person_nodes(self, session, chunk: Dict):\n",
    "        \"\"\"Create document and person nodes with relationships.\"\"\"\n",
    "        session.run(\"\"\"\n",
    "            MERGE (p:Person {name: $person_name})\n",
    "            SET p.age = $age\n",
    "            \n",
    "            MERGE (d:Document {id: $doc_id})\n",
    "            SET d.content = $content,\n",
    "                d.chunk_index = $chunk_index\n",
    "            \n",
    "            MERGE (p)-[:APPEARS_IN]->(d)\n",
    "            \n",
    "            WITH p, d\n",
    "            MATCH (p)-[:APPEARS_IN]->(other:Document)\n",
    "            WHERE other.id <> d.id\n",
    "            MERGE (d)-[:RELATED_TO]->(other)\n",
    "        \"\"\", {\n",
    "            \"person_name\": chunk[\"metadata\"][\"person_name\"],\n",
    "            \"age\": chunk[\"metadata\"][\"person_age\"],\n",
    "            \"doc_id\": f\"{chunk['metadata']['document_id']}_{chunk['metadata']['chunk_index']}\",\n",
    "            \"content\": chunk[\"text\"],\n",
    "            \"chunk_index\": chunk[\"metadata\"][\"chunk_index\"]\n",
    "        })\n",
    "\n",
    "    def _process_relationships(self, session, chunk: Dict):\n",
    "        \"\"\"Create relationships based on the content of each chunk.\"\"\"\n",
    "        # Extract persons mentioned in the text\n",
    "        mentioned_persons = self.find_mentioned_persons(chunk[\"text\"])\n",
    "        \n",
    "        # Get the main person from metadata\n",
    "        main_person = chunk[\"metadata\"].get(\"person_name\")\n",
    "        \n",
    "        # If there is no main person in this chunk, skip relationship creation\n",
    "        if not main_person:\n",
    "            return\n",
    "        \n",
    "        # Loop over each mentioned person to create a relationship in the graph\n",
    "        for mentioned_person in mentioned_persons:\n",
    "            # Define the relationship type - here it's just an example relationship type\n",
    "            relationship_type = \"KNOWS\"\n",
    "            \n",
    "            # Create relationship in Neo4j\n",
    "            session.run(\"\"\"\n",
    "                MATCH (p1:Person {name: $main_person})\n",
    "                MATCH (p2:Person {name: $mentioned_person})\n",
    "                MERGE (p1)-[r:KNOWS]->(p2)\n",
    "                RETURN r\n",
    "            \"\"\", {\n",
    "                \"main_person\": main_person,\n",
    "                \"mentioned_person\": mentioned_person\n",
    "            })   \n",
    "\n",
    "    def find_mentioned_persons(self, text: str) -> List[str]:\n",
    "        \"\"\"Find persons mentioned in text from known person names.\"\"\"\n",
    "        mentioned_persons = []\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Look for known person names in the text\n",
    "        for name in self.person_names:\n",
    "            if name.lower() in text_lower:\n",
    "                mentioned_persons.append(name)\n",
    "                \n",
    "        return mentioned_persons\n",
    "\n",
    "    def query(self, question: str) -> Dict:\n",
    "        \"\"\"Enhanced query processing with dynamic person detection.\"\"\"\n",
    "        # Find mentioned persons in the question\n",
    "        mentioned_persons = self.find_mentioned_persons(question)\n",
    "        \n",
    "        with self.driver.session() as session:\n",
    "            # Get person contexts for all mentioned persons\n",
    "            person_contexts = []\n",
    "            for person_name in mentioned_persons:\n",
    "                context = self._get_person_context(session, person_name)\n",
    "                if context:\n",
    "                    person_contexts.append(context)\n",
    "            \n",
    "            # Get related documents\n",
    "            documents = self._get_related_documents(session, mentioned_persons)\n",
    "            \n",
    "            return {\n",
    "                \"person_contexts\": person_contexts,\n",
    "                \"related_documents\": documents,\n",
    "                \"mentioned_persons\": mentioned_persons\n",
    "            }\n",
    "    \n",
    "    def _get_person_context(self, session, person_name: str) -> Dict:\n",
    "        \"\"\"Get comprehensive context for a person.\"\"\"\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (p:Person {name: $name})\n",
    "            OPTIONAL MATCH (p)-[:APPEARS_IN]->(d:Document)\n",
    "            OPTIONAL MATCH (d)-[:HAS_RELATIONSHIP]->(r:Relationship)\n",
    "            \n",
    "            WITH p,\n",
    "                 COLLECT(DISTINCT r.type) as relationship_types,\n",
    "                 COLLECT(DISTINCT d.content) as contents,\n",
    "                 COLLECT(DISTINCT d.id) as doc_ids\n",
    "            \n",
    "            RETURN {\n",
    "                name: p.name,\n",
    "                age: p.age,\n",
    "                relationship_types: relationship_types,\n",
    "                document_count: SIZE(contents),\n",
    "                document_ids: doc_ids\n",
    "            } as context\n",
    "        \"\"\", {\"name\": person_name})\n",
    "        \n",
    "        return result.single()[\"context\"] if result.peek() else None\n",
    "\n",
    "class Neo4jEnhancedRAGPipeline:\n",
    "    def __init__(self, base_rag_pipeline, neo4j_uri: str = \"neo4j://neo4j:7687\"):\n",
    "        self.base_rag = base_rag_pipeline\n",
    "        self.neo4j_manager = Neo4jDocumentManager(\n",
    "            uri=neo4j_uri,\n",
    "            username=\"neo4j\",\n",
    "            password=\"test\"\n",
    "        )\n",
    "    \n",
    "    def process_documents(self, folder_path: str):\n",
    "        \"\"\"Process documents and create graph structure.\"\"\"\n",
    "        try:\n",
    "            # Process with base RAG first\n",
    "            print(\"Processing documents with base RAG...\")\n",
    "            # Call process_documents and store all chunks\n",
    "            all_chunks = []\n",
    "            \n",
    "            # Get all .docx files in the folder\n",
    "            file_paths = [f for f in os.listdir(folder_path) if f.endswith('.docx')]\n",
    "            if not file_paths:\n",
    "                raise ValueError(f\"No .docx files found in {folder_path}\")\n",
    "            \n",
    "            print(f\"Found {len(file_paths)} documents to process\")\n",
    "            \n",
    "            for file in file_paths:\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                chunks = self.base_rag.doc_processor.process_document(file_path)\n",
    "                all_chunks.extend(chunks)\n",
    "            \n",
    "            if not all_chunks:\n",
    "                raise ValueError(\"No chunks were processed from documents\")\n",
    "            \n",
    "            print(f\"Processed {len(all_chunks)} chunks from documents\")\n",
    "            \n",
    "            # Create graph structure\n",
    "            print(\"Creating Neo4j graph structure...\")\n",
    "            self.neo4j_manager.create_document_graph(all_chunks)\n",
    "            \n",
    "            # Process with base RAG pipeline for vector embeddings\n",
    "            self.base_rag.process_documents(folder_path)\n",
    "            \n",
    "            return all_chunks\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing documents: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _combine_contexts(self, base_sources: List[Dict], \n",
    "                         graph_sources: List[Dict], \n",
    "                         person_contexts: List[Dict]) -> Dict:\n",
    "        \"\"\"Combine all sources of context intelligently.\"\"\"\n",
    "        combined = {\n",
    "            \"documents\": [],\n",
    "            \"relationships\": [],\n",
    "            \"persons\": []\n",
    "        }\n",
    "        \n",
    "        # Add base sources\n",
    "        for source in base_sources:\n",
    "            combined[\"documents\"].append({\n",
    "                \"content\": source[\"text\"],\n",
    "                \"source\": source[\"document_id\"],\n",
    "                \"relevance\": source[\"score\"]\n",
    "            })\n",
    "        \n",
    "        # Add graph-enhanced sources\n",
    "        for source in graph_sources:\n",
    "            if source not in combined[\"documents\"]:\n",
    "                combined[\"documents\"].append({\n",
    "                    \"content\": source[\"content\"],\n",
    "                    \"source\": source[\"doc_id\"],\n",
    "                    \"relevance\": source.get(\"relevance\", 0.5)\n",
    "                })\n",
    "        \n",
    "        # Add person contexts\n",
    "        for context in person_contexts:\n",
    "            combined[\"persons\"].append({\n",
    "                \"name\": context[\"name\"],\n",
    "                \"age\": context[\"age\"],\n",
    "                \"relationships\": context[\"relationship_types\"],\n",
    "                \"document_count\": context[\"document_count\"]\n",
    "            })\n",
    "            \n",
    "        return combined\n",
    "\n",
    "    def _generate_enhanced_answer(self, question: str, context: Dict) -> str:\n",
    "        \"\"\"Generate answer using combined context.\"\"\"\n",
    "        # Create enhanced prompt\n",
    "        prompt = f\"\"\"Käytä seuraavaa kontekstia vastataksesi kysymykseen. Huomioi erityisesti henkilöiden väliset suhteet.\n",
    "\n",
    "        Kysymys: {question}\n",
    "\n",
    "        Dokumenttikonteksti:\n",
    "        {self._format_document_context(context['documents'])}\n",
    "\n",
    "        Henkilökonteksti:\n",
    "        {self._format_person_context(context['persons'])}\n",
    "\n",
    "        Vastausohjeet:\n",
    "        1. Käytä dokumenttien ja suhteiden tietoja yhdessä\n",
    "        2. Mainitse lähteet selkeästi\n",
    "        3. Korosta henkilöiden välisiä suhteita\n",
    "        4. Käytä suoria lainauksia kun mahdollista\n",
    "\n",
    "        Vastaus:\"\"\"\n",
    "\n",
    "        # Generate answer using base pipeline\n",
    "        response = self.base_rag.pipeline(prompt)[0][\"generated_text\"]\n",
    "        return response.strip()\n",
    "\n",
    "    def _format_document_context(self, documents: List[Dict]) -> str:\n",
    "        \"\"\"Format document context for prompt.\"\"\"\n",
    "        return \"\\n\\n\".join([\n",
    "            f\"Dokumentti {i+1} ({doc['source']}):\\n{doc['content']}\"\n",
    "            for i, doc in enumerate(documents)\n",
    "        ])\n",
    "\n",
    "    def _format_person_context(self, persons: List[Dict]) -> str:\n",
    "        \"\"\"Format person context for prompt.\"\"\"\n",
    "        return \"\\n\\n\".join([\n",
    "            f\"Henkilö: {person['name']}\\n\"\n",
    "            f\"Ikä: {person['age']}\\n\"\n",
    "            f\"Suhteet: {', '.join(person['relationships'])}\"\n",
    "            for person in persons\n",
    "        ])\n",
    "\n",
    "def verify_neo4j_components():\n",
    "    \"\"\"Verify Neo4j components are working correctly.\"\"\"\n",
    "    try:\n",
    "        # Create base pipeline\n",
    "        base_pipeline = RAGPipeline()\n",
    "        \n",
    "        # Create enhanced pipeline\n",
    "        neo4j_rag = Neo4jEnhancedRAGPipeline(\n",
    "            base_rag_pipeline=base_pipeline,\n",
    "            neo4j_uri=\"neo4j://neo4j:7687\"\n",
    "        )\n",
    "        \n",
    "        # Verify Neo4j connection\n",
    "        with neo4j_rag.neo4j_manager.driver.session() as session:\n",
    "            result = session.run(\"RETURN 1 as test\")\n",
    "            assert result.single()[\"test\"] == 1\n",
    "        \n",
    "        print_status(\"Neo4j Components\", True, \"Connection verified\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print_status(\"Neo4j Components\", False, f\"Error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Add verification call after other verifications\n",
    "verify_section(\"Neo4j Components\", verify_neo4j_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b341d652-c9a0-4ddd-bf18-3af1daf84ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad4b567499f24ef6a8c738b0837a777a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCESS | LLM Pipeline\n",
      "  └─ Pipeline configured successfully\n",
      "\n",
      "✅ SUCCESS | LLM Setup\n",
      "  └─ Loaded Finnish-NLP/llama-7b-finnish-instruct-v0.2\n",
      "\n",
      "✅ SUCCESS | Document Processor\n",
      "  └─ Initialized with Finnish-optimized settings\n",
      "\n",
      "✅ SUCCESS | Embedding Model\n",
      "  └─ Loaded TurkuNLP/gpt3-finnish-large (dim=1536)\n",
      "\n",
      "✅ SUCCESS | Milvus Connection\n",
      "  └─ Cleaned up existing connection\n",
      "\n",
      "✅ SUCCESS | Milvus Connection\n",
      "  └─ Connected to milvus-standalone:19530\n",
      "\n",
      "✅ SUCCESS | Milvus Collection\n",
      "  └─ Dropped existing collection: document_embeddings\n",
      "\n",
      "✅ SUCCESS | Index Creation\n",
      "  └─ Created IVF_FLAT index\n",
      "\n",
      "✅ SUCCESS | Collection Load\n",
      "  └─ Loaded collection into memory\n",
      "\n",
      "✅ SUCCESS | Milvus Collection\n",
      "  └─ Created new collection: document_embeddings with dim=1536\n",
      "\n",
      "✅ SUCCESS | RAG Pipeline\n",
      "  └─ All components initialized\n",
      "\n",
      "✅ SUCCESS | Section Testing Infrastructure Verification\n",
      "  └─ Successfully executed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing Infrastructure\n",
    "class RAGTester:\n",
    "    def __init__(self, pipeline: RAGPipeline):\n",
    "        self.pipeline = pipeline\n",
    "        self.test_results = []\n",
    "        \n",
    "    def run_test_suite(self, test_questions: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Run comprehensive tests on the RAG pipeline.\"\"\"\n",
    "        test_results = []\n",
    "        summary_stats = {\n",
    "            \"total_questions\": len(test_questions),\n",
    "            \"successful_responses\": 0,\n",
    "            \"failed_responses\": 0,\n",
    "            \"average_similarity\": 0.0,\n",
    "            \"direct_quote_ratio\": 0.0\n",
    "        }\n",
    "        \n",
    "        for question in test_questions:\n",
    "            try:\n",
    "                # Get response from pipeline\n",
    "                result = self.pipeline.query(question)\n",
    "                \n",
    "                # Validate response\n",
    "                validation = self._validate_response(result[\"answer\"])\n",
    "                \n",
    "                # Calculate response metrics\n",
    "                metrics = {\n",
    "                    \"question\": question,\n",
    "                    \"has_direct_quote\": validation[\"has_direct_quote\"],\n",
    "                    \"source_count\": len(result[\"sources\"]),\n",
    "                    \"max_similarity\": max(s[\"similarity_score\"] for s in result[\"sources\"]),\n",
    "                    \"response_quality\": validation,\n",
    "                    \"response_length\": len(result[\"answer\"]),\n",
    "                }\n",
    "                \n",
    "                test_results.append(metrics)\n",
    "                summary_stats[\"successful_responses\"] += 1\n",
    "                summary_stats[\"average_similarity\"] += metrics[\"max_similarity\"]\n",
    "                summary_stats[\"direct_quote_ratio\"] += int(metrics[\"has_direct_quote\"])\n",
    "                \n",
    "                # Print detailed results\n",
    "                self._print_test_result(question, result, metrics)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error testing question '{question}': {str(e)}\")\n",
    "                summary_stats[\"failed_responses\"] += 1\n",
    "                \n",
    "        # Calculate final statistics\n",
    "        if summary_stats[\"successful_responses\"] > 0:\n",
    "            summary_stats[\"average_similarity\"] /= summary_stats[\"successful_responses\"]\n",
    "            summary_stats[\"direct_quote_ratio\"] /= summary_stats[\"successful_responses\"]\n",
    "        \n",
    "        return {\n",
    "            \"detailed_results\": test_results,\n",
    "            \"summary\": summary_stats\n",
    "        }\n",
    "    \n",
    "    def _validate_response(self, response: str) -> Dict[str, bool]:\n",
    "        \"\"\"Validate Finnish language response with detailed checks.\"\"\"\n",
    "        validation = {\n",
    "            # Basic structural checks\n",
    "            \"has_source_reference\": bool(re.search(r'\\[Dokumentti \\d+\\]', response)),\n",
    "            \"has_direct_quote\": '\"' in response,\n",
    "            \"is_complete_sentence\": response.strip().endswith(('.', '?', '!')),\n",
    "            \"has_confidence\": bool(re.search(r'\\d+\\s*%', response)),\n",
    "            \"reasonable_length\": 10 <= len(response) <= 500,\n",
    "            \n",
    "            # Finnish language specific checks\n",
    "            \"has_finnish_chars\": bool(re.search(r'[äöåÄÖÅ]', response)),\n",
    "            \"proper_finnish_structure\": self._check_finnish_structure(response)\n",
    "        }\n",
    "        return validation\n",
    "    \n",
    "    def _check_finnish_structure(self, text: str) -> bool:\n",
    "        \"\"\"Check if the response follows typical Finnish sentence structure.\"\"\"\n",
    "        finnish_endings = [\n",
    "            'ssa', 'ssä', 'sta', 'stä', 'lla', 'llä', 'lta', 'ltä',\n",
    "            'ksi', 'in', 'en', 'teen', 'seen'\n",
    "        ]\n",
    "        words = text.lower().split()\n",
    "        if not words:\n",
    "            return False\n",
    "            \n",
    "        has_finnish_ending = any(\n",
    "            any(word.endswith(ending) for ending in finnish_endings)\n",
    "            for word in words\n",
    "        )\n",
    "        \n",
    "        return has_finnish_ending\n",
    "    \n",
    "    def _print_test_result(self, question: str, result: Dict, metrics: Dict):\n",
    "        \"\"\"Print formatted test results.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Answer: {result['answer']}\")\n",
    "        print(\"\\nMetrics:\")\n",
    "        print(f\"- Source count: {metrics['source_count']}\")\n",
    "        print(f\"- Max similarity: {metrics['max_similarity']:.2%}\")\n",
    "        print(f\"- Response length: {metrics['response_length']}\")\n",
    "        print(\"\\nValidation:\")\n",
    "        for key, value in metrics['response_quality'].items():\n",
    "            print(f\"- {key}: {'✅' if value else '❌'}\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "\n",
    "def run_rag_tests(pipeline: RAGPipeline, test_questions: List[str] = None):\n",
    "    \"\"\"Execute RAG tests with default or custom test questions.\"\"\"\n",
    "    if test_questions is None:\n",
    "        test_questions = [\n",
    "            \"Onko Marjatta Eilan ystävä?\",\n",
    "            \"Miten Sulo kokee sosiaalisen kanssakäymisen merkityksen?\",\n",
    "            \"Montako sisarusta Sulolla on?\",\n",
    "            \"Millainen on Eilan arki?\",\n",
    "            \"Mikä on Sulolle tärkeää?\"\n",
    "        ]\n",
    "    \n",
    "    tester = RAGTester(pipeline)\n",
    "    results = tester.run_test_suite(test_questions)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nTest Summary:\")\n",
    "    print(f\"Total questions: {results['summary']['total_questions']}\")\n",
    "    print(f\"Successful responses: {results['summary']['successful_responses']}\")\n",
    "    print(f\"Failed responses: {results['summary']['failed_responses']}\")\n",
    "    print(f\"Average similarity: {results['summary']['average_similarity']:.2%}\")\n",
    "    print(f\"Direct quote ratio: {results['summary']['direct_quote_ratio']:.2%}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def verify_testing_infrastructure():\n",
    "    pipeline = RAGPipeline()\n",
    "    tester = RAGTester(pipeline)\n",
    "    return bool(tester)\n",
    "\n",
    "verify_section(\"Testing Infrastructure\", verify_testing_infrastructure)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be4e3db5-26a8-44c6-bd51-106ba6439296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting comparison test execution...\n",
      "\n",
      "1. Initializing base pipeline...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef7d8af16764578978710b5c0b63b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCESS | LLM Pipeline\n",
      "  └─ Pipeline configured successfully\n",
      "\n",
      "✅ SUCCESS | LLM Setup\n",
      "  └─ Loaded Finnish-NLP/llama-7b-finnish-instruct-v0.2\n",
      "\n",
      "✅ SUCCESS | Document Processor\n",
      "  └─ Initialized with Finnish-optimized settings\n",
      "\n",
      "✅ SUCCESS | Embedding Model\n",
      "  └─ Loaded TurkuNLP/gpt3-finnish-large (dim=1536)\n",
      "\n",
      "✅ SUCCESS | Milvus Connection\n",
      "  └─ Cleaned up existing connection\n",
      "\n",
      "✅ SUCCESS | Milvus Connection\n",
      "  └─ Connected to milvus-standalone:19530\n",
      "\n",
      "✅ SUCCESS | Milvus Collection\n",
      "  └─ Dropped existing collection: document_embeddings\n",
      "\n",
      "✅ SUCCESS | Index Creation\n",
      "  └─ Created IVF_FLAT index\n",
      "\n",
      "✅ SUCCESS | Collection Load\n",
      "  └─ Loaded collection into memory\n",
      "\n",
      "✅ SUCCESS | Milvus Collection\n",
      "  └─ Created new collection: document_embeddings with dim=1536\n",
      "\n",
      "✅ SUCCESS | RAG Pipeline\n",
      "  └─ All components initialized\n",
      "   ✓ Base pipeline initialized\n",
      "\n",
      "2. Creating Neo4j Enhanced Pipeline...\n",
      "Neo4j constraints setup completed\n",
      "   ✓ Neo4j Enhanced pipeline created\n",
      "\n",
      "3. Processing documents...\n",
      "Processing documents with base RAG...\n",
      "Found 12 documents to process\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Annikki 77v M2042.docx into 16 chunks\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Eila 81v SH-4.docx into 5 chunks\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Elisa 66v KL-49.docx into 17 chunks\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Jarmo 68v KL-40.docx into 16 chunks\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Kosti 76v M7-54.docx into 11 chunks\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Maire 75v KL-51.docx into 15 chunks\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Outi 69v M7-44.docx into 17 chunks\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Salli 81v M12-7.docx into 15 chunks\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Sulo 75v C5-50.docx into 4 chunks\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Terttu 78v C5-49.docx into 6 chunks\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Ulla 71v M9-17.docx into 24 chunks\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Voitto 69v KL-41.docx into 14 chunks\n",
      "Processed 160 chunks from documents\n",
      "Creating Neo4j graph structure...\n",
      "\n",
      "✅ SUCCESS | Document Loading\n",
      "  └─ Found 12 documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60604e61d8cc49768b4eebaa85183ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing documents:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Annikki 77v M2042.docx into 16 chunks\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Eila 81v SH-4.docx into 5 chunks\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Elisa 66v KL-49.docx into 17 chunks\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Jarmo 68v KL-40.docx into 16 chunks\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Kosti 76v M7-54.docx into 11 chunks\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Maire 75v KL-51.docx into 15 chunks\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Outi 69v M7-44.docx into 17 chunks\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Salli 81v M12-7.docx into 15 chunks\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Sulo 75v C5-50.docx into 4 chunks\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Terttu 78v C5-49.docx into 6 chunks\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Ulla 71v M9-17.docx into 24 chunks\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Voitto 69v KL-41.docx into 14 chunks\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 160 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Index Creation\n",
      "  └─ Created IVF_FLAT index\n",
      "\n",
      "✅ SUCCESS | Collection Load\n",
      "  └─ Loaded collection into memory\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed 160 chunks from 12 documents\n",
      "   ✓ Documents processed\n",
      "\n",
      "4. Running comparison tests...\n",
      "\n",
      "Starting RAG test execution...\n",
      "\n",
      "1. Initializing base pipeline...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c04a0f6448bf46a0bc2cd1e1f14005d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCESS | LLM Pipeline\n",
      "  └─ Pipeline configured successfully\n",
      "\n",
      "✅ SUCCESS | LLM Setup\n",
      "  └─ Loaded Finnish-NLP/llama-7b-finnish-instruct-v0.2\n",
      "\n",
      "✅ SUCCESS | Document Processor\n",
      "  └─ Initialized with Finnish-optimized settings\n",
      "\n",
      "✅ SUCCESS | Embedding Model\n",
      "  └─ Loaded TurkuNLP/gpt3-finnish-large (dim=1536)\n",
      "\n",
      "✅ SUCCESS | Milvus Connection\n",
      "  └─ Cleaned up existing connection\n",
      "\n",
      "✅ SUCCESS | Milvus Connection\n",
      "  └─ Connected to milvus-standalone:19530\n",
      "\n",
      "✅ SUCCESS | Milvus Collection\n",
      "  └─ Dropped existing collection: document_embeddings\n",
      "\n",
      "✅ SUCCESS | Index Creation\n",
      "  └─ Created IVF_FLAT index\n",
      "\n",
      "✅ SUCCESS | Collection Load\n",
      "  └─ Loaded collection into memory\n",
      "\n",
      "✅ SUCCESS | Milvus Collection\n",
      "  └─ Created new collection: document_embeddings with dim=1536\n",
      "\n",
      "✅ SUCCESS | RAG Pipeline\n",
      "  └─ All components initialized\n",
      "   ✓ Base pipeline initialized\n",
      "\n",
      "2. Creating Finnish RAG Agent...\n",
      "   ✓ Agent created\n",
      "\n",
      "3. Processing documents...\n",
      "\n",
      "✅ SUCCESS | Document Loading\n",
      "  └─ Found 12 documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d65f40ebaf954287bffb0d9f0b6c6f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing documents:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Annikki 77v M2042.docx into 16 chunks\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Eila 81v SH-4.docx into 5 chunks\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Elisa 66v KL-49.docx into 17 chunks\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Jarmo 68v KL-40.docx into 16 chunks\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Kosti 76v M7-54.docx into 11 chunks\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Maire 75v KL-51.docx into 15 chunks\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Outi 69v M7-44.docx into 17 chunks\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Salli 81v M12-7.docx into 15 chunks\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Sulo 75v C5-50.docx into 4 chunks\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Terttu 78v C5-49.docx into 6 chunks\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Ulla 71v M9-17.docx into 24 chunks\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Voitto 69v KL-41.docx into 14 chunks\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 160 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Index Creation\n",
      "  └─ Created IVF_FLAT index\n",
      "\n",
      "✅ SUCCESS | Collection Load\n",
      "  └─ Loaded collection into memory\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed 160 chunks from 12 documents\n",
      "   ✓ Documents processed\n",
      "\n",
      "4. Running test questions...\n",
      "\n",
      "Kysymys 1/5:\n",
      "--------------------------------------------------------------------------------\n",
      "Kuka on tärkeitä henkilöitä Eilalle?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "✅ SUCCESS | Collection Reload\n",
      "  └─ Reloaded collection: document_embeddings\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmp63_9cnfk\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmp63_9cnfk/_remote_module_non_scriptable.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCESS | Collection Reload\n",
      "  └─ Reloaded collection: document_embeddings\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2092 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VASTAUS:\n",
      "----------------------------------------\n",
      "Tehtävä: Etsi tarkka vastaus annettuun kysymykseen käyttäen vain alla olevaa kontekstia.\n",
      "    \n",
      "            Kysymys: Etsi tarkka vastaus seuraavaan kysymykseen käyttäen alla olevia dokumentteja.\n",
      "    \n",
      "    Kysymys: Kuka on tärkeitä henkilöitä Eilalle?\n",
      "    \n",
      "    Dokumentit:\n",
      "    \n",
      "Dokumentti 1 (C5-50):\n",
      "Lopuksi Koen, että on tärkeää saada sosiaalista kanssakäymistä.\n",
      "Silloin päivä menee hyvin.\n",
      "\n",
      "Dokumentti 2 (KL-51):\n",
      "Lisäksi omistan mielenkiintoisia kirjoja, joita voin lukea useampaankin kertaan, esimer...\n",
      "----------------------------------------\n",
      "\n",
      "Vastaus:\n",
      "Tehtävä: Etsi tarkka vastaus annettuun kysymykseen käyttäen vain alla olevaa kontekstia.\n",
      "    \n",
      "            Kysymys: Etsi tarkka vastaus seuraavaan kysymykseen käyttäen alla olevia dokumentteja.\n",
      "    \n",
      "    Kysymys: Kuka on tärkeitä henkilöitä Eilalle?\n",
      "    \n",
      "    Dokumentit:\n",
      "    \n",
      "Dokumentti 1 (C5-50):\n",
      "Lopuksi Koen, että on tärkeää saada sosiaalista kanssakäymistä.\n",
      "Silloin päivä menee hyvin.\n",
      "\n",
      "Dokumentti 2 (KL-51):\n",
      "Lisäksi omistan mielenkiintoisia kirjoja, joita voin lukea useampaankin kertaan, esimer...\n",
      "\n",
      "Lähde:\n",
      "- C5-50: Lopuksi Koen, että on tärkeää saada sosiaalista kanssakäymistä.\n",
      "Silloin päivä menee hyvin....\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Kysymys 2/5:\n",
      "--------------------------------------------------------------------------------\n",
      "Kuinka vanha on Sulo ja mikä on hänen syntymävuotensa?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "✅ SUCCESS | Collection Reload\n",
      "  └─ Reloaded collection: document_embeddings\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Collection Reload\n",
      "  └─ Reloaded collection: document_embeddings\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "VASTAUS:\n",
      "----------------------------------------\n",
      "Tehtävä: Etsi tarkka vastaus annettuun kysymykseen käyttäen vain alla olevaa kontekstia.\n",
      "    \n",
      "            Kysymys: Etsi tarkka vastaus seuraavaan kysymykseen käyttäen alla olevia dokumentteja.\n",
      "    \n",
      "    Kysymys: Kuinka vanha on Sulo ja mikä on hänen syntymävuotensa?\n",
      "    \n",
      "    Dokumentit:\n",
      "    \n",
      "Dokumentti 1 (KL-51):\n",
      "Lisäksi omistan mielenkiintoisia kirjoja, joita voin lukea useampaankin kertaan, esimerkiksi Juhani Ahon Papin tytär-papin rouva.\n",
      "Olen käynyt aiemmin taidenäyttelyissä ja musiikkia ra...\n",
      "----------------------------------------\n",
      "\n",
      "Vastaus:\n",
      "Tehtävä: Etsi tarkka vastaus annettuun kysymykseen käyttäen vain alla olevaa kontekstia.\n",
      "    \n",
      "            Kysymys: Etsi tarkka vastaus seuraavaan kysymykseen käyttäen alla olevia dokumentteja.\n",
      "    \n",
      "    Kysymys: Kuinka vanha on Sulo ja mikä on hänen syntymävuotensa?\n",
      "    \n",
      "    Dokumentit:\n",
      "    \n",
      "Dokumentti 1 (KL-51):\n",
      "Lisäksi omistan mielenkiintoisia kirjoja, joita voin lukea useampaankin kertaan, esimerkiksi Juhani Ahon Papin tytär-papin rouva.\n",
      "Olen käynyt aiemmin taidenäyttelyissä ja musiikkia ra...\n",
      "\n",
      "Lähde:\n",
      "- KL-51: Lisäksi omistan mielenkiintoisia kirjoja, joita voin lukea useampaankin kertaan, esimerkiksi Juhani Ahon Papin tytär-papin rouva.\n",
      "Olen käynyt aiemmin ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Kysymys 3/5:\n",
      "--------------------------------------------------------------------------------\n",
      "Mitä Eila harrastaa?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "✅ SUCCESS | Collection Reload\n",
      "  └─ Reloaded collection: document_embeddings\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Collection Reload\n",
      "  └─ Reloaded collection: document_embeddings\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "❌ FAILED | Query\n",
      "  └─ CUDA error: unknown error\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Error in response generation: CUDA error: unknown error\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "VASTAUS:\n",
      "----------------------------------------\n",
      "Virhe vastauksen muodostamisessa.\n",
      "----------------------------------------\n",
      "\n",
      "Vastaus:\n",
      "Virhe vastauksen muodostamisessa.\n",
      "\n",
      "Lähde:\n",
      "- KL-51: Lisäksi omistan mielenkiintoisia kirjoja, joita voin lukea useampaankin kertaan, esimerkiksi Juhani Ahon Papin tytär-papin rouva.\n",
      "Olen käynyt aiemmin ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Kysymys 4/5:\n",
      "--------------------------------------------------------------------------------\n",
      "Miten Sulo liikkuu?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "✅ SUCCESS | Collection Reload\n",
      "  └─ Reloaded collection: document_embeddings\n",
      "\n",
      "❌ FAILED | Embedding Generation\n",
      "  └─ CUDA error: unknown error\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "❌ FAILED | Query\n",
      "  └─ CUDA error: unknown error\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Error processing query: CUDA error: unknown error\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Virhe kysymyksen käsittelyssä: CUDA error: unknown error\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "Kysymys 5/5:\n",
      "--------------------------------------------------------------------------------\n",
      "Mistä asioista Eila pitää?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "✅ SUCCESS | Collection Reload\n",
      "  └─ Reloaded collection: document_embeddings\n",
      "\n",
      "❌ FAILED | Embedding Generation\n",
      "  └─ CUDA error: unknown error\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "❌ FAILED | Query\n",
      "  └─ CUDA error: unknown error\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Error processing query: CUDA error: unknown error\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Virhe kysymyksen käsittelyssä: CUDA error: unknown error\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "5. Testaus valmis\n",
      "\n",
      "Starting Neo4j Enhanced RAG test execution...\n",
      "\n",
      "1. Initializing base pipeline...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d3b1a3b43f44b780f5501413e3fa96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "❌ FAILED | LLM Pipeline\n",
      "  └─ CUDA error: unknown error\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "❌ FAILED | RAG Pipeline\n",
      "  └─ CUDA error: unknown error\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "Virhe Neo4j testauksessa: CUDA error: unknown error\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "Error in comparison testing: CUDA error: unknown error\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 589\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;66;03m# Instead of running run_improved_rag_tests() directly, use this:\u001b[39;00m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 589\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_comparison_tests\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 565\u001b[0m, in \u001b[0;36mrun_comparison_tests\u001b[0;34m()\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m4. Running comparison tests...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    564\u001b[0m standard_results \u001b[38;5;241m=\u001b[39m run_improved_rag_tests()  \u001b[38;5;66;03m# Keep this\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m neo4j_results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_neo4j_enhanced_tests\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# And this\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;66;03m# Print comparison\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m5. Results Comparison:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 400\u001b[0m, in \u001b[0;36mrun_neo4j_enhanced_tests\u001b[0;34m()\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;66;03m# Initialize base pipeline\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m1. Initializing base pipeline...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 400\u001b[0m base_pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mRAGPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   ✓ Base pipeline initialized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    403\u001b[0m \u001b[38;5;66;03m# Initialize Neo4j enhanced pipeline\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 285\u001b[0m, in \u001b[0;36mRAGPipeline.__init__\u001b[0;34m(self, model_id)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_id: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinnish-NLP/llama-7b-finnish-instruct-v0.2\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m         print_status(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLM Setup\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc_processor \u001b[38;5;241m=\u001b[39m DocumentProcessor()\n",
      "Cell \u001b[0;32mIn[2], line 368\u001b[0m, in \u001b[0;36mRAGPipeline.setup_llm\u001b[0;34m(self, model_id)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m     bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m    362\u001b[0m         load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    363\u001b[0m         bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m    364\u001b[0m         bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    365\u001b[0m         bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    366\u001b[0m     )\n\u001b[0;32m--> 368\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m6GiB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moffload\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m    379\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    380\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    386\u001b[0m         repetition_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.15\u001b[39m\n\u001b[1;32m    387\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py:4225\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4216\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   4218\u001b[0m     (\n\u001b[1;32m   4219\u001b[0m         model,\n\u001b[1;32m   4220\u001b[0m         missing_keys,\n\u001b[1;32m   4221\u001b[0m         unexpected_keys,\n\u001b[1;32m   4222\u001b[0m         mismatched_keys,\n\u001b[1;32m   4223\u001b[0m         offload_index,\n\u001b[1;32m   4224\u001b[0m         error_msgs,\n\u001b[0;32m-> 4225\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   4229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4232\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4233\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4236\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4237\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4243\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4245\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   4246\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py:4728\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)\u001b[0m\n\u001b[1;32m   4724\u001b[0m                 set_module_tensor_to_device(\n\u001b[1;32m   4725\u001b[0m                     model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   4726\u001b[0m                 )\n\u001b[1;32m   4727\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4728\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4729\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4730\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4731\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4732\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4733\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4734\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4735\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4736\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4737\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4738\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4739\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4740\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4741\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4742\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4743\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4744\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4745\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4746\u001b[0m     \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py:993\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\u001b[0m\n\u001b[1;32m    990\u001b[0m         param_device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;66;03m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[39;00m\n\u001b[0;32m--> 993\u001b[0m     \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mset_module_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    995\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/accelerate/utils/modeling.py:404\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    402\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 404\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    406\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Add these imports at the top of your file\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from typing import List, Dict, Any, Optional\n",
    "import re\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "\n",
    "class FinnishRAGAgent:\n",
    "    def __init__(self, base_pipeline: RAGPipeline):\n",
    "        self.pipeline = base_pipeline\n",
    "        self.memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "        self.setup_tools()\n",
    "        \n",
    "    def setup_tools(self):\n",
    "        \"\"\"Initialize search and analysis tools.\"\"\"\n",
    "        self.tools = {\n",
    "            \"semantic_search\": self._semantic_search,\n",
    "            \"exact_match\": self._exact_match_search,\n",
    "            \"context_analysis\": self._analyze_context\n",
    "        }\n",
    "    \n",
    "    def _semantic_search(self, query: str) -> List[Dict]:\n",
    "        \"\"\"Enhanced semantic search with Finnish preprocessing.\"\"\"\n",
    "        preprocessed_query = self._preprocess_finnish_text(query)\n",
    "        base_results = self.pipeline.query(preprocessed_query, top_k=5)\n",
    "        \n",
    "        # Enhance results with Finnish-specific scoring\n",
    "        enhanced_results = []\n",
    "        for result in base_results['sources']:\n",
    "            score = self._calculate_finnish_relevance(preprocessed_query, result['text'])\n",
    "            enhanced_results.append({\n",
    "                'text': result['text'],\n",
    "                'score': score * result['score'],  # Combine scores\n",
    "                'source': result['document_id'],\n",
    "                'metadata': {\n",
    "                    'person_name': result['person_name'],\n",
    "                    'chunk_index': result['chunk_index']\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        return sorted(enhanced_results, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "    def _exact_match_search(self, query: str) -> List[Dict]:\n",
    "        \"\"\"Direct text matching with Finnish normalization.\"\"\"\n",
    "        normalized_query = self._normalize_finnish_text(query)\n",
    "        \n",
    "        # Use pipeline's collection directly\n",
    "        collection = self.pipeline.collection\n",
    "        results = collection.query(\n",
    "            expr=f'text like \"%{normalized_query}%\"',\n",
    "            output_fields=[\"text\", \"person_name\", \"document_id\", \"chunk_index\"]\n",
    "        )\n",
    "        \n",
    "        return [\n",
    "            {\n",
    "                'text': r['text'],\n",
    "                'score': 1.0, \n",
    "                'source': r['document_id'],\n",
    "                'metadata': {'person_name': r['person_name']}\n",
    "            }\n",
    "            for r in results\n",
    "        ]\n",
    "\n",
    "    def _analyze_context(self, passages: List[Dict]) -> Dict:\n",
    "        \"\"\"Analyze relationships and context in Finnish text.\"\"\"\n",
    "        context_data = {\n",
    "            'entities': set(),\n",
    "            'relationships': [],\n",
    "            'temporal_refs': [],\n",
    "            'key_topics': set()\n",
    "        }\n",
    "        \n",
    "        for passage in passages:\n",
    "            text = passage['text']\n",
    "            \n",
    "            # Extract Finnish names and entities\n",
    "            entities = self._extract_finnish_entities(text)\n",
    "            context_data['entities'].update(entities)\n",
    "            \n",
    "            # Find relationships\n",
    "            relationships = self._find_relationships(text)\n",
    "            context_data['relationships'].extend(relationships)\n",
    "            \n",
    "            # Extract temporal information\n",
    "            temporal = self._extract_temporal_refs(text)\n",
    "            context_data['temporal_refs'].extend(temporal)\n",
    "            \n",
    "        return context_data\n",
    "\n",
    "    def _preprocess_finnish_text(self, text: str) -> str:\n",
    "        \"\"\"Preprocess Finnish text for better matching.\"\"\"\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text.strip())\n",
    "        \n",
    "        # Handle common Finnish abbreviations\n",
    "        abbreviations = {\n",
    "            'esim.': 'esimerkiksi',\n",
    "            'ns.': 'niin sanottu',\n",
    "            'jne.': 'ja niin edelleen'\n",
    "        }\n",
    "        for abbr, full in abbreviations.items():\n",
    "            text = text.replace(abbr, full)\n",
    "        \n",
    "        return text\n",
    "\n",
    "    # Add these methods to your FinnishRAGAgent class\n",
    "\n",
    "    def _find_relationships(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Find relationships in Finnish text.\"\"\"\n",
    "        relationships = []\n",
    "        \n",
    "        # Common Finnish relationship patterns\n",
    "        patterns = [\n",
    "            (r'(\\w+)\\s+on\\s+(\\w+)\\s+ystävä', 'ystävyys'),\n",
    "            (r'(\\w+)\\s+asuu\\s+(\\w+)', 'asuminen'),\n",
    "            (r'(\\w+)\\s+tekee\\s+(\\w+)', 'toiminta'),\n",
    "            (r'(\\w+)\\s+pitää\\s+(\\w+)', 'pitäminen'),\n",
    "            (r'(\\w+)\\s+kanssa', 'yhteys'),\n",
    "            (r'(\\w+)\\s+tärkeä', 'tärkeys'),\n",
    "            (r'(\\w+)\\s+auttaa\\s+(\\w+)', 'auttaminen')\n",
    "        ]\n",
    "        \n",
    "        for pattern, rel_type in patterns:\n",
    "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                relationships.append({\n",
    "                    'type': rel_type,\n",
    "                    'entities': match.groups(),\n",
    "                    'text': match.group(0)\n",
    "                })\n",
    "        \n",
    "        return relationships\n",
    "\n",
    "    def _extract_temporal_refs(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Extract temporal references from Finnish text.\"\"\"\n",
    "        temporal_refs = []\n",
    "        \n",
    "        # Common Finnish temporal patterns\n",
    "        patterns = [\n",
    "            (r'\\d+\\s*vuotta', 'duration'),\n",
    "            (r'\\d+\\s*vuotias', 'age'),\n",
    "            (r'(maanantai|tiistai|keskiviikko|torstai|perjantai|lauantai|sunnuntai)', 'weekday'),\n",
    "            (r'(tammikuu|helmikuu|maaliskuu|huhtikuu|toukokuu|kesäkuu|heinäkuu|elokuu|syyskuu|lokakuu|marraskuu|joulukuu)', 'month'),\n",
    "            (r'(aamu|päivä|ilta|yö)', 'time_of_day'),\n",
    "            (r'(eilen|tänään|huomenna)', 'relative_day'),\n",
    "            (r'(viikko|kuukausi|vuosi)', 'time_unit')\n",
    "        ]\n",
    "        \n",
    "        for pattern, ref_type in patterns:\n",
    "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                temporal_refs.append({\n",
    "                    'type': ref_type,\n",
    "                    'text': match.group(0),\n",
    "                    'position': match.span()\n",
    "                })\n",
    "        \n",
    "        return temporal_refs\n",
    "    \n",
    "    def _identify_key_topics(self, text: str) -> set:\n",
    "        \"\"\"Identify key topics in Finnish text.\"\"\"\n",
    "        topics = set()\n",
    "        \n",
    "        # Common Finnish topic indicators\n",
    "        key_patterns = [\n",
    "            (r'tärkeä\\w*\\s+(\\w+)', 'importance'),\n",
    "            (r'harrastaa\\w*\\s+(\\w+)', 'hobby'),\n",
    "            (r'pitää\\w*\\s+(\\w+)', 'preference'),\n",
    "            (r'ongelma\\w*\\s+(\\w+)', 'problem'),\n",
    "            (r'tavoite\\w*\\s+(\\w+)', 'goal')\n",
    "        ]\n",
    "        \n",
    "        for pattern, topic_type in key_patterns:\n",
    "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                if len(match.groups()) > 0:\n",
    "                    topics.add(f\"{topic_type}:{match.group(1)}\")\n",
    "        \n",
    "        return topics\n",
    "\n",
    "    def _normalize_finnish_text(self, text: str) -> str:\n",
    "        \"\"\"Normalize Finnish text for comparison.\"\"\"\n",
    "        text = text.lower().strip()\n",
    "        text = re.sub(r'[^\\w\\s\\äöåÄÖÅ]', '', text)\n",
    "        return text\n",
    "\n",
    "    def _calculate_finnish_relevance(self, query: str, text: str) -> float:\n",
    "        \"\"\"Calculate relevance score for Finnish text.\"\"\"\n",
    "        score = 1.0\n",
    "        \n",
    "        # Boost score for Finnish grammar patterns\n",
    "        if re.search(r'\\b(ssa|ssä|sta|stä|lla|llä|lta|ltä)\\b', text):\n",
    "            score *= 1.1\n",
    "            \n",
    "        # Boost for question-answer pairs\n",
    "        if '?' in query and '.' in text[:100]:\n",
    "            score *= 1.2\n",
    "            \n",
    "        # Check for named entity matches\n",
    "        query_entities = self._extract_finnish_entities(query)\n",
    "        text_entities = self._extract_finnish_entities(text)\n",
    "        if query_entities & text_entities:\n",
    "            score *= 1.3\n",
    "            \n",
    "        return min(1.0, score)\n",
    "\n",
    "    def _extract_finnish_entities(self, text: str) -> set:\n",
    "        \"\"\"Extract Finnish named entities.\"\"\"\n",
    "        entities = set()\n",
    "        \n",
    "        # Match Finnish names (simplified)\n",
    "        name_pattern = r'\\b[A-ZÄÖÅ][a-zäöå]+\\b'\n",
    "        entities.update(re.findall(name_pattern, text))\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def _generate_response(self, query: str, results: List[Dict], context: Dict) -> Dict:\n",
    "        \"\"\"Generate an enhanced response with proper citations.\"\"\"\n",
    "        if not results:\n",
    "            return {\n",
    "                \"answer\": \"En löytänyt vastausta kysymykseesi saatavilla olevista dokumenteista.\",\n",
    "                \"confidence\": 0.0,\n",
    "                \"sources\": []\n",
    "            }\n",
    "    \n",
    "        try:\n",
    "            # Build context-rich query\n",
    "            enhanced_query = self._build_enhanced_query(query, results)\n",
    "            \n",
    "            # Get response from base pipeline\n",
    "            response = self.pipeline.query(enhanced_query)\n",
    "            \n",
    "            # Process and validate response\n",
    "            answer = self._process_response(response, results, query)\n",
    "            \n",
    "            return {\n",
    "                \"answer\": answer,\n",
    "                \"confidence\": max(r['score'] for r in results),\n",
    "                \"sources\": results[:3],\n",
    "                \"context_analysis\": context\n",
    "            }\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in response generation: {str(e)}\")\n",
    "            return {\n",
    "                \"answer\": \"Virhe vastauksen muodostamisessa.\",\n",
    "                \"confidence\": 0.0,\n",
    "                \"sources\": results[:3],\n",
    "                \"context_analysis\": context\n",
    "            }\n",
    "\n",
    "    def _build_enhanced_query(self, query: str, results: List[Dict]) -> str:\n",
    "        \"\"\"Build a context-rich query with relevant information.\"\"\"\n",
    "        # Analyze query intent\n",
    "        query_type = self._analyze_query_type(query)\n",
    "        \n",
    "        # Build appropriate context structure based on query type\n",
    "        context = f\"\"\"Etsi tarkka vastaus seuraavaan kysymykseen käyttäen alla olevia dokumentteja.\n",
    "    \n",
    "    Kysymys: {query}\n",
    "    \n",
    "    Dokumentit:\n",
    "    \"\"\"\n",
    "        \n",
    "        # Add relevant documents with source information\n",
    "        for i, result in enumerate(results[:3], 1):\n",
    "            context += f\"\\nDokumentti {i} ({result['source']}):\\n{result['text'].strip()}\\n\"\n",
    "    \n",
    "        # Add query-specific instructions\n",
    "        context += self._get_query_instructions(query_type)\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def _analyze_query_type(self, query: str) -> str:\n",
    "        \"\"\"Analyze the type of query for better response structuring.\"\"\"\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        # Extract query characteristics without hardcoding specific names or values\n",
    "        characteristics = {\n",
    "            'personal_info': any(word in query_lower for word in ['kuka', 'kenen', 'kenelle']),\n",
    "            'age_related': any(word in query_lower for word in ['vanha', 'ikä', 'syntymä', 'vuosi']),\n",
    "            'activity_related': any(word in query_lower for word in ['tekee', 'harrastaa', 'pitää', 'tykkää']),\n",
    "            'ability_related': any(word in query_lower for word in ['pystyy', 'osaa', 'liikkuu', 'käyttää']),\n",
    "            'preference_related': any(word in query_lower for word in ['pitää', 'tykkää', 'haluaa']),\n",
    "            'relationship_related': any(word in query_lower for word in ['ystävä', 'tärkeä', 'läheinen'])\n",
    "        }\n",
    "        \n",
    "        # Return the most likely query type\n",
    "        return max(characteristics.items(), key=lambda x: x[1])[0]\n",
    "    \n",
    "    def _get_query_instructions(self, query_type: str) -> str:\n",
    "        \"\"\"Get specific instructions based on query type.\"\"\"\n",
    "        instructions = {\n",
    "            'personal_info': \"\"\"\n",
    "        Ohje: \n",
    "        1. Etsi henkilöön liittyvät suorat maininnat\n",
    "        2. Käytä suoria lainauksia henkilöiden nimistä ja suhteista\n",
    "        3. Mainitse dokumentin lähde\"\"\",\n",
    "                \n",
    "                'age_related': \"\"\"\n",
    "        Ohje:\n",
    "        1. Etsi tarkat ikä- ja syntymävuositiedot\n",
    "        2. Ilmoita sekä ikä että syntymävuosi jos molemmat löytyvät\n",
    "        3. Mainitse dokumentin lähde\"\"\",\n",
    "                \n",
    "                'activity_related': \"\"\"\n",
    "        Ohje:\n",
    "        1. Etsi kaikki mainitut aktiviteetit ja harrastukset\n",
    "        2. Käytä suoria lainauksia aktiviteettien kuvauksista\n",
    "        3. Mainitse dokumentin lähde\"\"\",\n",
    "                \n",
    "                'ability_related': \"\"\"\n",
    "        Ohje:\n",
    "        1. Etsi kuvaukset henkilön kyvyistä ja toiminnasta\n",
    "        2. Käytä suoria lainauksia toimintakyvyn kuvauksista\n",
    "        3. Mainitse dokumentin lähde\"\"\",\n",
    "                \n",
    "                'preference_related': \"\"\"\n",
    "        Ohje:\n",
    "        1. Etsi kaikki mainitut mieltymykset ja kiinnostukset\n",
    "        2. Käytä suoria lainauksia mieltymysten kuvauksista\n",
    "        3. Mainitse dokumentin lähde\"\"\",\n",
    "                \n",
    "                'relationship_related': \"\"\"\n",
    "        Ohje:\n",
    "        1. Etsi kuvaukset ihmissuhteista ja tärkeistä henkilöistä\n",
    "        2. Käytä suoria lainauksia suhteiden kuvauksista\n",
    "        3. Mainitse dokumentin lähde\"\"\"\n",
    "            }\n",
    "            \n",
    "        return instructions.get(query_type, \"\"\"\n",
    "        Ohje:\n",
    "        1. Etsi suora vastaus kysymykseen dokumenteista\n",
    "        2. Käytä suoria lainauksia\n",
    "        3. Mainitse dokumentin lähde\"\"\")\n",
    "    \n",
    "    def _process_response(self, response: Dict, results: List[Dict], query: str) -> str:\n",
    "        \"\"\"Process and validate the response.\"\"\"\n",
    "        if not response or 'answer' not in response:\n",
    "            return \"En löytänyt vastausta annetusta kontekstista.\"\n",
    "        \n",
    "        answer = response['answer'].strip()\n",
    "        \n",
    "        # Validate the answer has proper citations\n",
    "        if not any(f\"Dokumentin {result['source']}\" in answer for result in results):\n",
    "            # Try to add source information if missing\n",
    "            for result in results:\n",
    "                if any(quote in result['text'] for quote in re.findall(r'\"([^\"]*)\"', answer)):\n",
    "                    answer = f\"Dokumentin {result['source']} mukaan {answer}\"\n",
    "                    break\n",
    "        \n",
    "        # Validate answer has quotes\n",
    "        if '\"' not in answer and any(result['text'] in answer for result in results):\n",
    "            answer = re.sub(r'(Dokumentin [^\\s]+ mukaan) (.*)', r'\\1 \"\\2\"', answer)\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def process_query(self, query: str) -> Dict:\n",
    "        \"\"\"Process a query using all available tools.\"\"\"\n",
    "        try:\n",
    "            # Step 1: Get semantic search results\n",
    "            semantic_results = self._semantic_search(query)\n",
    "            \n",
    "            # Step 2: Check for exact matches if needed\n",
    "            if not semantic_results or max(r['score'] for r in semantic_results) < 0.5:\n",
    "                exact_matches = self._exact_match_search(query)\n",
    "                all_results = semantic_results + exact_matches\n",
    "            else:\n",
    "                all_results = semantic_results\n",
    "            \n",
    "            # Step 3: Analyze context\n",
    "            context_data = self._analyze_context(all_results[:3])\n",
    "            \n",
    "            # Step 4: Generate enhanced response\n",
    "            response = self._generate_response(query, all_results, context_data)\n",
    "            \n",
    "            # Store in memory for future context\n",
    "            self.memory.save_context(\n",
    "                {\"input\": query},\n",
    "                {\"output\": response['answer']}\n",
    "            )\n",
    "            \n",
    "            # Print formatted response for immediate feedback\n",
    "            print(\"\\nVASTAUS:\")\n",
    "            print(\"-\" * 40)\n",
    "            print(response['answer'])\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {str(e)}\")\n",
    "            raise\n",
    "def run_neo4j_enhanced_tests():\n",
    "    try:\n",
    "        print(\"\\nStarting Neo4j Enhanced RAG test execution...\")\n",
    "        \n",
    "        # Initialize base pipeline\n",
    "        print(\"\\n1. Initializing base pipeline...\")\n",
    "        base_pipeline = RAGPipeline()\n",
    "        print(\"   ✓ Base pipeline initialized\")\n",
    "        \n",
    "        # Initialize Neo4j enhanced pipeline\n",
    "        print(\"\\n2. Creating Neo4j Enhanced Pipeline...\")\n",
    "        neo4j_rag = Neo4jEnhancedRAGPipeline(\n",
    "            base_rag_pipeline=base_pipeline,\n",
    "            neo4j_uri=\"neo4j://neo4j:7687\"  # Update with your Neo4j URI\n",
    "        )\n",
    "        print(\"   ✓ Neo4j Enhanced pipeline created\")\n",
    "        \n",
    "        # Process documents\n",
    "        print(\"\\n3. Processing documents...\")\n",
    "        folder_path = '/home/jovyan/work/notebooks/data/'  # Update with your actual path\n",
    "        neo4j_rag.process_documents(folder_path)\n",
    "        print(\"   ✓ Documents processed with Neo4j enhancement\")\n",
    "        \n",
    "        # Define test questions\n",
    "        test_questions = [\n",
    "            \"Onko Marjatta Eilan ystävä?\",\n",
    "            \"Miten Sulo kokee sosiaalisen kanssakäymisen merkityksen?\",\n",
    "            \"Montako sisarusta Sulolla on?\",\n",
    "            \"Millainen on Eilan arki?\",\n",
    "            \"Mikä on Sulolle tärkeää?\"\n",
    "        ]\n",
    "        \n",
    "        # Run tests\n",
    "        print(\"\\n4. Running test questions with Neo4j enhancement...\")\n",
    "        results = []\n",
    "        for i, question in enumerate(test_questions, 1):\n",
    "            try:\n",
    "                print(f\"\\nKysymys {i}/{len(test_questions)}:\")\n",
    "                print(\"-\"*80)\n",
    "                print(f\"{question}\")\n",
    "                print(\"-\"*80)\n",
    "                \n",
    "                # Get results from Neo4j enhanced pipeline\n",
    "                response = neo4j_rag.query(question)\n",
    "                results.append({\n",
    "                    \"question\": question,\n",
    "                    \"response\": response\n",
    "                })\n",
    "                \n",
    "                # Print results\n",
    "                print(\"\\nNeo4j Enhanced Vastaus:\")\n",
    "                print(response.get('answer', 'Ei vastausta'))\n",
    "                print(\"\\nLähteet ja konteksti:\")\n",
    "                print(\"- Dokumenttilähteet:\")\n",
    "                for src in response.get('sources', [])[:2]:\n",
    "                    print(f\"  • {src['document_id']}: {src['text'][:100]}...\")\n",
    "                print(\"- Graafikonteksti:\")\n",
    "                if response.get('person_context'):\n",
    "                    print(f\"  • Henkilö: {response['person_context']['name']}\")\n",
    "                    print(f\"  • Suhteet: {', '.join(response['person_context'].get('relationship_types', []))}\")\n",
    "                print(\"-\"*80)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Virhe kysymyksen käsittelyssä: {str(e)}\")\n",
    "        \n",
    "        print(\"\\n5. Neo4j Enhanced testaus valmis\")\n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nVirhe Neo4j testauksessa: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def run_improved_rag_tests():\n",
    "    try:\n",
    "        print(\"\\nStarting RAG test execution...\")\n",
    "        \n",
    "        # Initialize base pipeline\n",
    "        print(\"\\n1. Initializing base pipeline...\")\n",
    "        base_pipeline = RAGPipeline()\n",
    "        print(\"   ✓ Base pipeline initialized\")\n",
    "        \n",
    "        # Initialize Finnish RAG Agent\n",
    "        print(\"\\n2. Creating Finnish RAG Agent...\")\n",
    "        agent = FinnishRAGAgent(base_pipeline)\n",
    "        print(\"   ✓ Agent created\")\n",
    "        \n",
    "        # Process documents\n",
    "        print(\"\\n3. Processing documents...\")\n",
    "        folder_path = '/home/jovyan/work/notebooks/data/'\n",
    "        base_pipeline.process_documents(folder_path)\n",
    "        print(\"   ✓ Documents processed\")\n",
    "        \n",
    "        # Define test questions\n",
    "        test_questions = [\n",
    "            \"Kuka on tärkeitä henkilöitä Eilalle?\",\n",
    "            \"Kuinka vanha on Sulo ja mikä on hänen syntymävuotensa?\",\n",
    "            \"Mitä Eila harrastaa?\",\n",
    "            \"Miten Sulo liikkuu?\",\n",
    "            \"Mistä asioista Eila pitää?\"\n",
    "        ]\n",
    "        \n",
    "        # Run tests through the agent\n",
    "        print(\"\\n4. Running test questions...\")\n",
    "        results = []\n",
    "        for i, question in enumerate(test_questions, 1):\n",
    "            try:\n",
    "                print(f\"\\nKysymys {i}/{len(test_questions)}:\")\n",
    "                print(\"-\"*80)\n",
    "                print(f\"{question}\")\n",
    "                print(\"-\"*80)\n",
    "                \n",
    "                response = agent.process_query(question)\n",
    "                results.append({\n",
    "                    \"question\": question,\n",
    "                    \"response\": response\n",
    "                })\n",
    "                \n",
    "                print(\"\\nVastaus:\")\n",
    "                print(response.get('answer', 'Ei vastausta'))\n",
    "                print(\"\\nLähde:\")\n",
    "                for src in response.get('sources', [])[:1]:\n",
    "                    print(f\"- {src['source']}: {src['text'][:150]}...\")\n",
    "                print(\"-\"*80)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Virhe kysymyksen käsittelyssä: {str(e)}\")\n",
    "        \n",
    "        print(\"\\n5. Testaus valmis\")\n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nVirhe testauksessa: {str(e)}\")\n",
    "        raise\n",
    "   \n",
    "\n",
    "def run_comparison_tests():\n",
    "    \"\"\"Run both standard and Neo4j enhanced tests for comparison.\"\"\"\n",
    "    try:\n",
    "        print(\"\\nStarting comparison test execution...\")\n",
    "        \n",
    "        # Initialize base pipeline\n",
    "        print(\"\\n1. Initializing base pipeline...\")\n",
    "        base_pipeline = RAGPipeline()\n",
    "        print(\"   ✓ Base pipeline initialized\")\n",
    "        \n",
    "        # Initialize Neo4j enhanced pipeline\n",
    "        print(\"\\n2. Creating Neo4j Enhanced Pipeline...\")\n",
    "        neo4j_rag = Neo4jEnhancedRAGPipeline(\n",
    "            base_rag_pipeline=base_pipeline,\n",
    "            neo4j_uri=\"neo4j://neo4j:7687\"\n",
    "        )\n",
    "        print(\"   ✓ Neo4j Enhanced pipeline created\")\n",
    "        \n",
    "        # Process documents (only need to do this once)\n",
    "        print(\"\\n3. Processing documents...\")\n",
    "        folder_path = '/home/jovyan/work/notebooks/data/'\n",
    "        neo4j_rag.process_documents(folder_path)\n",
    "        print(\"   ✓ Documents processed\")\n",
    "        \n",
    "        # Define test questions\n",
    "        test_questions = [\n",
    "            \"Onko Marjatta Eilan ystävä?\"\n",
    "        ]\n",
    "        \n",
    "        # Run both types of tests\n",
    "        print(\"\\n4. Running comparison tests...\")\n",
    "        standard_results = run_improved_rag_tests()  # Keep this\n",
    "        neo4j_results = run_neo4j_enhanced_tests()   # And this\n",
    "        \n",
    "        # Print comparison\n",
    "        print(\"\\n5. Results Comparison:\")\n",
    "        print(\"-\" * 80)\n",
    "        for std, neo in zip(standard_results, neo4j_results):\n",
    "            print(f\"\\nQuestion: {std['question']}\")\n",
    "            print(\"\\nStandard RAG Answer:\")\n",
    "            print(std[\"response\"][\"answer\"])\n",
    "            print(\"\\nNeo4j Enhanced Answer:\")\n",
    "            print(neo[\"response\"][\"answer\"])\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "        return {\n",
    "            \"standard_results\": standard_results,\n",
    "            \"neo4j_results\": neo4j_results\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in comparison testing: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Instead of running run_improved_rag_tests() directly, use this:\n",
    "if __name__ == \"__main__\":\n",
    "    results = run_comparison_tests()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
