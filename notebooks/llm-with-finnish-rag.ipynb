{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6076b32b-7828-476a-91df-4dcade6e6559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/opt/conda/lib/python3.11/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCESS | CUDA Check\n",
      "  └─ Using GPU: NVIDIA GeForce RTX 2070 SUPER\n",
      "\n",
      "✅ SUCCESS | NLTK Setup\n",
      "  └─ Downloaded finnish stopwords\n",
      "\n",
      "✅ SUCCESS | Embedding Dimension\n",
      "  └─ Using dimension: 1536\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "import logging\n",
    "from typing import List, Dict, Any\n",
    "from docx import Document\n",
    "from tqdm.notebook import tqdm\n",
    "from pymilvus import connections, Collection, utility, CollectionSchema, FieldSchema, DataType\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "## Env \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "\n",
    "# Utility Functions\n",
    "def print_status(section_name: str, status: bool, message: str = \"\"):\n",
    "    \"\"\"Print status of a section with colored output.\"\"\"\n",
    "    status_str = \"✅ SUCCESS\" if status else \"❌ FAILED\"\n",
    "    print(f\"\\n{status_str} | {section_name}\")\n",
    "    if message:\n",
    "        print(f\"  └─ {message}\")\n",
    "\n",
    "def verify_section(section_number: int, verification_func) -> bool:\n",
    "    \"\"\"Verify if a section was executed successfully.\"\"\"\n",
    "    try:\n",
    "        result = verification_func()\n",
    "        print_status(f\"Section {section_number} Verification\", True, \"Successfully executed\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print_status(f\"Section {section_number} Verification\", False, f\"Error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def get_model_embedding_dim(model_name: str = \"TurkuNLP/bert-base-finnish-cased-v1\") -> int:\n",
    "    \"\"\"Get embedding dimension from model config.\"\"\"\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    return model.config.hidden_size\n",
    "\n",
    "def check_cuda():\n",
    "    \"\"\"Check CUDA availability and print status.\"\"\"\n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            device_name = torch.cuda.get_device_name(0)\n",
    "            print_status(\"CUDA Check\", True, f\"Using GPU: {device_name}\")\n",
    "            return True\n",
    "        else:\n",
    "            print_status(\"CUDA Check\", True, \"Using CPU\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print_status(\"CUDA Check\", False, str(e))\n",
    "        return False\n",
    "\n",
    "def ensure_stopwords_downloaded(language='finnish'):\n",
    "    \"\"\"Download NLTK stopwords and print status.\"\"\"\n",
    "    try:\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "        print_status(\"NLTK Setup\", True, f\"Downloaded {language} stopwords\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print_status(\"NLTK Setup\", False, str(e))\n",
    "        return False\n",
    "\n",
    "# Global Constants\n",
    "#EMBEDDING_DIM = get_model_embedding_dim()\n",
    "EMBEDDING_DIM = 1536\n",
    "MILVUS_HOST = \"milvus-standalone\"\n",
    "MILVUS_PORT = \"19530\"\n",
    "MILVUS_ALIAS = \"default\"\n",
    "\n",
    "# Initial Setup Verification\n",
    "def verify_initial_setup():\n",
    "    check_cuda()\n",
    "    ensure_stopwords_downloaded()\n",
    "    print_status(\"Embedding Dimension\", True, f\"Using dimension: {EMBEDDING_DIM}\")\n",
    "    \n",
    "# Run initial verification\n",
    "verify_initial_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e32ba9a-93cc-4bf9-a81b-2cede41cd7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCESS | Document Processor\n",
      "  └─ Initialized with Finnish-optimized settings\n",
      "\n",
      "✅ SUCCESS | Milvus Connection\n",
      "  └─ Cleaned up existing connection\n",
      "\n",
      "✅ SUCCESS | Milvus Connection\n",
      "  └─ Connected to milvus-standalone:19530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCESS | Embedding Model\n",
      "  └─ Loaded TurkuNLP/gpt3-finnish-large (dim=1536)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4692aa33cfd7418dae9be5e0935fea11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCESS | LLM Pipeline\n",
      "  └─ Pipeline configured successfully\n",
      "\n",
      "✅ SUCCESS | LLM Setup\n",
      "  └─ Loaded Finnish-NLP/llama-7b-finnish-instruct-v0.2\n",
      "\n",
      "✅ SUCCESS | Document Processor\n",
      "  └─ Initialized with Finnish-optimized settings\n",
      "\n",
      "✅ SUCCESS | Embedding Model\n",
      "  └─ Loaded TurkuNLP/gpt3-finnish-large (dim=1536)\n",
      "\n",
      "✅ SUCCESS | Milvus Connection\n",
      "  └─ Cleaned up existing connection\n",
      "\n",
      "✅ SUCCESS | Milvus Connection\n",
      "  └─ Connected to milvus-standalone:19530\n",
      "\n",
      "✅ SUCCESS | Milvus Collection\n",
      "  └─ Dropped existing collection: document_embeddings\n",
      "\n",
      "✅ SUCCESS | Index Creation\n",
      "  └─ Created IVF_FLAT index\n",
      "\n",
      "✅ SUCCESS | Collection Load\n",
      "  └─ Loaded collection into memory\n",
      "\n",
      "✅ SUCCESS | Milvus Collection\n",
      "  └─ Created new collection: document_embeddings with dim=1536\n",
      "\n",
      "✅ SUCCESS | RAG Pipeline\n",
      "  └─ All components initialized\n",
      "\n",
      "✅ SUCCESS | Section Pipeline Components Verification\n",
      "  └─ Successfully executed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Core Pipeline Components\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, chunk_size=400, chunk_overlap=100):\n",
    "        try:\n",
    "            self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "                separators=[\n",
    "                    \"\\n\\n\",  # First split on double newlines\n",
    "                    \"\\n\",    # Then single newlines\n",
    "                    \".\",     # Then sentence endings\n",
    "                    \":\",     # Then colons (common in Finnish formatting)\n",
    "                    \";\",     # Then semicolons\n",
    "                    \",\",     # Then commas\n",
    "                    \" \",     # Finally, split on spaces if needed\n",
    "                    \"\"\n",
    "                ],\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                length_function=len,\n",
    "                keep_separator=True,\n",
    "                add_start_index=True\n",
    "            )\n",
    "            \n",
    "            # Add Finnish-specific cleaning patterns\n",
    "            self.clean_patterns = [\n",
    "                (r'\\s+', ' '),  # Normalize whitespace\n",
    "                (r'[\\(\\{\\[\\]\\}\\)]', ''),  # Remove brackets\n",
    "                (r'[^\\w\\s\\.\\,\\?\\!\\-\\:\\;äöåÄÖÅ]', ''),  # Keep Finnish characters\n",
    "                (r'\\s+\\.', '.'),  # Fix spacing around periods\n",
    "                (r'\\.+', '.'),  # Normalize multiple periods\n",
    "            ]\n",
    "            print_status(\"Document Processor\", True, \"Initialized with Finnish-optimized settings\")\n",
    "        except Exception as e:\n",
    "            print_status(\"Document Processor\", False, str(e))\n",
    "            raise\n",
    "\n",
    "    def extract_metadata_from_filename(self, filename: str) -> tuple:\n",
    "        \"\"\"Extract metadata from filename.\"\"\"\n",
    "        title = os.path.splitext(filename)[0]\n",
    "        match = re.match(r'([A-Za-z]+)\\s+(\\d{1,3})v\\s+([A-Za-z0-9\\-]+)', title)\n",
    "        if match:\n",
    "            return match.group(1), int(match.group(2)), match.group(3)\n",
    "        return None, None, None\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize Finnish text with improved handling.\"\"\"\n",
    "        try:\n",
    "            # Apply all cleaning patterns\n",
    "            for pattern, replacement in self.clean_patterns:\n",
    "                text = re.sub(pattern, replacement, text)\n",
    "            \n",
    "            # Ensure proper sentence boundaries\n",
    "            text = re.sub(r'([.!?])\\s*([A-ZÄÖÅ])', r'\\1\\n\\2', text)\n",
    "            \n",
    "            # Remove extra whitespace while preserving paragraph breaks\n",
    "            text = '\\n'.join(line.strip() for line in text.split('\\n'))\n",
    "            return text.strip()\n",
    "        except Exception as e:\n",
    "            print_status(\"Text Preprocessing\", False, str(e))\n",
    "            raise\n",
    "\n",
    "    def process_document(self, file_path: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process document with improved metadata and chunking.\"\"\"\n",
    "        try:\n",
    "            # Read document\n",
    "            doc = Document(file_path)\n",
    "            text = \"\\n\".join([para.text for para in doc.paragraphs if para.text.strip()])\n",
    "            \n",
    "            # Extract metadata\n",
    "            filename = os.path.basename(file_path)\n",
    "            name, age, doc_id = self.extract_metadata_from_filename(filename)\n",
    "            \n",
    "            # Preprocess and split text\n",
    "            clean_text = self.preprocess_text(text)\n",
    "            chunks = self.text_splitter.split_text(clean_text)\n",
    "            \n",
    "            # Create chunks with enhanced metadata\n",
    "            processed_chunks = []\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                # Calculate semantic importance score\n",
    "                importance_score = self._calculate_chunk_importance(chunk)\n",
    "                \n",
    "                processed_chunks.append({\n",
    "                    \"text\": chunk,\n",
    "                    \"metadata\": {\n",
    "                        \"source\": filename,\n",
    "                        \"person_name\": name,\n",
    "                        \"person_age\": age,\n",
    "                        \"document_id\": doc_id,\n",
    "                        \"chunk_index\": i,\n",
    "                        \"importance_score\": importance_score,\n",
    "                        \"chunk_length\": len(chunk),\n",
    "                        \"contains_question\": \"?\" in chunk,\n",
    "                    }\n",
    "                })\n",
    "            \n",
    "            print_status(\"Document Processing\", True, \n",
    "                        f\"Processed {filename} into {len(processed_chunks)} chunks\")\n",
    "            return processed_chunks\n",
    "            \n",
    "        except Exception as e:\n",
    "            print_status(\"Document Processing\", False, f\"Error processing {file_path}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _calculate_chunk_importance(self, chunk: str) -> float:\n",
    "        \"\"\"Calculate importance score for chunk based on Finnish text patterns.\"\"\"\n",
    "        score = 1.0\n",
    "        \n",
    "        # Key phrase indicators (common in Finnish documentation)\n",
    "        key_phrases = [\n",
    "            \"tärkeä\", \"merkittävä\", \"olennainen\", \"keskeinen\",\n",
    "            \"huomattava\", \"erityinen\", \"tärkein\", \"ensisijainen\"\n",
    "        ]\n",
    "        \n",
    "        # Increase score for chunks with key phrases\n",
    "        if any(phrase in chunk.lower() for phrase in key_phrases):\n",
    "            score *= 1.2\n",
    "            \n",
    "        # Prefer chunks with complete sentences\n",
    "        if chunk.count('.') > 0:\n",
    "            score *= 1.1\n",
    "            \n",
    "        # Prefer chunks with personal pronouns (common in Finnish personal documents)\n",
    "        if any(pronoun in chunk.lower() for pronoun in [\"minä\", \"minun\", \"minua\", \"minulla\"]):\n",
    "            score *= 1.15\n",
    "            \n",
    "        return score\n",
    "\n",
    "\n",
    "class MilvusManager:\n",
    "    def __init__(self, host: str = \"milvus-standalone\", port: str = \"19530\", alias: str = \"default\"):\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.alias = alias\n",
    "        self.connected = False\n",
    "        self.connect()\n",
    "        \n",
    "    def connect(self):\n",
    "        \"\"\"Establish connection to Milvus.\"\"\"\n",
    "        try:\n",
    "            try:\n",
    "                connections.remove_connection(alias=self.alias)\n",
    "                print_status(\"Milvus Connection\", True, \"Cleaned up existing connection\")\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            connections.connect(\n",
    "                alias=self.alias,\n",
    "                host=self.host,\n",
    "                port=self.port,\n",
    "                timeout=10.0\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                utility.get_server_version()\n",
    "                self.connected = True\n",
    "                print_status(\"Milvus Connection\", True, f\"Connected to {self.host}:{self.port}\")\n",
    "            except Exception as ve:\n",
    "                raise Exception(f\"Connection verification failed: {str(ve)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.connected = False\n",
    "            print_status(\"Milvus Connection\", False, str(e))\n",
    "            raise\n",
    "\n",
    "    def create_collection(self, collection_name: str = \"document_embeddings\"):\n",
    "        \"\"\"Create Milvus collection with appropriate schema.\"\"\"\n",
    "        try:\n",
    "            if utility.has_collection(collection_name):\n",
    "                Collection(name=collection_name).drop()\n",
    "                print_status(\"Milvus Collection\", True, f\"Dropped existing collection: {collection_name}\")\n",
    "                \n",
    "            fields = [\n",
    "                FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "                FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "                FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=EMBEDDING_DIM),\n",
    "                FieldSchema(name=\"person_name\", dtype=DataType.VARCHAR, max_length=100),\n",
    "                FieldSchema(name=\"person_age\", dtype=DataType.INT64),\n",
    "                FieldSchema(name=\"document_id\", dtype=DataType.VARCHAR, max_length=100),\n",
    "                FieldSchema(name=\"chunk_index\", dtype=DataType.INT64)\n",
    "            ]\n",
    "            \n",
    "            schema = CollectionSchema(\n",
    "                fields=fields,\n",
    "                description=\"Document embeddings collection\",\n",
    "                enable_dynamic_field=False\n",
    "            )\n",
    "            collection = Collection(name=collection_name, schema=schema)\n",
    "            \n",
    "            self.create_and_load_index(collection)\n",
    "            \n",
    "            print_status(\"Milvus Collection\", True, f\"Created new collection: {collection_name} with dim={EMBEDDING_DIM}\")\n",
    "            return collection\n",
    "        except Exception as e:\n",
    "            print_status(\"Milvus Collection\", False, str(e))\n",
    "            raise\n",
    "\n",
    "    def create_and_load_index(self, collection):\n",
    "        try:\n",
    "            index_params = {\n",
    "                \"metric_type\": \"IP\",\n",
    "                \"index_type\": \"IVF_FLAT\",\n",
    "                \"params\": {\"nlist\": 1024}\n",
    "            }\n",
    "            collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "            print_status(\"Index Creation\", True, \"Created IVF_FLAT index\")\n",
    "            \n",
    "            collection.load()\n",
    "            print_status(\"Collection Load\", True, \"Loaded collection into memory\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print_status(\"Index Creation\", False, str(e))\n",
    "            raise\n",
    "\n",
    "    def reload_collection(self, collection_name: str = \"document_embeddings\"):\n",
    "        try:\n",
    "            if utility.has_collection(collection_name):\n",
    "                collection = Collection(name=collection_name)\n",
    "                collection.load()\n",
    "                print_status(\"Collection Reload\", True, f\"Reloaded collection: {collection_name}\")\n",
    "                return collection\n",
    "            else:\n",
    "                raise Exception(f\"Collection {collection_name} does not exist\")\n",
    "        except Exception as e:\n",
    "            print_status(\"Collection Reload\", False, str(e))\n",
    "            raise\n",
    "class EmbeddingGenerator:\n",
    "    def __init__(self, model_name: str = \"TurkuNLP/gpt3-finnish-large\"):\n",
    "        try:\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "            self.embedding_dim = self.model.config.hidden_size\n",
    "            print_status(\"Embedding Model\", True, f\"Loaded {model_name} (dim={self.embedding_dim})\")\n",
    "        except Exception as e:\n",
    "            print_status(\"Embedding Model\", False, str(e))\n",
    "            raise\n",
    "        \n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        \n",
    "    def generate(self, texts: List[str], batch_size: int = 8) -> np.ndarray:\n",
    "        try:\n",
    "            all_embeddings = []\n",
    "            \n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                batch_texts = texts[i:i + batch_size]\n",
    "                \n",
    "                encoded_input = self.tokenizer(\n",
    "                    batch_texts,\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=512,\n",
    "                    return_tensors='pt'\n",
    "                ).to(self.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    model_output = self.model(**encoded_input)\n",
    "                \n",
    "                sentence_embeddings = self.mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "                sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "                \n",
    "                all_embeddings.append(sentence_embeddings.cpu().numpy())\n",
    "            \n",
    "            result = np.concatenate(all_embeddings)\n",
    "            \n",
    "            if result.shape[1] != EMBEDDING_DIM:\n",
    "                raise ValueError(f\"Embedding dimension mismatch. Expected {EMBEDDING_DIM}, got {result.shape[1]}\")\n",
    "            \n",
    "            print_status(\"Embedding Generation\", True, \n",
    "                    f\"Generated {len(texts)} embeddings with dimension {result.shape[1]}\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print_status(\"Embedding Generation\", False, str(e))\n",
    "            raise\n",
    "\n",
    "    def get_embedding_dim(self) -> int:\n",
    "        return self.embedding_dim\n",
    "\n",
    "\n",
    "class RAGPipeline:\n",
    "    def __init__(self, model_id: str = \"Finnish-NLP/llama-7b-finnish-instruct-v0.2\"):\n",
    "        try:\n",
    "            self.setup_llm(model_id)\n",
    "            print_status(\"LLM Setup\", True, f\"Loaded {model_id}\")\n",
    "            \n",
    "            self.doc_processor = DocumentProcessor()\n",
    "            self.embedding_generator = EmbeddingGenerator()\n",
    "            \n",
    "            if self.embedding_generator.get_embedding_dim() != EMBEDDING_DIM:\n",
    "                raise ValueError(f\"Embedding dimension mismatch. Global: {EMBEDDING_DIM}, \" \n",
    "                               f\"Generator: {self.embedding_generator.get_embedding_dim()}\")\n",
    "            \n",
    "            self.milvus_manager = MilvusManager(\n",
    "                host=MILVUS_HOST,\n",
    "                port=MILVUS_PORT,\n",
    "                alias=MILVUS_ALIAS\n",
    "            )\n",
    "            self.collection = self.milvus_manager.create_collection()\n",
    "            print_status(\"RAG Pipeline\", True, \"All components initialized\")\n",
    "        except Exception as e:\n",
    "            print_status(\"RAG Pipeline\", False, str(e))\n",
    "            raise\n",
    "    def process_documents(self, folder_path: str):\n",
    "        \"\"\"Process all documents in the specified folder.\"\"\"\n",
    "        try:\n",
    "            # Get all .docx files in the folder\n",
    "            file_paths = [f for f in os.listdir(folder_path) if f.endswith('.docx')]\n",
    "            if not file_paths:\n",
    "                raise ValueError(f\"No .docx files found in {folder_path}\")\n",
    "                \n",
    "            print_status(\"Document Loading\", True, f\"Found {len(file_paths)} documents\")\n",
    "            all_chunks = []\n",
    "            \n",
    "            # Process each document\n",
    "            for file in tqdm(file_paths, desc=\"Processing documents\"):\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                chunks = self.doc_processor.process_document(file_path)\n",
    "                all_chunks.extend(chunks)\n",
    "                \n",
    "            # Generate embeddings for all chunks\n",
    "            texts = [chunk[\"text\"] for chunk in all_chunks]\n",
    "            embeddings = self.embedding_generator.generate(texts)\n",
    "            \n",
    "            # Prepare entities for Milvus\n",
    "            entities = []\n",
    "            for i, (text, embedding, chunk) in enumerate(zip(texts, embeddings, all_chunks)):\n",
    "                entity = {\n",
    "                    \"text\": text,\n",
    "                    \"embedding\": embedding.tolist(),\n",
    "                    \"person_name\": chunk[\"metadata\"][\"person_name\"],\n",
    "                    \"person_age\": chunk[\"metadata\"][\"person_age\"],\n",
    "                    \"document_id\": chunk[\"metadata\"][\"document_id\"],\n",
    "                    \"chunk_index\": chunk[\"metadata\"][\"chunk_index\"]\n",
    "                }\n",
    "                entities.append(entity)\n",
    "            \n",
    "            # Insert into Milvus in batches\n",
    "            batch_size = 100\n",
    "            for i in range(0, len(entities), batch_size):\n",
    "                batch = entities[i:i + batch_size]\n",
    "                self.collection.insert(batch)\n",
    "            \n",
    "            # Ensure data is persisted\n",
    "            self.collection.flush()\n",
    "            \n",
    "            # Create index and load collection\n",
    "            self.milvus_manager.create_and_load_index(self.collection)\n",
    "            \n",
    "            print_status(\"Document Processing\", True, \n",
    "                        f\"Processed {len(texts)} chunks from {len(file_paths)} documents\")\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print_status(\"Document Processing\", False, f\"Error: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "    def setup_llm(self, model_id: str):\n",
    "        try:\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\"\n",
    "            )\n",
    "            \n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                quantization_config=bnb_config,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                max_memory={0: \"6GiB\"},\n",
    "                offload_folder=\"offload\"\n",
    "            )\n",
    "            \n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "            self.pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                max_new_tokens=512,\n",
    "                do_sample=True,\n",
    "                temperature=0.3,\n",
    "                top_p=0.95,\n",
    "                repetition_penalty=1.15\n",
    "            )\n",
    "            print_status(\"LLM Pipeline\", True, \"Pipeline configured successfully\")\n",
    "        except Exception as e:\n",
    "            print_status(\"LLM Pipeline\", False, str(e))\n",
    "            raise\n",
    "\n",
    "    def query(self, question: str, top_k: int = 5):\n",
    "        try:\n",
    "            self.collection = self.milvus_manager.reload_collection()\n",
    "            \n",
    "            # Preprocess question\n",
    "            question = question.strip()\n",
    "            if not question.endswith('?'):\n",
    "                question += '?'\n",
    "            \n",
    "            # Generate question embedding and search\n",
    "            question_embedding = self.embedding_generator.generate([question])[0]\n",
    "            \n",
    "            search_params = {\n",
    "                \"metric_type\": \"IP\",\n",
    "                \"params\": {\"nprobe\": 50}\n",
    "            }\n",
    "            \n",
    "            # Get search results\n",
    "            search_results = self.collection.search(\n",
    "                data=[question_embedding.tolist()],\n",
    "                anns_field=\"embedding\",\n",
    "                param=search_params,\n",
    "                limit=top_k * 2,\n",
    "                output_fields=[\"text\", \"person_name\", \"document_id\", \"chunk_index\"]\n",
    "            )\n",
    "    \n",
    "            # Convert search results to a format we can work with\n",
    "            initial_results = []\n",
    "            for hit in search_results[0]:  # Fix: Properly handle Milvus Hit objects\n",
    "                initial_results.append({\n",
    "                    'text': hit.entity.get('text'),\n",
    "                    'person_name': hit.entity.get('person_name'),\n",
    "                    'document_id': hit.entity.get('document_id'),\n",
    "                    'chunk_index': hit.entity.get('chunk_index'),\n",
    "                    'score': float(hit.score)\n",
    "                })\n",
    "            \n",
    "            # Rerank results\n",
    "            reranked_results = self._rerank_results(question, initial_results)\n",
    "            \n",
    "            # Take top_k after reranking\n",
    "            results = reranked_results[:top_k]\n",
    "            \n",
    "            context_parts = []\n",
    "            for i, hit in enumerate(results):\n",
    "                context_parts.append(\n",
    "                    f\"[Dokumentti {i+1}]\\n\"\n",
    "                    f\"Lähde: {hit['document_id']}\\n\"\n",
    "                    f\"Henkilö: {hit['person_name']}\\n\"\n",
    "                    f\"Luotettavuus: {hit['score']:.2%}\\n\"\n",
    "                    f\"Tekstikatkelma:\\n{hit['text']}\\n\"\n",
    "                    f\"{'-' * 40}\\n\"\n",
    "                )\n",
    "            \n",
    "            context = \"\\n\".join(context_parts)\n",
    "            \n",
    "            prompt = f\"\"\"Tehtävä: Etsi tarkka vastaus annettuun kysymykseen käyttäen vain alla olevaa kontekstia.\n",
    "    \n",
    "            Kysymys: {question}\n",
    "            \n",
    "            Konteksti:\n",
    "            {context}\n",
    "            \n",
    "            Tärkeät ohjeet:\n",
    "            1. Jos löydät suoran vastauksen:\n",
    "               - Mainitse AINA ensin dokumentti, josta vastaus löytyy (esim. \"Dokumentti 1:\")\n",
    "               - Lainaa TARKASTI alkuperäistä tekstiä käyttäen lainausmerkkejä\n",
    "               - Perustele vastauksen luotettavuus yhdellä lyhyellä lauseella\n",
    "            \n",
    "            2. Jos löydät vain osittaisen vastauksen:\n",
    "               - Kerro selkeästi mikä osa vastauksesta löytyi ja mikä puuttuu\n",
    "               - Käytä silti suoria lainauksia löytyneestä osasta\n",
    "            \n",
    "            3. Jos et löydä minkäänlaista vastausta:\n",
    "               - Vastaa vain: \"En löydä suoraa vastausta annetusta kontekstista\"\n",
    "            \n",
    "            Vastaus:\"\"\"\n",
    "            \n",
    "            response = self.pipeline(\n",
    "                prompt,\n",
    "                max_new_tokens=300,\n",
    "                do_sample=True,\n",
    "                temperature=0.1,\n",
    "                top_p=0.85,\n",
    "                repetition_penalty=1.2\n",
    "            )[0][\"generated_text\"]\n",
    "    \n",
    "            response = self._clean_response(response)\n",
    "            \n",
    "            return {\n",
    "                \"answer\": response,\n",
    "                \"sources\": results,\n",
    "                \"metadata\": {\n",
    "                    \"question\": question,\n",
    "                    \"num_chunks_retrieved\": len(results),\n",
    "                    \"max_similarity_score\": max(hit['score'] for hit in results)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print_status(\"Query\", False, str(e))\n",
    "            raise\n",
    "    \n",
    "    def _rerank_results(self, question: str, results: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Rerank results using both semantic and exact matching.\"\"\"\n",
    "        reranked = []\n",
    "        question_lower = question.lower()\n",
    "        \n",
    "        for hit in results:\n",
    "            text = hit['text']\n",
    "            score = hit['score']\n",
    "            \n",
    "            # Boost score if text contains exact matches\n",
    "            if any(word in text.lower() for word in question_lower.split()):\n",
    "                score *= 1.2\n",
    "            \n",
    "            # Boost score for complete sentences\n",
    "            if '.' in text:\n",
    "                score *= 1.1\n",
    "                \n",
    "            # Boost score for direct answers\n",
    "            if '?' in text and '.' in text:\n",
    "                score *= 1.15\n",
    "                \n",
    "            hit['score'] = score\n",
    "            reranked.append(hit)\n",
    "        \n",
    "        # Sort by final score\n",
    "        reranked.sort(key=lambda x: x['score'], reverse=True)\n",
    "        return reranked\n",
    "\n",
    "    def _clean_response(self, response: str) -> str:\n",
    "        response = re.sub(r'(\\[Dokumentti \\d+\\])\\s*\\1', r'\\1', response)\n",
    "        response = re.sub(r'Luottamus: \\d+%\\s*Selitys:', '', response)\n",
    "        if len(response) > 500:\n",
    "            response = response[:497] + '...'\n",
    "        return response.strip()\n",
    "\n",
    "    def _create_prompt(self, question: str, context: str) -> str:\n",
    "        return f\"\"\"Tehtävä: Vastaa annettuun kysymykseen käyttäen vain alla olevaa kontekstia.\n",
    "\n",
    "        Kysymys: {question}\n",
    "        \n",
    "        Konteksti:\n",
    "        {context}\n",
    "        \n",
    "        Vastausohjeet:\n",
    "        1. Jos löydät suoran vastauksen:\n",
    "           - Mainitse ensin dokumentti, josta vastaus löytyy\n",
    "           - Lainaa tekstiä tarkasti käyttäen lainausmerkkejä\n",
    "           - Mainitse vastauksen luotettavuus prosentteina\n",
    "        \n",
    "        2. Jos löydät osittaisen vastauksen:\n",
    "           - Kerro, mitä tietoa löysit ja mistä\n",
    "           - Mainitse selkeästi, mitä tietoa puuttuu\n",
    "        \n",
    "        3. Jos et löydä vastausta:\n",
    "           - Vastaa vain: \"En löydä suoraa vastausta annetusta kontekstista\"\n",
    "        \n",
    "        Vastaus:\"\"\"\n",
    "\n",
    "## validating \n",
    "def verify_pipeline_components():\n",
    "    doc_processor = DocumentProcessor()\n",
    "    milvus_manager = MilvusManager()\n",
    "    embedding_generator = EmbeddingGenerator()\n",
    "    pipeline = RAGPipeline()\n",
    "    return all([doc_processor, milvus_manager, embedding_generator, pipeline])\n",
    "\n",
    "verify_section(\"Pipeline Components\", verify_pipeline_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b341d652-c9a0-4ddd-bf18-3af1daf84ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35096e62d6f94987a84e621b3e7723ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCESS | LLM Pipeline\n",
      "  └─ Pipeline configured successfully\n",
      "\n",
      "✅ SUCCESS | LLM Setup\n",
      "  └─ Loaded Finnish-NLP/llama-7b-finnish-instruct-v0.2\n",
      "\n",
      "✅ SUCCESS | Document Processor\n",
      "  └─ Initialized with Finnish-optimized settings\n",
      "\n",
      "✅ SUCCESS | Embedding Model\n",
      "  └─ Loaded TurkuNLP/gpt3-finnish-large (dim=1536)\n",
      "\n",
      "✅ SUCCESS | Milvus Connection\n",
      "  └─ Cleaned up existing connection\n",
      "\n",
      "✅ SUCCESS | Milvus Connection\n",
      "  └─ Connected to milvus-standalone:19530\n",
      "\n",
      "✅ SUCCESS | Milvus Collection\n",
      "  └─ Dropped existing collection: document_embeddings\n",
      "\n",
      "✅ SUCCESS | Index Creation\n",
      "  └─ Created IVF_FLAT index\n",
      "\n",
      "✅ SUCCESS | Collection Load\n",
      "  └─ Loaded collection into memory\n",
      "\n",
      "✅ SUCCESS | Milvus Collection\n",
      "  └─ Created new collection: document_embeddings with dim=1536\n",
      "\n",
      "✅ SUCCESS | RAG Pipeline\n",
      "  └─ All components initialized\n",
      "\n",
      "✅ SUCCESS | Section Testing Infrastructure Verification\n",
      "  └─ Successfully executed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing Infrastructure\n",
    "class RAGTester:\n",
    "    def __init__(self, pipeline: RAGPipeline):\n",
    "        self.pipeline = pipeline\n",
    "        self.test_results = []\n",
    "        \n",
    "    def run_test_suite(self, test_questions: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Run comprehensive tests on the RAG pipeline.\"\"\"\n",
    "        test_results = []\n",
    "        summary_stats = {\n",
    "            \"total_questions\": len(test_questions),\n",
    "            \"successful_responses\": 0,\n",
    "            \"failed_responses\": 0,\n",
    "            \"average_similarity\": 0.0,\n",
    "            \"direct_quote_ratio\": 0.0\n",
    "        }\n",
    "        \n",
    "        for question in test_questions:\n",
    "            try:\n",
    "                # Get response from pipeline\n",
    "                result = self.pipeline.query(question)\n",
    "                \n",
    "                # Validate response\n",
    "                validation = self._validate_response(result[\"answer\"])\n",
    "                \n",
    "                # Calculate response metrics\n",
    "                metrics = {\n",
    "                    \"question\": question,\n",
    "                    \"has_direct_quote\": validation[\"has_direct_quote\"],\n",
    "                    \"source_count\": len(result[\"sources\"]),\n",
    "                    \"max_similarity\": max(s[\"similarity_score\"] for s in result[\"sources\"]),\n",
    "                    \"response_quality\": validation,\n",
    "                    \"response_length\": len(result[\"answer\"]),\n",
    "                }\n",
    "                \n",
    "                test_results.append(metrics)\n",
    "                summary_stats[\"successful_responses\"] += 1\n",
    "                summary_stats[\"average_similarity\"] += metrics[\"max_similarity\"]\n",
    "                summary_stats[\"direct_quote_ratio\"] += int(metrics[\"has_direct_quote\"])\n",
    "                \n",
    "                # Print detailed results\n",
    "                self._print_test_result(question, result, metrics)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error testing question '{question}': {str(e)}\")\n",
    "                summary_stats[\"failed_responses\"] += 1\n",
    "                \n",
    "        # Calculate final statistics\n",
    "        if summary_stats[\"successful_responses\"] > 0:\n",
    "            summary_stats[\"average_similarity\"] /= summary_stats[\"successful_responses\"]\n",
    "            summary_stats[\"direct_quote_ratio\"] /= summary_stats[\"successful_responses\"]\n",
    "        \n",
    "        return {\n",
    "            \"detailed_results\": test_results,\n",
    "            \"summary\": summary_stats\n",
    "        }\n",
    "    \n",
    "    def _validate_response(self, response: str) -> Dict[str, bool]:\n",
    "        \"\"\"Validate Finnish language response with detailed checks.\"\"\"\n",
    "        validation = {\n",
    "            # Basic structural checks\n",
    "            \"has_source_reference\": bool(re.search(r'\\[Dokumentti \\d+\\]', response)),\n",
    "            \"has_direct_quote\": '\"' in response,\n",
    "            \"is_complete_sentence\": response.strip().endswith(('.', '?', '!')),\n",
    "            \"has_confidence\": bool(re.search(r'\\d+\\s*%', response)),\n",
    "            \"reasonable_length\": 10 <= len(response) <= 500,\n",
    "            \n",
    "            # Finnish language specific checks\n",
    "            \"has_finnish_chars\": bool(re.search(r'[äöåÄÖÅ]', response)),\n",
    "            \"proper_finnish_structure\": self._check_finnish_structure(response)\n",
    "        }\n",
    "        return validation\n",
    "    \n",
    "    def _check_finnish_structure(self, text: str) -> bool:\n",
    "        \"\"\"Check if the response follows typical Finnish sentence structure.\"\"\"\n",
    "        finnish_endings = [\n",
    "            'ssa', 'ssä', 'sta', 'stä', 'lla', 'llä', 'lta', 'ltä',\n",
    "            'ksi', 'in', 'en', 'teen', 'seen'\n",
    "        ]\n",
    "        words = text.lower().split()\n",
    "        if not words:\n",
    "            return False\n",
    "            \n",
    "        has_finnish_ending = any(\n",
    "            any(word.endswith(ending) for ending in finnish_endings)\n",
    "            for word in words\n",
    "        )\n",
    "        \n",
    "        return has_finnish_ending\n",
    "    \n",
    "    def _print_test_result(self, question: str, result: Dict, metrics: Dict):\n",
    "        \"\"\"Print formatted test results.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Answer: {result['answer']}\")\n",
    "        print(\"\\nMetrics:\")\n",
    "        print(f\"- Source count: {metrics['source_count']}\")\n",
    "        print(f\"- Max similarity: {metrics['max_similarity']:.2%}\")\n",
    "        print(f\"- Response length: {metrics['response_length']}\")\n",
    "        print(\"\\nValidation:\")\n",
    "        for key, value in metrics['response_quality'].items():\n",
    "            print(f\"- {key}: {'✅' if value else '❌'}\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "\n",
    "def run_rag_tests(pipeline: RAGPipeline, test_questions: List[str] = None):\n",
    "    \"\"\"Execute RAG tests with default or custom test questions.\"\"\"\n",
    "    if test_questions is None:\n",
    "        test_questions = [\n",
    "            \"Onko Marjatta Eilan ystävä?\",\n",
    "            \"Miten Sulo kokee sosiaalisen kanssakäymisen merkityksen?\",\n",
    "            \"Montako sisarusta Sulolla on?\",\n",
    "            \"Millainen on Eilan arki?\",\n",
    "            \"Mikä on Sulolle tärkeää?\"\n",
    "        ]\n",
    "    \n",
    "    tester = RAGTester(pipeline)\n",
    "    results = tester.run_test_suite(test_questions)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nTest Summary:\")\n",
    "    print(f\"Total questions: {results['summary']['total_questions']}\")\n",
    "    print(f\"Successful responses: {results['summary']['successful_responses']}\")\n",
    "    print(f\"Failed responses: {results['summary']['failed_responses']}\")\n",
    "    print(f\"Average similarity: {results['summary']['average_similarity']:.2%}\")\n",
    "    print(f\"Direct quote ratio: {results['summary']['direct_quote_ratio']:.2%}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def verify_testing_infrastructure():\n",
    "    pipeline = RAGPipeline()\n",
    "    tester = RAGTester(pipeline)\n",
    "    return bool(tester)\n",
    "\n",
    "verify_section(\"Testing Infrastructure\", verify_testing_infrastructure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be4e3db5-26a8-44c6-bd51-106ba6439296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting RAG test execution...\n",
      "\n",
      "1. Initializing base pipeline...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364d3e3aaa2b4d5585daf5639b20ae36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCESS | LLM Pipeline\n",
      "  └─ Pipeline configured successfully\n",
      "\n",
      "✅ SUCCESS | LLM Setup\n",
      "  └─ Loaded Finnish-NLP/llama-7b-finnish-instruct-v0.2\n",
      "\n",
      "✅ SUCCESS | Document Processor\n",
      "  └─ Initialized with Finnish-optimized settings\n",
      "\n",
      "✅ SUCCESS | Embedding Model\n",
      "  └─ Loaded TurkuNLP/gpt3-finnish-large (dim=1536)\n",
      "\n",
      "✅ SUCCESS | Milvus Connection\n",
      "  └─ Cleaned up existing connection\n",
      "\n",
      "✅ SUCCESS | Milvus Connection\n",
      "  └─ Connected to milvus-standalone:19530\n",
      "\n",
      "✅ SUCCESS | Milvus Collection\n",
      "  └─ Dropped existing collection: document_embeddings\n",
      "\n",
      "✅ SUCCESS | Index Creation\n",
      "  └─ Created IVF_FLAT index\n",
      "\n",
      "✅ SUCCESS | Collection Load\n",
      "  └─ Loaded collection into memory\n",
      "\n",
      "✅ SUCCESS | Milvus Collection\n",
      "  └─ Created new collection: document_embeddings with dim=1536\n",
      "\n",
      "✅ SUCCESS | RAG Pipeline\n",
      "  └─ All components initialized\n",
      "   ✓ Base pipeline initialized\n",
      "\n",
      "2. Creating Finnish RAG Agent...\n",
      "   ✓ Agent created\n",
      "\n",
      "3. Processing documents...\n",
      "\n",
      "✅ SUCCESS | Document Loading\n",
      "  └─ Found 2 documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0681752b8d104edf82126dea88ae8075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing documents:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Eila 81v SH-4.docx into 5 chunks\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed Sulo 75v C5-50.docx into 4 chunks\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 9 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Index Creation\n",
      "  └─ Created IVF_FLAT index\n",
      "\n",
      "✅ SUCCESS | Collection Load\n",
      "  └─ Loaded collection into memory\n",
      "\n",
      "✅ SUCCESS | Document Processing\n",
      "  └─ Processed 9 chunks from 2 documents\n",
      "   ✓ Documents processed\n",
      "\n",
      "4. Running test questions...\n",
      "\n",
      "Kysymys 1/5:\n",
      "--------------------------------------------------------------------------------\n",
      "Kuka on tärkeitä henkilöitä Eilalle?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "✅ SUCCESS | Collection Reload\n",
      "  └─ Reloaded collection: document_embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmp0lnczvtx\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmp0lnczvtx/_remote_module_non_scriptable.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Collection Reload\n",
      "  └─ Reloaded collection: document_embeddings\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "VASTAUS:\n",
      "----------------------------------------\n",
      "Tehtävä: Etsi tarkka vastaus annettuun kysymykseen käyttäen vain alla olevaa kontekstia.\n",
      "    \n",
      "            Kysymys: Etsi tarkka vastaus seuraavaan kysymykseen käyttäen alla olevia dokumentteja.\n",
      "    \n",
      "    Kysymys: Kuka on tärkeitä henkilöitä Eilalle?\n",
      "    \n",
      "    Dokumentit:\n",
      "    \n",
      "Dokumentti 1 (C5-50):\n",
      "Lopuksi Koen, että on tärkeää saada sosiaalista kanssakäymistä.\n",
      "Silloin päivä menee hyvin.\n",
      "\n",
      "Dokumentti 2 (SH-4):\n",
      "Milloin rupattelen kenenkin kanssa ja mitä.\n",
      "Marjatta on minun ystäväni.\n",
      "En ole terveydes...\n",
      "----------------------------------------\n",
      "\n",
      "Vastaus:\n",
      "Tehtävä: Etsi tarkka vastaus annettuun kysymykseen käyttäen vain alla olevaa kontekstia.\n",
      "    \n",
      "            Kysymys: Etsi tarkka vastaus seuraavaan kysymykseen käyttäen alla olevia dokumentteja.\n",
      "    \n",
      "    Kysymys: Kuka on tärkeitä henkilöitä Eilalle?\n",
      "    \n",
      "    Dokumentit:\n",
      "    \n",
      "Dokumentti 1 (C5-50):\n",
      "Lopuksi Koen, että on tärkeää saada sosiaalista kanssakäymistä.\n",
      "Silloin päivä menee hyvin.\n",
      "\n",
      "Dokumentti 2 (SH-4):\n",
      "Milloin rupattelen kenenkin kanssa ja mitä.\n",
      "Marjatta on minun ystäväni.\n",
      "En ole terveydes...\n",
      "\n",
      "Lähde:\n",
      "- C5-50: Lopuksi Koen, että on tärkeää saada sosiaalista kanssakäymistä.\n",
      "Silloin päivä menee hyvin....\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Kysymys 2/5:\n",
      "--------------------------------------------------------------------------------\n",
      "Kuinka vanha on Sulo ja mikä on hänen syntymävuotensa?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "✅ SUCCESS | Collection Reload\n",
      "  └─ Reloaded collection: document_embeddings\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Collection Reload\n",
      "  └─ Reloaded collection: document_embeddings\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "VASTAUS:\n",
      "----------------------------------------\n",
      "Tehtävä: Etsi tarkka vastaus annettuun kysymykseen käyttäen vain alla olevaa kontekstia.\n",
      "    \n",
      "            Kysymys: Etsi tarkka vastaus seuraavaan kysymykseen käyttäen alla olevia dokumentteja.\n",
      "    \n",
      "    Kysymys: Kuinka vanha on Sulo ja mikä on hänen syntymävuotensa?\n",
      "    \n",
      "    Dokumentit:\n",
      "    \n",
      "Dokumentti 1 (C5-50):\n",
      "Lopuksi Koen, että on tärkeää saada sosiaalista kanssakäymistä.\n",
      "Silloin päivä menee hyvin.\n",
      "\n",
      "Dokumentti 2 (SH-4):\n",
      "Milloin rupattelen kenenkin kanssa ja mitä.\n",
      "Marjatta on minun ystäväni...\n",
      "----------------------------------------\n",
      "\n",
      "Vastaus:\n",
      "Tehtävä: Etsi tarkka vastaus annettuun kysymykseen käyttäen vain alla olevaa kontekstia.\n",
      "    \n",
      "            Kysymys: Etsi tarkka vastaus seuraavaan kysymykseen käyttäen alla olevia dokumentteja.\n",
      "    \n",
      "    Kysymys: Kuinka vanha on Sulo ja mikä on hänen syntymävuotensa?\n",
      "    \n",
      "    Dokumentit:\n",
      "    \n",
      "Dokumentti 1 (C5-50):\n",
      "Lopuksi Koen, että on tärkeää saada sosiaalista kanssakäymistä.\n",
      "Silloin päivä menee hyvin.\n",
      "\n",
      "Dokumentti 2 (SH-4):\n",
      "Milloin rupattelen kenenkin kanssa ja mitä.\n",
      "Marjatta on minun ystäväni...\n",
      "\n",
      "Lähde:\n",
      "- C5-50: Lopuksi Koen, että on tärkeää saada sosiaalista kanssakäymistä.\n",
      "Silloin päivä menee hyvin....\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Kysymys 3/5:\n",
      "--------------------------------------------------------------------------------\n",
      "Mitä Eila harrastaa?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "✅ SUCCESS | Collection Reload\n",
      "  └─ Reloaded collection: document_embeddings\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Collection Reload\n",
      "  └─ Reloaded collection: document_embeddings\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "VASTAUS:\n",
      "----------------------------------------\n",
      "Tehtävä: Etsi tarkka vastaus annettuun kysymykseen käyttäen vain alla olevaa kontekstia.\n",
      "    \n",
      "            Kysymys: Etsi tarkka vastaus seuraavaan kysymykseen käyttäen alla olevia dokumentteja.\n",
      "    \n",
      "    Kysymys: Mitä Eila harrastaa?\n",
      "    \n",
      "    Dokumentit:\n",
      "    \n",
      "Dokumentti 1 (SH-4):\n",
      "Milloin rupattelen kenenkin kanssa ja mitä.\n",
      "Marjatta on minun ystäväni.\n",
      "En ole terveydessä huomannut ongelmia, mitä nyt flunssaa on ollut.\n",
      "Musiikista pidän ja kahvista.\n",
      "On mukavaa, kun kysytään kävelylle tai kauppaan m...\n",
      "----------------------------------------\n",
      "\n",
      "Vastaus:\n",
      "Tehtävä: Etsi tarkka vastaus annettuun kysymykseen käyttäen vain alla olevaa kontekstia.\n",
      "    \n",
      "            Kysymys: Etsi tarkka vastaus seuraavaan kysymykseen käyttäen alla olevia dokumentteja.\n",
      "    \n",
      "    Kysymys: Mitä Eila harrastaa?\n",
      "    \n",
      "    Dokumentit:\n",
      "    \n",
      "Dokumentti 1 (SH-4):\n",
      "Milloin rupattelen kenenkin kanssa ja mitä.\n",
      "Marjatta on minun ystäväni.\n",
      "En ole terveydessä huomannut ongelmia, mitä nyt flunssaa on ollut.\n",
      "Musiikista pidän ja kahvista.\n",
      "On mukavaa, kun kysytään kävelylle tai kauppaan m...\n",
      "\n",
      "Lähde:\n",
      "- SH-4: Milloin rupattelen kenenkin kanssa ja mitä.\n",
      "Marjatta on minun ystäväni.\n",
      "En ole terveydessä huomannut ongelmia, mitä nyt flunssaa on ollut.\n",
      "Musiikista ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Kysymys 4/5:\n",
      "--------------------------------------------------------------------------------\n",
      "Miten Sulo liikkuu?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "✅ SUCCESS | Collection Reload\n",
      "  └─ Reloaded collection: document_embeddings\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Collection Reload\n",
      "  └─ Reloaded collection: document_embeddings\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "VASTAUS:\n",
      "----------------------------------------\n",
      "Tehtävä: Etsi tarkka vastaus annettuun kysymykseen käyttäen vain alla olevaa kontekstia.\n",
      "    \n",
      "            Kysymys: Etsi tarkka vastaus seuraavaan kysymykseen käyttäen alla olevia dokumentteja.\n",
      "    \n",
      "    Kysymys: Miten Sulo liikkuu?\n",
      "    \n",
      "    Dokumentit:\n",
      "    \n",
      "Dokumentti 1 (C5-50):\n",
      "Lopuksi Koen, että on tärkeää saada sosiaalista kanssakäymistä.\n",
      "Silloin päivä menee hyvin.\n",
      "\n",
      "Dokumentti 2 (SH-4):\n",
      "Milloin rupattelen kenenkin kanssa ja mitä.\n",
      "Marjatta on minun ystäväni.\n",
      "En ole terveydessä huomannut onge...\n",
      "----------------------------------------\n",
      "\n",
      "Vastaus:\n",
      "Tehtävä: Etsi tarkka vastaus annettuun kysymykseen käyttäen vain alla olevaa kontekstia.\n",
      "    \n",
      "            Kysymys: Etsi tarkka vastaus seuraavaan kysymykseen käyttäen alla olevia dokumentteja.\n",
      "    \n",
      "    Kysymys: Miten Sulo liikkuu?\n",
      "    \n",
      "    Dokumentit:\n",
      "    \n",
      "Dokumentti 1 (C5-50):\n",
      "Lopuksi Koen, että on tärkeää saada sosiaalista kanssakäymistä.\n",
      "Silloin päivä menee hyvin.\n",
      "\n",
      "Dokumentti 2 (SH-4):\n",
      "Milloin rupattelen kenenkin kanssa ja mitä.\n",
      "Marjatta on minun ystäväni.\n",
      "En ole terveydessä huomannut onge...\n",
      "\n",
      "Lähde:\n",
      "- C5-50: Lopuksi Koen, että on tärkeää saada sosiaalista kanssakäymistä.\n",
      "Silloin päivä menee hyvin....\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Kysymys 5/5:\n",
      "--------------------------------------------------------------------------------\n",
      "Mistä asioista Eila pitää?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "✅ SUCCESS | Collection Reload\n",
      "  └─ Reloaded collection: document_embeddings\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "✅ SUCCESS | Collection Reload\n",
      "  └─ Reloaded collection: document_embeddings\n",
      "\n",
      "✅ SUCCESS | Embedding Generation\n",
      "  └─ Generated 1 embeddings with dimension 1536\n",
      "\n",
      "VASTAUS:\n",
      "----------------------------------------\n",
      "Tehtävä: Etsi tarkka vastaus annettuun kysymykseen käyttäen vain alla olevaa kontekstia.\n",
      "    \n",
      "            Kysymys: Etsi tarkka vastaus seuraavaan kysymykseen käyttäen alla olevia dokumentteja.\n",
      "    \n",
      "    Kysymys: Mistä asioista Eila pitää?\n",
      "    \n",
      "    Dokumentit:\n",
      "    \n",
      "Dokumentti 1 (C5-50):\n",
      "Lopuksi Koen, että on tärkeää saada sosiaalista kanssakäymistä.\n",
      "Silloin päivä menee hyvin.\n",
      "\n",
      "Dokumentti 2 (SH-4):\n",
      "Syntymävuosi: 1942 81v.\n",
      "Sukupuoli: nainen Kansalaisuus: suomi Maakunta: uusimaa Talotyyppi: ryhmäk...\n",
      "----------------------------------------\n",
      "\n",
      "Vastaus:\n",
      "Tehtävä: Etsi tarkka vastaus annettuun kysymykseen käyttäen vain alla olevaa kontekstia.\n",
      "    \n",
      "            Kysymys: Etsi tarkka vastaus seuraavaan kysymykseen käyttäen alla olevia dokumentteja.\n",
      "    \n",
      "    Kysymys: Mistä asioista Eila pitää?\n",
      "    \n",
      "    Dokumentit:\n",
      "    \n",
      "Dokumentti 1 (C5-50):\n",
      "Lopuksi Koen, että on tärkeää saada sosiaalista kanssakäymistä.\n",
      "Silloin päivä menee hyvin.\n",
      "\n",
      "Dokumentti 2 (SH-4):\n",
      "Syntymävuosi: 1942 81v.\n",
      "Sukupuoli: nainen Kansalaisuus: suomi Maakunta: uusimaa Talotyyppi: ryhmäk...\n",
      "\n",
      "Lähde:\n",
      "- C5-50: Lopuksi Koen, että on tärkeää saada sosiaalista kanssakäymistä.\n",
      "Silloin päivä menee hyvin....\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "5. Testaus valmis\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'question': 'Kuka on tärkeitä henkilöitä Eilalle?',\n",
       "  'response': {'answer': 'Tehtävä: Etsi tarkka vastaus annettuun kysymykseen käyttäen vain alla olevaa kontekstia.\\n    \\n            Kysymys: Etsi tarkka vastaus seuraavaan kysymykseen käyttäen alla olevia dokumentteja.\\n    \\n    Kysymys: Kuka on tärkeitä henkilöitä Eilalle?\\n    \\n    Dokumentit:\\n    \\nDokumentti 1 (C5-50):\\nLopuksi Koen, että on tärkeää saada sosiaalista kanssakäymistä.\\nSilloin päivä menee hyvin.\\n\\nDokumentti 2 (SH-4):\\nMilloin rupattelen kenenkin kanssa ja mitä.\\nMarjatta on minun ystäväni.\\nEn ole terveydes...',\n",
       "   'confidence': 1.088071975708008,\n",
       "   'sources': [{'text': 'Lopuksi Koen, että on tärkeää saada sosiaalista kanssakäymistä.\\nSilloin päivä menee hyvin.',\n",
       "     'score': 1.088071975708008,\n",
       "     'source': 'C5-50',\n",
       "     'metadata': {'person_name': 'Sulo', 'chunk_index': 3}},\n",
       "    {'text': 'Milloin rupattelen kenenkin kanssa ja mitä.\\nMarjatta on minun ystäväni.\\nEn ole terveydessä huomannut ongelmia, mitä nyt flunssaa on ollut.\\nMusiikista pidän ja kahvista.\\nOn mukavaa, kun kysytään kävelylle tai kauppaan mukaan.\\nRaha-asiat huolettavat ja välillä on epäkuntoisia päiviä, kuten hammassärkyä.\\nJoskus koen yksinäisyyttä, jos ei ole määrätynlainen ihminen lähelläkään.',\n",
       "     'score': 1.0069434928894043,\n",
       "     'source': 'SH-4',\n",
       "     'metadata': {'person_name': 'Eila', 'chunk_index': 1}},\n",
       "    {'text': 'Läheinen omainen on tärkeä elämässä.\\nJos käy kaupassa tai ulkona, niin omainen on siinä tärkeä.\\nJos ei ystävälle sovi lähteä kanssani ulos ja on tärkeää mennä, lähden yksin.\\nTärkeitä henkilöitä on ystäväni Marjatta ja tietysti aviomies.\\nTärkeimmät omaiset.\\nHarrastan milloin mitäkin.\\nMielelläni tykkään mennä tanssimaan.\\nYstävyys on kaikkein tärkein asia.',\n",
       "     'score': 0.9844434356689453,\n",
       "     'source': 'SH-4',\n",
       "     'metadata': {'person_name': 'Eila', 'chunk_index': 3}}],\n",
       "   'context_analysis': {'entities': {'En',\n",
       "     'Harrastan',\n",
       "     'Jos',\n",
       "     'Joskus',\n",
       "     'Koen',\n",
       "     'Lopuksi',\n",
       "     'Läheinen',\n",
       "     'Marjatta',\n",
       "     'Mielelläni',\n",
       "     'Milloin',\n",
       "     'Musiikista',\n",
       "     'On',\n",
       "     'Raha',\n",
       "     'Silloin',\n",
       "     'Tärkeimmät',\n",
       "     'Tärkeitä',\n",
       "     'Ystävyys'},\n",
       "    'relationships': [{'type': 'yhteys',\n",
       "      'entities': ('sosiaalista',),\n",
       "      'text': 'sosiaalista kanssa'},\n",
       "     {'type': 'tärkeys', 'entities': ('on',), 'text': 'on tärkeä'},\n",
       "     {'type': 'ystävyys',\n",
       "      'entities': ('Marjatta', 'minun'),\n",
       "      'text': 'Marjatta on minun ystävä'},\n",
       "     {'type': 'yhteys', 'entities': ('kenenkin',), 'text': 'kenenkin kanssa'},\n",
       "     {'type': 'yhteys', 'entities': ('lähteä',), 'text': 'lähteä kanssa'},\n",
       "     {'type': 'tärkeys', 'entities': ('on',), 'text': 'on tärkeä'},\n",
       "     {'type': 'tärkeys', 'entities': ('siinä',), 'text': 'siinä tärkeä'},\n",
       "     {'type': 'tärkeys', 'entities': ('on',), 'text': 'on tärkeä'}],\n",
       "    'temporal_refs': [{'type': 'time_of_day',\n",
       "      'text': 'päivä',\n",
       "      'position': (72, 77)}],\n",
       "    'key_topics': set()}}},\n",
       " {'question': 'Kuinka vanha on Sulo ja mikä on hänen syntymävuotensa?',\n",
       "  'response': {'answer': 'Tehtävä: Etsi tarkka vastaus annettuun kysymykseen käyttäen vain alla olevaa kontekstia.\\n    \\n            Kysymys: Etsi tarkka vastaus seuraavaan kysymykseen käyttäen alla olevia dokumentteja.\\n    \\n    Kysymys: Kuinka vanha on Sulo ja mikä on hänen syntymävuotensa?\\n    \\n    Dokumentit:\\n    \\nDokumentti 1 (C5-50):\\nLopuksi Koen, että on tärkeää saada sosiaalista kanssakäymistä.\\nSilloin päivä menee hyvin.\\n\\nDokumentti 2 (SH-4):\\nMilloin rupattelen kenenkin kanssa ja mitä.\\nMarjatta on minun ystäväni...',\n",
       "   'confidence': 0.9487631392478943,\n",
       "   'sources': [{'text': 'Lopuksi Koen, että on tärkeää saada sosiaalista kanssakäymistä.\\nSilloin päivä menee hyvin.',\n",
       "     'score': 0.9487631392478943,\n",
       "     'source': 'C5-50',\n",
       "     'metadata': {'person_name': 'Sulo', 'chunk_index': 3}},\n",
       "    {'text': 'Milloin rupattelen kenenkin kanssa ja mitä.\\nMarjatta on minun ystäväni.\\nEn ole terveydessä huomannut ongelmia, mitä nyt flunssaa on ollut.\\nMusiikista pidän ja kahvista.\\nOn mukavaa, kun kysytään kävelylle tai kauppaan mukaan.\\nRaha-asiat huolettavat ja välillä on epäkuntoisia päiviä, kuten hammassärkyä.\\nJoskus koen yksinäisyyttä, jos ei ole määrätynlainen ihminen lähelläkään.',\n",
       "     'score': 0.909520139694214,\n",
       "     'source': 'SH-4',\n",
       "     'metadata': {'person_name': 'Eila', 'chunk_index': 1}},\n",
       "    {'text': 'Tunnistetieto: C5-50 Haastattelun päivämäärä: 22.6.23 Syntymävuosi: 1948 75 Sukupuoli: mies Kansalaisuus: suomi Maakunta: Keski-pohjanmaa Talotyyppi: Muu tuettu palveluasuminen Asiasanat: trauma, sosiaalisuus Arjen kuvaus Oikeanpuoleinen heikkouteni ja jäykkyyteni hankaloittaa toimintaani, mutta pystyn itse kävelemään rollaattorilla.',\n",
       "     'score': 0.8739263105392456,\n",
       "     'source': 'C5-50',\n",
       "     'metadata': {'person_name': 'Sulo', 'chunk_index': 0}}],\n",
       "   'context_analysis': {'entities': {'Arjen',\n",
       "     'Asiasanat',\n",
       "     'En',\n",
       "     'Haastattelun',\n",
       "     'Joskus',\n",
       "     'Kansalaisuus',\n",
       "     'Keski',\n",
       "     'Koen',\n",
       "     'Lopuksi',\n",
       "     'Maakunta',\n",
       "     'Marjatta',\n",
       "     'Milloin',\n",
       "     'Musiikista',\n",
       "     'Muu',\n",
       "     'Oikeanpuoleinen',\n",
       "     'On',\n",
       "     'Raha',\n",
       "     'Silloin',\n",
       "     'Sukupuoli',\n",
       "     'Syntymävuosi',\n",
       "     'Talotyyppi',\n",
       "     'Tunnistetieto'},\n",
       "    'relationships': [{'type': 'yhteys',\n",
       "      'entities': ('sosiaalista',),\n",
       "      'text': 'sosiaalista kanssa'},\n",
       "     {'type': 'tärkeys', 'entities': ('on',), 'text': 'on tärkeä'},\n",
       "     {'type': 'ystävyys',\n",
       "      'entities': ('Marjatta', 'minun'),\n",
       "      'text': 'Marjatta on minun ystävä'},\n",
       "     {'type': 'yhteys', 'entities': ('kenenkin',), 'text': 'kenenkin kanssa'}],\n",
       "    'temporal_refs': [{'type': 'time_of_day',\n",
       "      'text': 'päivä',\n",
       "      'position': (72, 77)},\n",
       "     {'type': 'time_of_day', 'text': 'päivä', 'position': (34, 39)},\n",
       "     {'type': 'time_unit', 'text': 'vuosi', 'position': (61, 66)}],\n",
       "    'key_topics': set()}}},\n",
       " {'question': 'Mitä Eila harrastaa?',\n",
       "  'response': {'answer': 'Tehtävä: Etsi tarkka vastaus annettuun kysymykseen käyttäen vain alla olevaa kontekstia.\\n    \\n            Kysymys: Etsi tarkka vastaus seuraavaan kysymykseen käyttäen alla olevia dokumentteja.\\n    \\n    Kysymys: Mitä Eila harrastaa?\\n    \\n    Dokumentit:\\n    \\nDokumentti 1 (SH-4):\\nMilloin rupattelen kenenkin kanssa ja mitä.\\nMarjatta on minun ystäväni.\\nEn ole terveydessä huomannut ongelmia, mitä nyt flunssaa on ollut.\\nMusiikista pidän ja kahvista.\\nOn mukavaa, kun kysytään kävelylle tai kauppaan m...',\n",
       "   'confidence': 0.9864804124832154,\n",
       "   'sources': [{'text': 'Milloin rupattelen kenenkin kanssa ja mitä.\\nMarjatta on minun ystäväni.\\nEn ole terveydessä huomannut ongelmia, mitä nyt flunssaa on ollut.\\nMusiikista pidän ja kahvista.\\nOn mukavaa, kun kysytään kävelylle tai kauppaan mukaan.\\nRaha-asiat huolettavat ja välillä on epäkuntoisia päiviä, kuten hammassärkyä.\\nJoskus koen yksinäisyyttä, jos ei ole määrätynlainen ihminen lähelläkään.',\n",
       "     'score': 0.9864804124832154,\n",
       "     'source': 'SH-4',\n",
       "     'metadata': {'person_name': 'Eila', 'chunk_index': 1}},\n",
       "    {'text': 'Läheinen omainen on tärkeä elämässä.\\nJos käy kaupassa tai ulkona, niin omainen on siinä tärkeä.\\nJos ei ystävälle sovi lähteä kanssani ulos ja on tärkeää mennä, lähden yksin.\\nTärkeitä henkilöitä on ystäväni Marjatta ja tietysti aviomies.\\nTärkeimmät omaiset.\\nHarrastan milloin mitäkin.\\nMielelläni tykkään mennä tanssimaan.\\nYstävyys on kaikkein tärkein asia.',\n",
       "     'score': 0.9428009891510011,\n",
       "     'source': 'SH-4',\n",
       "     'metadata': {'person_name': 'Eila', 'chunk_index': 3}},\n",
       "    {'text': 'Syntymävuosi: 1942 81v.\\nSukupuoli: nainen Kansalaisuus: suomi Maakunta: uusimaa Talotyyppi: ryhmäkoti Omannäköinen arki Arjessa teen milloin mitäkin.\\nArjen kohokohtia ovat tapaamiset omaisten kanssa.\\nToivon, että omaiset pitävät hyvänä minua.\\nSaan nimenomaan elää omannäköistä elämää ja päättää omista asioistani, tottakai.\\nMilloin rupattelen kenenkin kanssa ja mitä.\\nMarjatta on minun ystäväni.',\n",
       "     'score': 0.9213803958892822,\n",
       "     'source': 'SH-4',\n",
       "     'metadata': {'person_name': 'Eila', 'chunk_index': 0}}],\n",
       "   'context_analysis': {'entities': {'Arjen',\n",
       "     'Arjessa',\n",
       "     'En',\n",
       "     'Harrastan',\n",
       "     'Jos',\n",
       "     'Joskus',\n",
       "     'Kansalaisuus',\n",
       "     'Läheinen',\n",
       "     'Maakunta',\n",
       "     'Marjatta',\n",
       "     'Mielelläni',\n",
       "     'Milloin',\n",
       "     'Musiikista',\n",
       "     'Omannäköinen',\n",
       "     'On',\n",
       "     'Raha',\n",
       "     'Saan',\n",
       "     'Sukupuoli',\n",
       "     'Syntymävuosi',\n",
       "     'Talotyyppi',\n",
       "     'Toivon',\n",
       "     'Tärkeimmät',\n",
       "     'Tärkeitä',\n",
       "     'Ystävyys'},\n",
       "    'relationships': [{'type': 'ystävyys',\n",
       "      'entities': ('Marjatta', 'minun'),\n",
       "      'text': 'Marjatta on minun ystävä'},\n",
       "     {'type': 'yhteys', 'entities': ('kenenkin',), 'text': 'kenenkin kanssa'},\n",
       "     {'type': 'yhteys', 'entities': ('lähteä',), 'text': 'lähteä kanssa'},\n",
       "     {'type': 'tärkeys', 'entities': ('on',), 'text': 'on tärkeä'},\n",
       "     {'type': 'tärkeys', 'entities': ('siinä',), 'text': 'siinä tärkeä'},\n",
       "     {'type': 'tärkeys', 'entities': ('on',), 'text': 'on tärkeä'},\n",
       "     {'type': 'ystävyys',\n",
       "      'entities': ('Marjatta', 'minun'),\n",
       "      'text': 'Marjatta on minun ystävä'},\n",
       "     {'type': 'yhteys', 'entities': ('omaisten',), 'text': 'omaisten kanssa'},\n",
       "     {'type': 'yhteys', 'entities': ('kenenkin',), 'text': 'kenenkin kanssa'}],\n",
       "    'temporal_refs': [{'type': 'time_unit',\n",
       "      'text': 'vuosi',\n",
       "      'position': (7, 12)}],\n",
       "    'key_topics': set()}}},\n",
       " {'question': 'Miten Sulo liikkuu?',\n",
       "  'response': {'answer': 'Tehtävä: Etsi tarkka vastaus annettuun kysymykseen käyttäen vain alla olevaa kontekstia.\\n    \\n            Kysymys: Etsi tarkka vastaus seuraavaan kysymykseen käyttäen alla olevia dokumentteja.\\n    \\n    Kysymys: Miten Sulo liikkuu?\\n    \\n    Dokumentit:\\n    \\nDokumentti 1 (C5-50):\\nLopuksi Koen, että on tärkeää saada sosiaalista kanssakäymistä.\\nSilloin päivä menee hyvin.\\n\\nDokumentti 2 (SH-4):\\nMilloin rupattelen kenenkin kanssa ja mitä.\\nMarjatta on minun ystäväni.\\nEn ole terveydessä huomannut onge...',\n",
       "   'confidence': 0.8683759093284608,\n",
       "   'sources': [{'text': 'Lopuksi Koen, että on tärkeää saada sosiaalista kanssakäymistä.\\nSilloin päivä menee hyvin.',\n",
       "     'score': 0.8683759093284608,\n",
       "     'source': 'C5-50',\n",
       "     'metadata': {'person_name': 'Sulo', 'chunk_index': 3}},\n",
       "    {'text': 'Milloin rupattelen kenenkin kanssa ja mitä.\\nMarjatta on minun ystäväni.\\nEn ole terveydessä huomannut ongelmia, mitä nyt flunssaa on ollut.\\nMusiikista pidän ja kahvista.\\nOn mukavaa, kun kysytään kävelylle tai kauppaan mukaan.\\nRaha-asiat huolettavat ja välillä on epäkuntoisia päiviä, kuten hammassärkyä.\\nJoskus koen yksinäisyyttä, jos ei ole määrätynlainen ihminen lähelläkään.',\n",
       "     'score': 0.8243904411792756,\n",
       "     'source': 'SH-4',\n",
       "     'metadata': {'person_name': 'Eila', 'chunk_index': 1}},\n",
       "    {'text': 'Läheinen omainen on tärkeä elämässä.\\nJos käy kaupassa tai ulkona, niin omainen on siinä tärkeä.\\nJos ei ystävälle sovi lähteä kanssani ulos ja on tärkeää mennä, lähden yksin.\\nTärkeitä henkilöitä on ystäväni Marjatta ja tietysti aviomies.\\nTärkeimmät omaiset.\\nHarrastan milloin mitäkin.\\nMielelläni tykkään mennä tanssimaan.\\nYstävyys on kaikkein tärkein asia.',\n",
       "     'score': 0.7816890001296998,\n",
       "     'source': 'SH-4',\n",
       "     'metadata': {'person_name': 'Eila', 'chunk_index': 3}}],\n",
       "   'context_analysis': {'entities': {'En',\n",
       "     'Harrastan',\n",
       "     'Jos',\n",
       "     'Joskus',\n",
       "     'Koen',\n",
       "     'Lopuksi',\n",
       "     'Läheinen',\n",
       "     'Marjatta',\n",
       "     'Mielelläni',\n",
       "     'Milloin',\n",
       "     'Musiikista',\n",
       "     'On',\n",
       "     'Raha',\n",
       "     'Silloin',\n",
       "     'Tärkeimmät',\n",
       "     'Tärkeitä',\n",
       "     'Ystävyys'},\n",
       "    'relationships': [{'type': 'yhteys',\n",
       "      'entities': ('sosiaalista',),\n",
       "      'text': 'sosiaalista kanssa'},\n",
       "     {'type': 'tärkeys', 'entities': ('on',), 'text': 'on tärkeä'},\n",
       "     {'type': 'ystävyys',\n",
       "      'entities': ('Marjatta', 'minun'),\n",
       "      'text': 'Marjatta on minun ystävä'},\n",
       "     {'type': 'yhteys', 'entities': ('kenenkin',), 'text': 'kenenkin kanssa'},\n",
       "     {'type': 'yhteys', 'entities': ('lähteä',), 'text': 'lähteä kanssa'},\n",
       "     {'type': 'tärkeys', 'entities': ('on',), 'text': 'on tärkeä'},\n",
       "     {'type': 'tärkeys', 'entities': ('siinä',), 'text': 'siinä tärkeä'},\n",
       "     {'type': 'tärkeys', 'entities': ('on',), 'text': 'on tärkeä'}],\n",
       "    'temporal_refs': [{'type': 'time_of_day',\n",
       "      'text': 'päivä',\n",
       "      'position': (72, 77)}],\n",
       "    'key_topics': set()}}},\n",
       " {'question': 'Mistä asioista Eila pitää?',\n",
       "  'response': {'answer': 'Tehtävä: Etsi tarkka vastaus annettuun kysymykseen käyttäen vain alla olevaa kontekstia.\\n    \\n            Kysymys: Etsi tarkka vastaus seuraavaan kysymykseen käyttäen alla olevia dokumentteja.\\n    \\n    Kysymys: Mistä asioista Eila pitää?\\n    \\n    Dokumentit:\\n    \\nDokumentti 1 (C5-50):\\nLopuksi Koen, että on tärkeää saada sosiaalista kanssakäymistä.\\nSilloin päivä menee hyvin.\\n\\nDokumentti 2 (SH-4):\\nSyntymävuosi: 1942 81v.\\nSukupuoli: nainen Kansalaisuus: suomi Maakunta: uusimaa Talotyyppi: ryhmäk...',\n",
       "   'confidence': 1.0093142223358154,\n",
       "   'sources': [{'text': 'Lopuksi Koen, että on tärkeää saada sosiaalista kanssakäymistä.\\nSilloin päivä menee hyvin.',\n",
       "     'score': 1.0093142223358154,\n",
       "     'source': 'C5-50',\n",
       "     'metadata': {'person_name': 'Sulo', 'chunk_index': 3}},\n",
       "    {'text': 'Syntymävuosi: 1942 81v.\\nSukupuoli: nainen Kansalaisuus: suomi Maakunta: uusimaa Talotyyppi: ryhmäkoti Omannäköinen arki Arjessa teen milloin mitäkin.\\nArjen kohokohtia ovat tapaamiset omaisten kanssa.\\nToivon, että omaiset pitävät hyvänä minua.\\nSaan nimenomaan elää omannäköistä elämää ja päättää omista asioistani, tottakai.\\nMilloin rupattelen kenenkin kanssa ja mitä.\\nMarjatta on minun ystäväni.',\n",
       "     'score': 0.9086435079574585,\n",
       "     'source': 'SH-4',\n",
       "     'metadata': {'person_name': 'Eila', 'chunk_index': 0}},\n",
       "    {'text': 'Digitaaliset laitteet Minulla on käytössä perinteinen kännykkä.\\nSe on helppo käyttää.\\nAsuminen Olen asunut tässä asumisyksikössä useamman vuoden.\\nElämän käännekohdat Olen nuorena ollut auto-onnettomuudessa.\\nTämän seurauksena sain vammoja ja minulle jäi afasia.\\nOlin aloittanut tätä ennen opiskelut.\\nMenetin haaveammattini suunnitelmat.\\nLopuksi Koen, että on tärkeää saada sosiaalista kanssakäymistä.',\n",
       "     'score': 0.8095081186294556,\n",
       "     'source': 'C5-50',\n",
       "     'metadata': {'person_name': 'Sulo', 'chunk_index': 2}}],\n",
       "   'context_analysis': {'entities': {'Arjen',\n",
       "     'Arjessa',\n",
       "     'Asuminen',\n",
       "     'Digitaaliset',\n",
       "     'Elämän',\n",
       "     'Kansalaisuus',\n",
       "     'Koen',\n",
       "     'Lopuksi',\n",
       "     'Maakunta',\n",
       "     'Marjatta',\n",
       "     'Menetin',\n",
       "     'Milloin',\n",
       "     'Minulla',\n",
       "     'Olen',\n",
       "     'Olin',\n",
       "     'Omannäköinen',\n",
       "     'Saan',\n",
       "     'Se',\n",
       "     'Silloin',\n",
       "     'Sukupuoli',\n",
       "     'Syntymävuosi',\n",
       "     'Talotyyppi',\n",
       "     'Toivon',\n",
       "     'Tämän'},\n",
       "    'relationships': [{'type': 'yhteys',\n",
       "      'entities': ('sosiaalista',),\n",
       "      'text': 'sosiaalista kanssa'},\n",
       "     {'type': 'tärkeys', 'entities': ('on',), 'text': 'on tärkeä'},\n",
       "     {'type': 'ystävyys',\n",
       "      'entities': ('Marjatta', 'minun'),\n",
       "      'text': 'Marjatta on minun ystävä'},\n",
       "     {'type': 'yhteys', 'entities': ('omaisten',), 'text': 'omaisten kanssa'},\n",
       "     {'type': 'yhteys', 'entities': ('kenenkin',), 'text': 'kenenkin kanssa'},\n",
       "     {'type': 'yhteys',\n",
       "      'entities': ('sosiaalista',),\n",
       "      'text': 'sosiaalista kanssa'},\n",
       "     {'type': 'tärkeys', 'entities': ('on',), 'text': 'on tärkeä'}],\n",
       "    'temporal_refs': [{'type': 'time_of_day',\n",
       "      'text': 'päivä',\n",
       "      'position': (72, 77)},\n",
       "     {'type': 'time_unit', 'text': 'vuosi', 'position': (7, 12)}],\n",
       "    'key_topics': set()}}}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add these imports at the top of your file\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from typing import List, Dict, Any, Optional\n",
    "import re\n",
    "\n",
    "class FinnishRAGAgent:\n",
    "    def __init__(self, base_pipeline: RAGPipeline):\n",
    "        self.pipeline = base_pipeline\n",
    "        self.memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "        self.setup_tools()\n",
    "        \n",
    "    def setup_tools(self):\n",
    "        \"\"\"Initialize search and analysis tools.\"\"\"\n",
    "        self.tools = {\n",
    "            \"semantic_search\": self._semantic_search,\n",
    "            \"exact_match\": self._exact_match_search,\n",
    "            \"context_analysis\": self._analyze_context\n",
    "        }\n",
    "    \n",
    "    def _semantic_search(self, query: str) -> List[Dict]:\n",
    "        \"\"\"Enhanced semantic search with Finnish preprocessing.\"\"\"\n",
    "        preprocessed_query = self._preprocess_finnish_text(query)\n",
    "        base_results = self.pipeline.query(preprocessed_query, top_k=5)\n",
    "        \n",
    "        # Enhance results with Finnish-specific scoring\n",
    "        enhanced_results = []\n",
    "        for result in base_results['sources']:\n",
    "            score = self._calculate_finnish_relevance(preprocessed_query, result['text'])\n",
    "            enhanced_results.append({\n",
    "                'text': result['text'],\n",
    "                'score': score * result['score'],  # Combine scores\n",
    "                'source': result['document_id'],\n",
    "                'metadata': {\n",
    "                    'person_name': result['person_name'],\n",
    "                    'chunk_index': result['chunk_index']\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        return sorted(enhanced_results, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "    def _exact_match_search(self, query: str) -> List[Dict]:\n",
    "        \"\"\"Direct text matching with Finnish normalization.\"\"\"\n",
    "        normalized_query = self._normalize_finnish_text(query)\n",
    "        \n",
    "        # Use pipeline's collection directly\n",
    "        collection = self.pipeline.collection\n",
    "        results = collection.query(\n",
    "            expr=f'text like \"%{normalized_query}%\"',\n",
    "            output_fields=[\"text\", \"person_name\", \"document_id\", \"chunk_index\"]\n",
    "        )\n",
    "        \n",
    "        return [\n",
    "            {\n",
    "                'text': r['text'],\n",
    "                'score': 1.0, \n",
    "                'source': r['document_id'],\n",
    "                'metadata': {'person_name': r['person_name']}\n",
    "            }\n",
    "            for r in results\n",
    "        ]\n",
    "\n",
    "    def _analyze_context(self, passages: List[Dict]) -> Dict:\n",
    "        \"\"\"Analyze relationships and context in Finnish text.\"\"\"\n",
    "        context_data = {\n",
    "            'entities': set(),\n",
    "            'relationships': [],\n",
    "            'temporal_refs': [],\n",
    "            'key_topics': set()\n",
    "        }\n",
    "        \n",
    "        for passage in passages:\n",
    "            text = passage['text']\n",
    "            \n",
    "            # Extract Finnish names and entities\n",
    "            entities = self._extract_finnish_entities(text)\n",
    "            context_data['entities'].update(entities)\n",
    "            \n",
    "            # Find relationships\n",
    "            relationships = self._find_relationships(text)\n",
    "            context_data['relationships'].extend(relationships)\n",
    "            \n",
    "            # Extract temporal information\n",
    "            temporal = self._extract_temporal_refs(text)\n",
    "            context_data['temporal_refs'].extend(temporal)\n",
    "            \n",
    "        return context_data\n",
    "\n",
    "    def _preprocess_finnish_text(self, text: str) -> str:\n",
    "        \"\"\"Preprocess Finnish text for better matching.\"\"\"\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text.strip())\n",
    "        \n",
    "        # Handle common Finnish abbreviations\n",
    "        abbreviations = {\n",
    "            'esim.': 'esimerkiksi',\n",
    "            'ns.': 'niin sanottu',\n",
    "            'jne.': 'ja niin edelleen'\n",
    "        }\n",
    "        for abbr, full in abbreviations.items():\n",
    "            text = text.replace(abbr, full)\n",
    "        \n",
    "        return text\n",
    "\n",
    "    # Add these methods to your FinnishRAGAgent class\n",
    "\n",
    "    def _find_relationships(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Find relationships in Finnish text.\"\"\"\n",
    "        relationships = []\n",
    "        \n",
    "        # Common Finnish relationship patterns\n",
    "        patterns = [\n",
    "            (r'(\\w+)\\s+on\\s+(\\w+)\\s+ystävä', 'ystävyys'),\n",
    "            (r'(\\w+)\\s+asuu\\s+(\\w+)', 'asuminen'),\n",
    "            (r'(\\w+)\\s+tekee\\s+(\\w+)', 'toiminta'),\n",
    "            (r'(\\w+)\\s+pitää\\s+(\\w+)', 'pitäminen'),\n",
    "            (r'(\\w+)\\s+kanssa', 'yhteys'),\n",
    "            (r'(\\w+)\\s+tärkeä', 'tärkeys'),\n",
    "            (r'(\\w+)\\s+auttaa\\s+(\\w+)', 'auttaminen')\n",
    "        ]\n",
    "        \n",
    "        for pattern, rel_type in patterns:\n",
    "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                relationships.append({\n",
    "                    'type': rel_type,\n",
    "                    'entities': match.groups(),\n",
    "                    'text': match.group(0)\n",
    "                })\n",
    "        \n",
    "        return relationships\n",
    "\n",
    "    def _extract_temporal_refs(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Extract temporal references from Finnish text.\"\"\"\n",
    "        temporal_refs = []\n",
    "        \n",
    "        # Common Finnish temporal patterns\n",
    "        patterns = [\n",
    "            (r'\\d+\\s*vuotta', 'duration'),\n",
    "            (r'\\d+\\s*vuotias', 'age'),\n",
    "            (r'(maanantai|tiistai|keskiviikko|torstai|perjantai|lauantai|sunnuntai)', 'weekday'),\n",
    "            (r'(tammikuu|helmikuu|maaliskuu|huhtikuu|toukokuu|kesäkuu|heinäkuu|elokuu|syyskuu|lokakuu|marraskuu|joulukuu)', 'month'),\n",
    "            (r'(aamu|päivä|ilta|yö)', 'time_of_day'),\n",
    "            (r'(eilen|tänään|huomenna)', 'relative_day'),\n",
    "            (r'(viikko|kuukausi|vuosi)', 'time_unit')\n",
    "        ]\n",
    "        \n",
    "        for pattern, ref_type in patterns:\n",
    "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                temporal_refs.append({\n",
    "                    'type': ref_type,\n",
    "                    'text': match.group(0),\n",
    "                    'position': match.span()\n",
    "                })\n",
    "        \n",
    "        return temporal_refs\n",
    "    \n",
    "    def _identify_key_topics(self, text: str) -> set:\n",
    "        \"\"\"Identify key topics in Finnish text.\"\"\"\n",
    "        topics = set()\n",
    "        \n",
    "        # Common Finnish topic indicators\n",
    "        key_patterns = [\n",
    "            (r'tärkeä\\w*\\s+(\\w+)', 'importance'),\n",
    "            (r'harrastaa\\w*\\s+(\\w+)', 'hobby'),\n",
    "            (r'pitää\\w*\\s+(\\w+)', 'preference'),\n",
    "            (r'ongelma\\w*\\s+(\\w+)', 'problem'),\n",
    "            (r'tavoite\\w*\\s+(\\w+)', 'goal')\n",
    "        ]\n",
    "        \n",
    "        for pattern, topic_type in key_patterns:\n",
    "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                if len(match.groups()) > 0:\n",
    "                    topics.add(f\"{topic_type}:{match.group(1)}\")\n",
    "        \n",
    "        return topics\n",
    "\n",
    "    def _normalize_finnish_text(self, text: str) -> str:\n",
    "        \"\"\"Normalize Finnish text for comparison.\"\"\"\n",
    "        text = text.lower().strip()\n",
    "        text = re.sub(r'[^\\w\\s\\äöåÄÖÅ]', '', text)\n",
    "        return text\n",
    "\n",
    "    def _calculate_finnish_relevance(self, query: str, text: str) -> float:\n",
    "        \"\"\"Calculate relevance score for Finnish text.\"\"\"\n",
    "        score = 1.0\n",
    "        \n",
    "        # Boost score for Finnish grammar patterns\n",
    "        if re.search(r'\\b(ssa|ssä|sta|stä|lla|llä|lta|ltä)\\b', text):\n",
    "            score *= 1.1\n",
    "            \n",
    "        # Boost for question-answer pairs\n",
    "        if '?' in query and '.' in text[:100]:\n",
    "            score *= 1.2\n",
    "            \n",
    "        # Check for named entity matches\n",
    "        query_entities = self._extract_finnish_entities(query)\n",
    "        text_entities = self._extract_finnish_entities(text)\n",
    "        if query_entities & text_entities:\n",
    "            score *= 1.3\n",
    "            \n",
    "        return min(1.0, score)\n",
    "\n",
    "    def _extract_finnish_entities(self, text: str) -> set:\n",
    "        \"\"\"Extract Finnish named entities.\"\"\"\n",
    "        entities = set()\n",
    "        \n",
    "        # Match Finnish names (simplified)\n",
    "        name_pattern = r'\\b[A-ZÄÖÅ][a-zäöå]+\\b'\n",
    "        entities.update(re.findall(name_pattern, text))\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def _generate_response(self, query: str, results: List[Dict], context: Dict) -> Dict:\n",
    "        \"\"\"Generate an enhanced response with proper citations.\"\"\"\n",
    "        if not results:\n",
    "            return {\n",
    "                \"answer\": \"En löytänyt vastausta kysymykseesi saatavilla olevista dokumenteista.\",\n",
    "                \"confidence\": 0.0,\n",
    "                \"sources\": []\n",
    "            }\n",
    "    \n",
    "        try:\n",
    "            # Build context-rich query\n",
    "            enhanced_query = self._build_enhanced_query(query, results)\n",
    "            \n",
    "            # Get response from base pipeline\n",
    "            response = self.pipeline.query(enhanced_query)\n",
    "            \n",
    "            # Process and validate response\n",
    "            answer = self._process_response(response, results, query)\n",
    "            \n",
    "            return {\n",
    "                \"answer\": answer,\n",
    "                \"confidence\": max(r['score'] for r in results),\n",
    "                \"sources\": results[:3],\n",
    "                \"context_analysis\": context\n",
    "            }\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in response generation: {str(e)}\")\n",
    "            return {\n",
    "                \"answer\": \"Virhe vastauksen muodostamisessa.\",\n",
    "                \"confidence\": 0.0,\n",
    "                \"sources\": results[:3],\n",
    "                \"context_analysis\": context\n",
    "            }\n",
    "\n",
    "    def _build_enhanced_query(self, query: str, results: List[Dict]) -> str:\n",
    "        \"\"\"Build a context-rich query with relevant information.\"\"\"\n",
    "        # Analyze query intent\n",
    "        query_type = self._analyze_query_type(query)\n",
    "        \n",
    "        # Build appropriate context structure based on query type\n",
    "        context = f\"\"\"Etsi tarkka vastaus seuraavaan kysymykseen käyttäen alla olevia dokumentteja.\n",
    "    \n",
    "    Kysymys: {query}\n",
    "    \n",
    "    Dokumentit:\n",
    "    \"\"\"\n",
    "        \n",
    "        # Add relevant documents with source information\n",
    "        for i, result in enumerate(results[:3], 1):\n",
    "            context += f\"\\nDokumentti {i} ({result['source']}):\\n{result['text'].strip()}\\n\"\n",
    "    \n",
    "        # Add query-specific instructions\n",
    "        context += self._get_query_instructions(query_type)\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def _analyze_query_type(self, query: str) -> str:\n",
    "        \"\"\"Analyze the type of query for better response structuring.\"\"\"\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        # Extract query characteristics without hardcoding specific names or values\n",
    "        characteristics = {\n",
    "            'personal_info': any(word in query_lower for word in ['kuka', 'kenen', 'kenelle']),\n",
    "            'age_related': any(word in query_lower for word in ['vanha', 'ikä', 'syntymä', 'vuosi']),\n",
    "            'activity_related': any(word in query_lower for word in ['tekee', 'harrastaa', 'pitää', 'tykkää']),\n",
    "            'ability_related': any(word in query_lower for word in ['pystyy', 'osaa', 'liikkuu', 'käyttää']),\n",
    "            'preference_related': any(word in query_lower for word in ['pitää', 'tykkää', 'haluaa']),\n",
    "            'relationship_related': any(word in query_lower for word in ['ystävä', 'tärkeä', 'läheinen'])\n",
    "        }\n",
    "        \n",
    "        # Return the most likely query type\n",
    "        return max(characteristics.items(), key=lambda x: x[1])[0]\n",
    "    \n",
    "    def _get_query_instructions(self, query_type: str) -> str:\n",
    "        \"\"\"Get specific instructions based on query type.\"\"\"\n",
    "        instructions = {\n",
    "            'personal_info': \"\"\"\n",
    "        Ohje: \n",
    "        1. Etsi henkilöön liittyvät suorat maininnat\n",
    "        2. Käytä suoria lainauksia henkilöiden nimistä ja suhteista\n",
    "        3. Mainitse dokumentin lähde\"\"\",\n",
    "                \n",
    "                'age_related': \"\"\"\n",
    "        Ohje:\n",
    "        1. Etsi tarkat ikä- ja syntymävuositiedot\n",
    "        2. Ilmoita sekä ikä että syntymävuosi jos molemmat löytyvät\n",
    "        3. Mainitse dokumentin lähde\"\"\",\n",
    "                \n",
    "                'activity_related': \"\"\"\n",
    "        Ohje:\n",
    "        1. Etsi kaikki mainitut aktiviteetit ja harrastukset\n",
    "        2. Käytä suoria lainauksia aktiviteettien kuvauksista\n",
    "        3. Mainitse dokumentin lähde\"\"\",\n",
    "                \n",
    "                'ability_related': \"\"\"\n",
    "        Ohje:\n",
    "        1. Etsi kuvaukset henkilön kyvyistä ja toiminnasta\n",
    "        2. Käytä suoria lainauksia toimintakyvyn kuvauksista\n",
    "        3. Mainitse dokumentin lähde\"\"\",\n",
    "                \n",
    "                'preference_related': \"\"\"\n",
    "        Ohje:\n",
    "        1. Etsi kaikki mainitut mieltymykset ja kiinnostukset\n",
    "        2. Käytä suoria lainauksia mieltymysten kuvauksista\n",
    "        3. Mainitse dokumentin lähde\"\"\",\n",
    "                \n",
    "                'relationship_related': \"\"\"\n",
    "        Ohje:\n",
    "        1. Etsi kuvaukset ihmissuhteista ja tärkeistä henkilöistä\n",
    "        2. Käytä suoria lainauksia suhteiden kuvauksista\n",
    "        3. Mainitse dokumentin lähde\"\"\"\n",
    "            }\n",
    "            \n",
    "        return instructions.get(query_type, \"\"\"\n",
    "        Ohje:\n",
    "        1. Etsi suora vastaus kysymykseen dokumenteista\n",
    "        2. Käytä suoria lainauksia\n",
    "        3. Mainitse dokumentin lähde\"\"\")\n",
    "    \n",
    "    def _process_response(self, response: Dict, results: List[Dict], query: str) -> str:\n",
    "        \"\"\"Process and validate the response.\"\"\"\n",
    "        if not response or 'answer' not in response:\n",
    "            return \"En löytänyt vastausta annetusta kontekstista.\"\n",
    "        \n",
    "        answer = response['answer'].strip()\n",
    "        \n",
    "        # Validate the answer has proper citations\n",
    "        if not any(f\"Dokumentin {result['source']}\" in answer for result in results):\n",
    "            # Try to add source information if missing\n",
    "            for result in results:\n",
    "                if any(quote in result['text'] for quote in re.findall(r'\"([^\"]*)\"', answer)):\n",
    "                    answer = f\"Dokumentin {result['source']} mukaan {answer}\"\n",
    "                    break\n",
    "        \n",
    "        # Validate answer has quotes\n",
    "        if '\"' not in answer and any(result['text'] in answer for result in results):\n",
    "            answer = re.sub(r'(Dokumentin [^\\s]+ mukaan) (.*)', r'\\1 \"\\2\"', answer)\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def process_query(self, query: str) -> Dict:\n",
    "        \"\"\"Process a query using all available tools.\"\"\"\n",
    "        try:\n",
    "            # Step 1: Get semantic search results\n",
    "            semantic_results = self._semantic_search(query)\n",
    "            \n",
    "            # Step 2: Check for exact matches if needed\n",
    "            if not semantic_results or max(r['score'] for r in semantic_results) < 0.5:\n",
    "                exact_matches = self._exact_match_search(query)\n",
    "                all_results = semantic_results + exact_matches\n",
    "            else:\n",
    "                all_results = semantic_results\n",
    "            \n",
    "            # Step 3: Analyze context\n",
    "            context_data = self._analyze_context(all_results[:3])\n",
    "            \n",
    "            # Step 4: Generate enhanced response\n",
    "            response = self._generate_response(query, all_results, context_data)\n",
    "            \n",
    "            # Store in memory for future context\n",
    "            self.memory.save_context(\n",
    "                {\"input\": query},\n",
    "                {\"output\": response['answer']}\n",
    "            )\n",
    "            \n",
    "            # Print formatted response for immediate feedback\n",
    "            print(\"\\nVASTAUS:\")\n",
    "            print(\"-\" * 40)\n",
    "            print(response['answer'])\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    \n",
    "def run_improved_rag_tests():\n",
    "    try:\n",
    "        print(\"\\nStarting RAG test execution...\")\n",
    "        \n",
    "        # Initialize base pipeline\n",
    "        print(\"\\n1. Initializing base pipeline...\")\n",
    "        base_pipeline = RAGPipeline()\n",
    "        print(\"   ✓ Base pipeline initialized\")\n",
    "        \n",
    "        # Initialize Finnish RAG Agent\n",
    "        print(\"\\n2. Creating Finnish RAG Agent...\")\n",
    "        agent = FinnishRAGAgent(base_pipeline)\n",
    "        print(\"   ✓ Agent created\")\n",
    "        \n",
    "        # Process documents\n",
    "        print(\"\\n3. Processing documents...\")\n",
    "        folder_path = '/home/jovyan/work/notebooks/data/'\n",
    "        base_pipeline.process_documents(folder_path)\n",
    "        print(\"   ✓ Documents processed\")\n",
    "        \n",
    "        # Define test questions\n",
    "        test_questions = [\n",
    "            \"Kuka on tärkeitä henkilöitä Eilalle?\",\n",
    "            \"Kuinka vanha on Sulo ja mikä on hänen syntymävuotensa?\",\n",
    "            \"Mitä Eila harrastaa?\",\n",
    "            \"Miten Sulo liikkuu?\",\n",
    "            \"Mistä asioista Eila pitää?\"\n",
    "        ]\n",
    "        \n",
    "        # Run tests through the agent\n",
    "        print(\"\\n4. Running test questions...\")\n",
    "        results = []\n",
    "        for i, question in enumerate(test_questions, 1):\n",
    "            try:\n",
    "                print(f\"\\nKysymys {i}/{len(test_questions)}:\")\n",
    "                print(\"-\"*80)\n",
    "                print(f\"{question}\")\n",
    "                print(\"-\"*80)\n",
    "                \n",
    "                response = agent.process_query(question)\n",
    "                results.append({\n",
    "                    \"question\": question,\n",
    "                    \"response\": response\n",
    "                })\n",
    "                \n",
    "                print(\"\\nVastaus:\")\n",
    "                print(response.get('answer', 'Ei vastausta'))\n",
    "                print(\"\\nLähde:\")\n",
    "                for src in response.get('sources', [])[:1]:\n",
    "                    print(f\"- {src['source']}: {src['text'][:150]}...\")\n",
    "                print(\"-\"*80)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Virhe kysymyksen käsittelyssä: {str(e)}\")\n",
    "        \n",
    "        print(\"\\n5. Testaus valmis\")\n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nVirhe testauksessa: {str(e)}\")\n",
    "        raise\n",
    "## call the function so that it runs\n",
    "run_improved_rag_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983bd66f-badf-4729-ba72-b5367c1628b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
