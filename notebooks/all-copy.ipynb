{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## section 1\n",
    "# Imports\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "import logging\n",
    "from typing import List, Dict, Any\n",
    "from docx import Document\n",
    "from tqdm.notebook import tqdm\n",
    "from pymilvus import connections, Collection, utility, CollectionSchema, FieldSchema, DataType\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "import hashlib\n",
    "import time\n",
    "from typing import Optional\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "## Env \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "\n",
    "# Utility Functions\n",
    "def print_status(section_name: str, status: bool, message: str = \"\"):\n",
    "    \"\"\"Print status of a section with colored output.\"\"\"\n",
    "    status_str = \"✅ SUCCESS\" if status else \"❌ FAILED\"\n",
    "    print(f\"\\n{status_str} | {section_name}\")\n",
    "    if message:\n",
    "        print(f\"  └─ {message}\")\n",
    "\n",
    "def verify_section(section_number: int, verification_func) -> bool:\n",
    "    \"\"\"Verify if a section was executed successfully.\"\"\"\n",
    "    try:\n",
    "        result = verification_func()\n",
    "        print_status(f\"Section {section_number} Verification\", True, \"Successfully executed\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print_status(f\"Section {section_number} Verification\", False, f\"Error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def get_model_embedding_dim(model_name: str = \"TurkuNLP/bert-base-finnish-cased-v1\") -> int:\n",
    "    \"\"\"Get embedding dimension from model config.\"\"\"\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    return model.config.hidden_size\n",
    "\n",
    "def check_cuda():\n",
    "    \"\"\"Check CUDA availability and print status.\"\"\"\n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            device_name = torch.cuda.get_device_name(0)\n",
    "            print_status(\"CUDA Check\", True, f\"Using GPU: {device_name}\")\n",
    "            return True\n",
    "        else:\n",
    "            print_status(\"CUDA Check\", True, \"Using CPU\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print_status(\"CUDA Check\", False, str(e))\n",
    "        return False\n",
    "\n",
    "def ensure_stopwords_downloaded(language='finnish'):\n",
    "    \"\"\"Download NLTK stopwords and print status.\"\"\"\n",
    "    try:\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "        print_status(\"NLTK Setup\", True, f\"Downloaded {language} stopwords\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print_status(\"NLTK Setup\", False, str(e))\n",
    "        return False\n",
    "\n",
    "# Global Constants\n",
    "#EMBEDDING_DIM = get_model_embedding_dim()\n",
    "EMBEDDING_DIM = 768\n",
    "MILVUS_HOST = \"87.92.59.201\"\n",
    "MILVUS_PORT = \"19530\"\n",
    "MILVUS_ALIAS = \"default\"\n",
    "\n",
    "# Initial Setup Verification\n",
    "def verify_initial_setup():\n",
    "    check_cuda()\n",
    "    ensure_stopwords_downloaded()\n",
    "    print_status(\"Embedding Dimension\", True, f\"Using dimension: {EMBEDDING_DIM}\")\n",
    "    \n",
    "# Run initial verification\n",
    "verify_initial_setup()\n",
    "\n",
    "## section2\n",
    "# import just in case it doesn't load properly!\n",
    "from typing import Optional, List, Dict\n",
    "import logging\n",
    "import hashlib\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "class TokenManager:\n",
    "    def __init__(self, max_tokens: int = 2048):\n",
    "        self.max_tokens = max_tokens\n",
    "        \n",
    "    def truncate_context(self, context: List[Dict], tokenizer) -> List[Dict]:\n",
    "        \"\"\"Truncate context while preserving most relevant information.\"\"\"\n",
    "        total_tokens = 0\n",
    "        truncated_context = []\n",
    "        \n",
    "        # Sort by relevance\n",
    "        sorted_context = sorted(context, key=lambda x: x.get('score', 0), reverse=True)\n",
    "        \n",
    "        for doc in sorted_context:\n",
    "            # Get tokens for current document\n",
    "            try:\n",
    "                tokens = tokenizer.encode(doc['text'])\n",
    "                token_count = len(tokens)\n",
    "                \n",
    "                # Check if adding this document would exceed limit\n",
    "                if total_tokens + token_count > self.max_tokens:\n",
    "                    # If we can fit a partial document, do so\n",
    "                    remaining_tokens = self.max_tokens - total_tokens\n",
    "                    if remaining_tokens > 100:  # Only if worth including\n",
    "                        partial_tokens = tokens[:remaining_tokens]\n",
    "                        truncated_text = tokenizer.decode(partial_tokens, skip_special_tokens=True)\n",
    "                        # End at a complete sentence if possible\n",
    "                        last_period = truncated_text.rfind('.')\n",
    "                        if last_period > 0:\n",
    "                            truncated_text = truncated_text[:last_period + 1]\n",
    "                        truncated_context.append({\n",
    "                            **doc,\n",
    "                            'text': truncated_text,\n",
    "                            'truncated': True\n",
    "                        })\n",
    "                    break\n",
    "                \n",
    "                total_tokens += token_count\n",
    "                truncated_context.append(doc)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error truncating document: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        return truncated_context\n",
    "\n",
    "    def format_for_llm(self, context: List[Dict], question: str) -> str:\n",
    "        \"\"\"Format truncated context for LLM input.\"\"\"\n",
    "        formatted_parts = [\n",
    "            \"Kysymys: \" + question,\n",
    "            \"\\nKonteksti:\"\n",
    "        ]\n",
    "        \n",
    "        for i, doc in enumerate(context, 1):\n",
    "            doc_text = (\n",
    "                f\"\\nDokumentti {i}:\\n\"\n",
    "                f\"Lähde: {doc.get('source', 'Tuntematon')}\\n\"\n",
    "                f\"Teksti: {doc['text']}\\n\"\n",
    "                f\"{'(Katkaistu)' if doc.get('truncated') else ''}\"\n",
    "            )\n",
    "            formatted_parts.append(doc_text)\n",
    "            \n",
    "        return \"\\n\".join(formatted_parts)\n",
    "class TokenLimitManager:\n",
    "    def __init__(self, max_tokens: int = 2048):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.tokenizer = None\n",
    "        \n",
    "    def setup_tokenizer(self, tokenizer):\n",
    "        \"\"\"Set up tokenizer for token counting.\"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def truncate_text(self, text: str, max_tokens: Optional[int] = None) -> str:\n",
    "        \"\"\"Truncate text to fit within token limit.\"\"\"\n",
    "        if not max_tokens:\n",
    "            max_tokens = self.max_tokens\n",
    "            \n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        if len(tokens) <= max_tokens:\n",
    "            return text\n",
    "            \n",
    "        # Truncate tokens and decode back to text\n",
    "        truncated_tokens = tokens[:max_tokens]\n",
    "        truncated_text = self.tokenizer.decode(truncated_tokens, skip_special_tokens=True)\n",
    "        \n",
    "        # Try to end at a sentence boundary\n",
    "        last_period = truncated_text.rfind('.')\n",
    "        if last_period > 0:\n",
    "            return truncated_text[:last_period + 1]\n",
    "        return truncated_text\n",
    "        \n",
    "# Core Pipeline Components\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, chunk_size=400, chunk_overlap=80):\n",
    "        try:\n",
    "            self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "                separators=[\n",
    "                    \"\\n\\n\",  # First split on double newlines\n",
    "                    \"\\n\",    # Then single newlines\n",
    "                    \".\",     # Then sentence endings\n",
    "                    \":\",     # Then colons (common in Finnish formatting)\n",
    "                    \";\",     # Then semicolons\n",
    "                    \",\",     # Then commas\n",
    "                    \" \",     # Finally, split on spaces if needed\n",
    "                    \"\"\n",
    "                ],\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                length_function=len,\n",
    "                keep_separator=True,\n",
    "                add_start_index=True\n",
    "            )\n",
    "\n",
    "            # Add Finnish-specific cleaning patterns\n",
    "            self.clean_patterns = [\n",
    "                (r'\\s+', ' '),  # Normalize whitespace\n",
    "                (r'[\\(\\{\\[\\]\\}\\)]', ''),  # Remove brackets\n",
    "                (r'[^\\w\\s\\.\\,\\?\\!\\-\\:\\;äöåÄÖÅ]', ''),  # Keep Finnish characters\n",
    "                (r'\\s+\\.', '.'),  # Fix spacing around periods\n",
    "                (r'\\.+', '.'),  # Normalize multiple periods\n",
    "            ]\n",
    "            print_status(\"Document Processor\", True, \"Initialized with Finnish-optimized settings\")\n",
    "        except Exception as e:\n",
    "            print_status(\"Document Processor\", False, str(e))\n",
    "            raise\n",
    "\n",
    "    def _calculate_chunk_importance(self, chunk: str) -> float:\n",
    "        \"\"\"Calculate importance score for chunk based on Finnish text patterns.\"\"\"\n",
    "        score = 1.0\n",
    "        \n",
    "        # Key phrase indicators (extended for improved scoring)\n",
    "        key_phrases = {\n",
    "            'high': ['erittäin tärkeä', 'merkittävä', 'olennainen', 'keskeinen'],\n",
    "            'medium': ['tärkeä', 'huomattava', 'kiinnostava'],\n",
    "            'low': ['mainittava', 'mahdollinen']\n",
    "        }\n",
    "        \n",
    "        # Check for key phrases with weighted importance\n",
    "        chunk_lower = chunk.lower()\n",
    "        for phrase in key_phrases['high']:\n",
    "            if phrase in chunk_lower:\n",
    "                score *= 1.3\n",
    "        for phrase in key_phrases['medium']:\n",
    "            if phrase in chunk_lower:\n",
    "                score *= 1.2\n",
    "        for phrase in key_phrases['low']:\n",
    "            if phrase in chunk_lower:\n",
    "                score *= 1.1\n",
    "        \n",
    "        # Prefer complete sentences\n",
    "        sentence_count = len(re.findall(r'[.!?]+', chunk))\n",
    "        if sentence_count > 0:\n",
    "            score *= (1 + (0.1 * sentence_count))\n",
    "        \n",
    "        # Prefer chunks with personal pronouns\n",
    "        if any(pronoun in chunk_lower for pronoun in [\"minä\", \"minun\", \"minua\", \"minulla\"]):\n",
    "            score *= 1.15\n",
    "        \n",
    "        # Prefer chunks with specific details\n",
    "        if re.search(r'\\d+', chunk):  # Contains numbers\n",
    "            score *= 1.1\n",
    "        \n",
    "        # Prefer chunks with names\n",
    "        if re.search(r'[A-ZÄÖÅ][a-zäöå]+', chunk):\n",
    "            score *= 1.1\n",
    "        \n",
    "        return score\n",
    "\n",
    "    def process_document(self, file_path: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process document with improved metadata and chunking.\"\"\"\n",
    "        try:\n",
    "            # Read document\n",
    "            doc = Document(file_path)\n",
    "            text = \"\\n\".join([para.text for para in doc.paragraphs if para.text.strip()])\n",
    "            \n",
    "            # Extract metadata\n",
    "            filename = os.path.basename(file_path)\n",
    "            name, age, doc_id = self.extract_metadata_from_filename(filename)\n",
    "            \n",
    "            # Preprocess and split text\n",
    "            clean_text = self.preprocess_text(text)\n",
    "            chunks = self.text_splitter.split_text(clean_text)\n",
    "            \n",
    "            # Create chunks with enhanced metadata\n",
    "            processed_chunks = []\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                # Calculate semantic importance score\n",
    "                importance_score = self._calculate_chunk_importance(chunk)\n",
    "                \n",
    "                processed_chunks.append({\n",
    "                    \"text\": chunk,\n",
    "                    \"metadata\": {\n",
    "                        \"source\": filename,\n",
    "                        \"person_name\": name,\n",
    "                        \"person_age\": age,\n",
    "                        \"document_id\": doc_id,\n",
    "                        \"chunk_index\": i,\n",
    "                        \"importance_score\": importance_score,\n",
    "                        \"chunk_length\": len(chunk),\n",
    "                        \"contains_question\": \"?\" in chunk,\n",
    "                    }\n",
    "                })\n",
    "            \n",
    "            print_status(\"Document Processing\", True, \n",
    "                        f\"Processed {filename} into {len(processed_chunks)} chunks\")\n",
    "            return processed_chunks\n",
    "            \n",
    "        except Exception as e:\n",
    "            print_status(\"Document Processing\", False, f\"Error processing {file_path}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def extract_metadata_from_filename(self, filename: str) -> tuple:\n",
    "        \"\"\"Extract metadata from filename.\"\"\"\n",
    "        title = os.path.splitext(filename)[0]\n",
    "        match = re.match(r'([A-Za-z]+)\\s+(\\d{1,3})v\\s+([A-Za-z0-9\\-]+)', title)\n",
    "        if match:\n",
    "            return match.group(1), int(match.group(2)), match.group(3)\n",
    "        return None, None, None\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize Finnish text with improved handling.\"\"\"\n",
    "        try:\n",
    "            # Apply all cleaning patterns\n",
    "            for pattern, replacement in self.clean_patterns:\n",
    "                text = re.sub(pattern, replacement, text)\n",
    "            \n",
    "            # Ensure proper sentence boundaries\n",
    "            text = re.sub(r'([.!?])\\s*([A-ZÄÖÅ])', r'\\1\\n\\2', text)\n",
    "            \n",
    "            # Remove extra whitespace while preserving paragraph breaks\n",
    "            text = '\\n'.join(line.strip() for line in text.split('\\n'))\n",
    "            return text.strip()\n",
    "        except Exception as e:\n",
    "            print_status(\"Text Preprocessing\", False, str(e))\n",
    "            raise\n",
    "\n",
    "class MilvusManager:\n",
    "    def __init__(self, host: str = \"87.92.59.201\", port: str = \"19530\", alias: str = \"default\"):\n",
    "        self.host = str(host)\n",
    "        self.port = str(port)\n",
    "        self.alias = alias\n",
    "        self.connected = False\n",
    "        self.connect()\n",
    "        \n",
    "    def connect(self):\n",
    "        \"\"\"Establish connection to Milvus.\"\"\"\n",
    "        try:\n",
    "            try:\n",
    "                connections.remove_connection(alias=self.alias)\n",
    "                print_status(\"Milvus Connection\", True, \"Cleaned up existing connection\")\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            connections.connect(\n",
    "                alias=self.alias,\n",
    "                host=self.host,\n",
    "                port=self.port,\n",
    "                timeout=10.0\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                utility.get_server_version()\n",
    "                self.connected = True\n",
    "                print_status(\"Milvus Connection\", True, f\"Connected to {self.host}:{self.port}\")\n",
    "            except Exception as ve:\n",
    "                raise Exception(f\"Connection verification failed: {str(ve)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.connected = False\n",
    "            print_status(\"Milvus Connection\", False, str(e))\n",
    "            raise\n",
    "\n",
    "    def create_collection(self, collection_name: str = \"document_embeddings\"):\n",
    "        \"\"\"Create Milvus collection with appropriate schema.\"\"\"\n",
    "        try:\n",
    "            if utility.has_collection(collection_name):\n",
    "                Collection(name=collection_name).drop()\n",
    "                print_status(\"Milvus Collection\", True, f\"Dropped existing collection: {collection_name}\")\n",
    "                \n",
    "            fields = [\n",
    "                FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "                FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "                FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=EMBEDDING_DIM),\n",
    "                FieldSchema(name=\"person_name\", dtype=DataType.VARCHAR, max_length=100),\n",
    "                FieldSchema(name=\"person_age\", dtype=DataType.INT64),\n",
    "                FieldSchema(name=\"document_id\", dtype=DataType.VARCHAR, max_length=100),\n",
    "                FieldSchema(name=\"chunk_index\", dtype=DataType.INT64)\n",
    "            ]\n",
    "            \n",
    "            schema = CollectionSchema(\n",
    "                fields=fields,\n",
    "                description=\"Document embeddings collection\",\n",
    "                enable_dynamic_field=False\n",
    "            )\n",
    "            collection = Collection(name=collection_name, schema=schema)\n",
    "            \n",
    "            self.create_and_load_index(collection)\n",
    "            \n",
    "            print_status(\"Milvus Collection\", True, f\"Created new collection: {collection_name} with dim={EMBEDDING_DIM}\")\n",
    "            return collection\n",
    "        except Exception as e:\n",
    "            print_status(\"Milvus Collection\", False, str(e))\n",
    "            raise\n",
    "\n",
    "    def create_and_load_index(self, collection):\n",
    "        try:\n",
    "            index_params = {\n",
    "                \"metric_type\": \"IP\",\n",
    "                \"index_type\": \"IVF_FLAT\",\n",
    "                \"params\": {\"nlist\": 1024}\n",
    "            }\n",
    "            collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "            print_status(\"Index Creation\", True, \"Created IVF_FLAT index\")\n",
    "            \n",
    "            collection.load()\n",
    "            print_status(\"Collection Load\", True, \"Loaded collection into memory\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print_status(\"Index Creation\", False, str(e))\n",
    "            raise\n",
    "\n",
    "    def reload_collection(self, collection_name: str = \"document_embeddings\"):\n",
    "        try:\n",
    "            if utility.has_collection(collection_name):\n",
    "                collection = Collection(name=collection_name)\n",
    "                collection.load()\n",
    "                print_status(\"Collection Reload\", True, f\"Reloaded collection: {collection_name}\")\n",
    "                return collection\n",
    "            else:\n",
    "                raise Exception(f\"Collection {collection_name} does not exist\")\n",
    "        except Exception as e:\n",
    "            print_status(\"Collection Reload\", False, str(e))\n",
    "            raise\n",
    "class EmbeddingCache:\n",
    "    def __init__(self, cache_dir: str = \"/scratch/project_2011638/safdarih/huggingface_cache/embedding_cache\"):\n",
    "        self.cache_dir = cache_dir\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        self.cache = {}\n",
    "        \n",
    "    def get_cache_path(self, text_hash: str) -> str:\n",
    "        return os.path.join(self.cache_dir, f\"{text_hash}.npy\")\n",
    "        \n",
    "    def get(self, text: str) -> Optional[np.ndarray]:\n",
    "        \"\"\"Get embedding from cache.\"\"\"\n",
    "        text_hash = hashlib.md5(text.encode()).hexdigest()\n",
    "        if text_hash in self.cache:\n",
    "            return self.cache[text_hash]\n",
    "            \n",
    "        cache_path = self.get_cache_path(text_hash)\n",
    "        if os.path.exists(cache_path):\n",
    "            embedding = np.load(cache_path)\n",
    "            self.cache[text_hash] = embedding\n",
    "            return embedding\n",
    "        return None\n",
    "        \n",
    "    def put(self, text: str, embedding: np.ndarray):\n",
    "        \"\"\"Store embedding in cache.\"\"\"\n",
    "        text_hash = hashlib.md5(text.encode()).hexdigest()\n",
    "        self.cache[text_hash] = embedding\n",
    "        cache_path = self.get_cache_path(text_hash)\n",
    "        np.save(cache_path, embedding)\n",
    "        \n",
    "class EnhancedEmbeddingGenerator:\n",
    "    def __init__(self, \n",
    "                 model_name: str = \"TurkuNLP/sbert-cased-finnish-paraphrase\",\n",
    "                 cache_dir: str = \"/scratch/project_2011638/safdarih/huggingface_cache\",\n",
    "                 device: str = None):\n",
    "        self.model_name = model_name\n",
    "        self.cache_dir = cache_dir\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.embedding_cache = {}\n",
    "        self._initialize_model()\n",
    "\n",
    "    def _initialize_model(self):\n",
    "        \"\"\"Initialize model with proper error handling.\"\"\"\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_name,\n",
    "                cache_dir=self.cache_dir\n",
    "            )\n",
    "            self.model = AutoModel.from_pretrained(\n",
    "                self.model_name,\n",
    "                cache_dir=self.cache_dir\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # Set model to evaluation mode\n",
    "            self.model.eval()\n",
    "            \n",
    "            self.embedding_dim = self.model.config.hidden_size\n",
    "            logger.info(f\"Loaded {self.model_name} (dim={self.embedding_dim})\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Model initialization failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _mean_pooling(self, model_output: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Perform mean pooling on token embeddings.\"\"\"\n",
    "        token_embeddings = model_output[0]\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n",
    "            input_mask_expanded.sum(1), min=1e-9\n",
    "        )\n",
    "\n",
    "    def generate(self, texts: List[str], batch_size: int = 8) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings with improved batching and error handling.\"\"\"\n",
    "        try:\n",
    "            all_embeddings = []\n",
    "            \n",
    "            # Process in batches\n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                batch_texts = texts[i:i + batch_size]\n",
    "                \n",
    "                # Check cache first\n",
    "                batch_embeddings = []\n",
    "                uncached_texts = []\n",
    "                uncached_indices = []\n",
    "                \n",
    "                for j, text in enumerate(batch_texts):\n",
    "                    cache_key = hash(text)\n",
    "                    if cache_key in self.embedding_cache:\n",
    "                        batch_embeddings.append(self.embedding_cache[cache_key])\n",
    "                    else:\n",
    "                        uncached_texts.append(text)\n",
    "                        uncached_indices.append(j)\n",
    "                \n",
    "                if uncached_texts:\n",
    "                    # Generate new embeddings\n",
    "                    with torch.no_grad():\n",
    "                        encoded_input = self.tokenizer(\n",
    "                            uncached_texts,\n",
    "                            padding=True,\n",
    "                            truncation=True,\n",
    "                            max_length=512,\n",
    "                            return_tensors='pt'\n",
    "                        ).to(self.device)\n",
    "                        \n",
    "                        model_output = self.model(**encoded_input)\n",
    "                        sentence_embeddings = self._mean_pooling(\n",
    "                            model_output,\n",
    "                            encoded_input['attention_mask']\n",
    "                        )\n",
    "                        # Normalize embeddings\n",
    "                        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "                        \n",
    "                        # Move to CPU and convert to numpy\n",
    "                        embeddings_np = sentence_embeddings.cpu().numpy()\n",
    "                        \n",
    "                        # Cache new embeddings\n",
    "                        for text, embedding in zip(uncached_texts, embeddings_np):\n",
    "                            self.embedding_cache[hash(text)] = embedding\n",
    "                            \n",
    "                        # Insert new embeddings into correct positions\n",
    "                        for idx, embedding in zip(uncached_indices, embeddings_np):\n",
    "                            batch_embeddings.insert(idx, embedding)\n",
    "                \n",
    "                all_embeddings.extend(batch_embeddings)\n",
    "            \n",
    "            final_embeddings = np.vstack(all_embeddings)\n",
    "            \n",
    "            # Verify embedding dimension\n",
    "            if final_embeddings.shape[1] != self.embedding_dim:\n",
    "                raise ValueError(\n",
    "                    f\"Embedding dimension mismatch. Expected {self.embedding_dim}, \"\n",
    "                    f\"got {final_embeddings.shape[1]}\"\n",
    "                )\n",
    "            \n",
    "            return final_embeddings\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating embeddings: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def compute_similarity(self, query_embedding: np.ndarray, doc_embeddings: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute cosine similarity between query and documents.\"\"\"\n",
    "        return np.dot(doc_embeddings, query_embedding.T).squeeze()\n",
    "\n",
    "    def batch_compute_similarity(self, queries: List[str], documents: List[str]) -> np.ndarray:\n",
    "        \"\"\"Compute similarities between multiple queries and documents efficiently.\"\"\"\n",
    "        query_embeddings = self.generate(queries)\n",
    "        doc_embeddings = self.generate(documents)\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        similarity_matrix = np.dot(query_embeddings, doc_embeddings.T)\n",
    "        \n",
    "        return similarity_matrix\n",
    "\n",
    "class RAGPipeline:\n",
    "    def __init__(self, model_id: str = \"Finnish-NLP/llama-7b-finnish-instruct-v0.2\"):\n",
    "        try:\n",
    "            # Initialize token managers\n",
    "            self.token_manager = TokenManager(max_tokens=2048)\n",
    "            self.token_limit_manager = TokenLimitManager(max_tokens=2048)\n",
    "            \n",
    "            # Initialize core components\n",
    "            self.doc_processor = DocumentProcessor()  \n",
    "            self.embedding_generator = EnhancedEmbeddingGenerator(\n",
    "                model_name=\"TurkuNLP/sbert-cased-finnish-paraphrase\",\n",
    "                cache_dir=\"/scratch/project_2011638/safdarih/huggingface_cache\"\n",
    "            )\n",
    "            EMBEDDING_DIM = self.embedding_generator.embedding_dim\n",
    "            \n",
    "            # Initialize Milvus components\n",
    "            self.milvus_manager = MilvusManager(\n",
    "                host=MILVUS_HOST,\n",
    "                port=MILVUS_PORT,\n",
    "                alias=MILVUS_ALIAS\n",
    "            )\n",
    "            \n",
    "            # Create collection if it doesn't exist\n",
    "            self.collection_manager = CollectionManager()\n",
    "            try:\n",
    "                self.collection = self.collection_manager.collection\n",
    "            except Exception as e:\n",
    "                print(f\"Creating new collection: {str(e)}\")\n",
    "                self.collection = self.milvus_manager.create_collection()\n",
    "\n",
    "            # Initialize pipeline components\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\"\n",
    "            )\n",
    "\n",
    "            # Initialize tokenizer and model\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_id,\n",
    "                cache_dir=\"/scratch/project_2011638/safdarih/huggingface_cache\" \n",
    "            )\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=\"auto\",\n",
    "                cache_dir=\"/scratch/project_2011638/safdarih/huggingface_cache\"\n",
    "            )\n",
    "\n",
    "            # Initialize pipeline\n",
    "            self.pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer=self.tokenizer,\n",
    "                max_new_tokens=300,\n",
    "                do_sample=True,\n",
    "                temperature=0.1,\n",
    "                top_p=0.95,\n",
    "                repetition_penalty=1.2\n",
    "            )\n",
    "\n",
    "            print_status(\"RAG Pipeline\", True, \"Successfully initialized all components\")\n",
    "        except Exception as e:\n",
    "            print_status(\"RAG Pipeline\", False, f\"Initialization error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def process_documents(self, folder_path: str) -> List[Dict]:\n",
    "        \"\"\"Process documents and prepare them for RAG.\"\"\"\n",
    "        try:\n",
    "            # Get all .docx files in the folder\n",
    "            file_paths = [f for f in os.listdir(folder_path) if f.endswith('.docx')]\n",
    "            if not file_paths:\n",
    "                raise ValueError(f\"No .docx files found in {folder_path}\")\n",
    "            \n",
    "            print(f\"Found {len(file_paths)} documents to process\")\n",
    "            \n",
    "            all_chunks = []\n",
    "            for file in file_paths:\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                chunks = self.doc_processor.process_document(file_path)\n",
    "                all_chunks.extend(chunks)\n",
    "            \n",
    "            if not all_chunks:\n",
    "                raise ValueError(\"No chunks were processed from documents\")\n",
    "            \n",
    "            # Generate embeddings for all chunks\n",
    "            texts = [chunk[\"text\"] for chunk in all_chunks]\n",
    "            try:\n",
    "                embeddings = self.embedding_generator.generate(texts)\n",
    "                \n",
    "                # Prepare entities for Milvus\n",
    "                entities = []\n",
    "                for chunk, embedding in zip(all_chunks, embeddings):\n",
    "                    entity = {\n",
    "                        \"text\": chunk[\"text\"],\n",
    "                        \"embedding\": embedding.tolist(),\n",
    "                        \"person_name\": chunk[\"metadata\"][\"person_name\"],\n",
    "                        \"person_age\": chunk[\"metadata\"][\"person_age\"],\n",
    "                        \"document_id\": chunk[\"metadata\"][\"document_id\"],\n",
    "                        \"chunk_index\": chunk[\"metadata\"][\"chunk_index\"]\n",
    "                    }\n",
    "                    entities.append(entity)\n",
    "                \n",
    "                # Insert into Milvus\n",
    "                self.collection.insert(entities)\n",
    "                self.collection.flush()\n",
    "                \n",
    "                print_status(\"Document Processing\", True, \n",
    "                            f\"Processed {len(all_chunks)} chunks from {len(file_paths)} documents\")\n",
    "                return all_chunks\n",
    "                \n",
    "            except Exception as e:\n",
    "                print_status(\"Embedding Generation\", False, f\"Error generating embeddings: {str(e)}\")\n",
    "                raise\n",
    "                \n",
    "        except Exception as e:\n",
    "            print_status(\"Document Processing\", False, f\"Error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _prepare_context(self, question: str, top_k: int = 5) -> List[Dict]:\n",
    "        \"\"\"Prepare and validate context for question.\"\"\"\n",
    "        try:\n",
    "            # Reload collection only if needed (add this flag to __init__)\n",
    "            if not hasattr(self, '_collection_loaded'):\n",
    "                self.collection = self.milvus_manager.reload_collection()\n",
    "                self._collection_loaded = True\n",
    "\n",
    "            # Generate question embedding\n",
    "            question_embedding = self.embedding_generator.generate([question])[0]\n",
    "            \n",
    "            # Search parameters\n",
    "            search_params = {\n",
    "                \"metric_type\": \"IP\",\n",
    "                \"params\": {\"nprobe\": 50}\n",
    "            }\n",
    "            \n",
    "            # Perform search with larger initial pool\n",
    "            initial_results = self.collection.search(\n",
    "                data=[question_embedding.tolist()],\n",
    "                anns_field=\"embedding\",\n",
    "                param=search_params,\n",
    "                limit=top_k * 2,  # Get more results initially for better reranking\n",
    "                output_fields=[\"text\", \"person_name\", \"document_id\", \"chunk_index\"]\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            processed_results = []\n",
    "            for hit in initial_results[0]:\n",
    "                processed_results.append({\n",
    "                    'text': hit.entity.get('text'),\n",
    "                    'source': hit.entity.get('document_id'),\n",
    "                    'person_name': hit.entity.get('person_name'),\n",
    "                    'chunk_index': hit.entity.get('chunk_index'),\n",
    "                    'score': float(hit.score)\n",
    "                })\n",
    "            \n",
    "            return processed_results[:top_k]  # Return top_k after processing\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error preparing context: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def query(self, question: str, top_k: int = 5) -> Dict:\n",
    "        \"\"\"Enhanced query processing with token management.\"\"\"\n",
    "        try:\n",
    "            # Get initial results\n",
    "            results = self._prepare_context(question, top_k)\n",
    "            \n",
    "            # Truncate context to fit token limits\n",
    "            truncated_context = self.token_manager.truncate_context(\n",
    "                results,\n",
    "                self.pipeline.tokenizer\n",
    "            )\n",
    "            \n",
    "            # Format context for LLM\n",
    "            formatted_input = self.token_manager.format_for_llm(\n",
    "                truncated_context,\n",
    "                question\n",
    "            )\n",
    "            \n",
    "            # Generate response with proper prompt\n",
    "            prompt = f\"\"\"Tehtävä: Etsi tarkka vastaus kysymykseen käyttäen vain annettua kontekstia.\n",
    "\n",
    "{formatted_input}\n",
    "\n",
    "Vastausohjeet:\n",
    "1. Jos löydät suoran vastauksen:\n",
    "   - Mainitse dokumentti, josta vastaus löytyy\n",
    "   - Käytä suoria lainauksia\n",
    "   - Arvioi vastauksen luotettavuus\n",
    "2. Jos et löydä vastausta:\n",
    "   - Ilmoita selkeästi, ettei vastausta löydy annetusta kontekstista\n",
    "3. Jos löydät vain osittaisen vastauksen:\n",
    "   - Kerro mitä tietoa löysit ja mitä puuttuu\n",
    "\n",
    "Vastaus:\"\"\"\n",
    "\n",
    "            # Generate response\n",
    "            response = self.pipeline(\n",
    "                prompt,\n",
    "                max_new_tokens=300,\n",
    "                do_sample=True,\n",
    "                temperature=0.1,\n",
    "                top_p=0.85,\n",
    "                repetition_penalty=1.2\n",
    "            )[0][\"generated_text\"]\n",
    "            \n",
    "            return {\n",
    "                \"answer\": response,\n",
    "                \"sources\": truncated_context,\n",
    "                \"metadata\": {\n",
    "                    \"question\": question,\n",
    "                    \"num_chunks_retrieved\": len(truncated_context),\n",
    "                    \"max_similarity_score\": max(doc['score'] for doc in truncated_context)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in query processing: {str(e)}\")\n",
    "            raise    \n",
    "            \n",
    "    def _rerank_results(self, question: str, results: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Enhanced reranking with Finnish-specific handling.\"\"\"\n",
    "        reranked = []\n",
    "        \n",
    "        # Get question embedding once and normalize\n",
    "        question_embedding = self.embedding_generator.generate([question])[0]\n",
    "        \n",
    "        # Get all text embeddings at once for efficiency\n",
    "        texts = [hit['text'] for hit in results]\n",
    "        text_embeddings = self.embedding_generator.generate(texts)\n",
    "        \n",
    "        # Compute similarities all at once\n",
    "        similarities = self.embedding_generator.compute_similarity(\n",
    "            question_embedding, \n",
    "            text_embeddings\n",
    "        )\n",
    "        \n",
    "        for hit, similarity in zip(results, similarities):\n",
    "            score = hit['score']\n",
    "            text = hit['text']\n",
    "            \n",
    "            # Finnish-specific boosts\n",
    "            boost_score = 1.0\n",
    "            \n",
    "            # Boost for complete sentences\n",
    "            if re.search(r'[.!?][\\s]*', text):\n",
    "                boost_score *= 1.1\n",
    "                \n",
    "            # Boost for answer indicators\n",
    "            if any(indicator in text.lower() for indicator in ['on', 'ovat', 'oli']):\n",
    "                boost_score *= 1.15\n",
    "                \n",
    "            # Boost for question-context match\n",
    "            if any(word in text.lower() for word in question.lower().split()):\n",
    "                boost_score *= 1.2\n",
    "            \n",
    "            final_score = score * similarity * boost_score\n",
    "            reranked.append({**hit, 'score': final_score})\n",
    "            \n",
    "        return sorted(reranked, key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    def _extract_entities(self, text: str) -> List[str]:\n",
    "        \"\"\"Simple entity extraction based on capitalized words.\"\"\"\n",
    "        return re.findall(r'\\b[A-Z][a-zäöå]*\\b', text)\n",
    "\n",
    "    def _clean_response(self, response: str) -> str:\n",
    "        response = re.sub(r'(\\[Dokumentti \\d+\\])\\s*\\1', r'\\1', response)\n",
    "        response = re.sub(r'Luottamus: \\d+%\\s*Selitys:', '', response)\n",
    "        if len(response) > 500:\n",
    "            response = response[:497] + '...'\n",
    "        return response.strip()\n",
    "\n",
    "    def _create_prompt(self, question: str, context: str) -> str:\n",
    "        return f\"\"\"Tehtävä: Vastaa annettuun kysymykseen käyttäen vain alla olevaa kontekstia.\n",
    "\n",
    "        Kysymys: {question}\n",
    "        \n",
    "        Konteksti:\n",
    "        {context}\n",
    "        \n",
    "        Vastausohjeet:\n",
    "        1. Jos löydät suoran vastauksen:\n",
    "           - Mainitse ensin dokumentti, josta vastaus löytyy\n",
    "           - Lainaa tekstiä tarkasti käyttäen lainausmerkkejä\n",
    "           - Mainitse vastauksen luotettavuus prosentteina\n",
    "        \n",
    "        2. Jos löydät osittaisen vastauksen:\n",
    "           - Kerro, mitä tietoa löysit ja mistä\n",
    "           - Mainitse selkeästi, mitä tietoa puuttuu\n",
    "        \n",
    "        3. Jos et löydä vastausta:\n",
    "           - Vastaa vain: \"En löydä suoraa vastausta annetusta kontekstista\"\n",
    "        \n",
    "        Vastaus:\"\"\"\n",
    "class CollectionManager:\n",
    "    def __init__(self):\n",
    "        self._collection = None\n",
    "        self._last_reload_time = 0\n",
    "        self.reload_interval = 300  # 5 minutes\n",
    "        \n",
    "    @property\n",
    "    def collection(self):\n",
    "        current_time = time.time()\n",
    "        if (not self._collection or \n",
    "            current_time - self._last_reload_time > self.reload_interval):\n",
    "            self._reload_collection()\n",
    "        return self._collection\n",
    "        \n",
    "    def _reload_collection(self):\n",
    "        try:\n",
    "            if not utility.has_collection(\"document_embeddings\"):\n",
    "                raise Exception(\"Collection does not exist\")\n",
    "                \n",
    "            self._collection = Collection(name=\"document_embeddings\")\n",
    "            self._collection.load()\n",
    "            self._last_reload_time = time.time()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reloading collection: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def create_collection(self, collection_name: str = \"document_embeddings\"):\n",
    "        \"\"\"Create a new collection if it doesn't exist.\"\"\"\n",
    "        try:\n",
    "            if utility.has_collection(collection_name):\n",
    "                Collection(name=collection_name).drop()\n",
    "                print_status(\"Collection Manager\", True, f\"Dropped existing collection: {collection_name}\")\n",
    "            \n",
    "            fields = [\n",
    "                FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "                FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "                FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=EMBEDDING_DIM),\n",
    "                FieldSchema(name=\"person_name\", dtype=DataType.VARCHAR, max_length=100),\n",
    "                FieldSchema(name=\"person_age\", dtype=DataType.INT64),\n",
    "                FieldSchema(name=\"document_id\", dtype=DataType.VARCHAR, max_length=100),\n",
    "                FieldSchema(name=\"chunk_index\", dtype=DataType.INT64)\n",
    "            ]\n",
    "            \n",
    "            schema = CollectionSchema(\n",
    "                fields=fields,\n",
    "                description=\"Document embeddings collection\",\n",
    "                enable_dynamic_field=False\n",
    "            )\n",
    "            \n",
    "            self._collection = Collection(name=collection_name, schema=schema)\n",
    "            self._create_index()\n",
    "            self._collection.load()\n",
    "            self._last_reload_time = time.time()\n",
    "            \n",
    "            print_status(\"Collection Manager\", True, f\"Created new collection: {collection_name}\")\n",
    "            return self._collection\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating collection: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _create_index(self):\n",
    "        \"\"\"Create index for the collection.\"\"\"\n",
    "        try:\n",
    "            index_params = {\n",
    "                \"metric_type\": \"IP\",\n",
    "                \"index_type\": \"IVF_FLAT\",\n",
    "                \"params\": {\"nlist\": 1024}\n",
    "            }\n",
    "            self._collection.create_index(\n",
    "                field_name=\"embedding\",\n",
    "                index_params=index_params\n",
    "            )\n",
    "            print_status(\"Collection Manager\", True, \"Created index\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating index: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "\n",
    "def verify_pipeline_components():\n",
    "    doc_processor = DocumentProcessor()\n",
    "    milvus_manager = MilvusManager()\n",
    "    embedding_generator = EnhancedEmbeddingGenerator()\n",
    "    pipeline = RAGPipeline()\n",
    "    return all([doc_processor, milvus_manager, embedding_generator, pipeline])\n",
    "\n",
    "verify_section(\"Pipeline Components\", verify_pipeline_components)\n",
    "\n",
    "## section3:\n",
    "from neo4j import GraphDatabase\n",
    "class Neo4jDocumentManager:\n",
    "    def __init__(self, uri: str = \"bolt://87.92.59.201:7687\", \n",
    "                 username: str = \"neo4j\", \n",
    "                 password: str = \"test\"):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "        self.setup_constraints()\n",
    "        self.person_names = set()\n",
    "        \n",
    "    def get_related_documents(self, session, mentioned_persons: List[str]) -> List[Dict]:\n",
    "        \"\"\"Get documents related to mentioned persons.\"\"\"\n",
    "        try:\n",
    "            documents = []\n",
    "            for person in mentioned_persons:\n",
    "                result = session.run(\"\"\"\n",
    "                    MATCH (p:Person {name: $name})-[:APPEARS_IN]->(d:Document)\n",
    "                    RETURN d.content as content, d.id as doc_id, \n",
    "                           d.chunk_index as chunk_index\n",
    "                    ORDER BY d.chunk_index\n",
    "                \"\"\", {\"name\": person})\n",
    "                \n",
    "                for record in result:\n",
    "                    documents.append({\n",
    "                        \"content\": record[\"content\"],\n",
    "                        \"doc_id\": record[\"doc_id\"],\n",
    "                        \"chunk_index\": record[\"chunk_index\"],\n",
    "                        \"relevance\": 1.0\n",
    "                    })\n",
    "            \n",
    "            return documents\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error retrieving related documents: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    # Remove the duplicate _get_related_documents method\n",
    "    # All other methods remain the same\n",
    "    def check_connection(self) -> bool:\n",
    "        \"\"\"Check if Neo4j connection is working.\"\"\"\n",
    "        try:\n",
    "            with self.driver.session() as session:\n",
    "                result = session.run(\"RETURN 1 as test\")\n",
    "                assert result.single()[\"test\"] == 1\n",
    "                print(\"Neo4j connection test successful\")\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            print(f\"Neo4j connection test failed: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def setup_constraints(self):\n",
    "        \"\"\"Setup necessary constraints for the graph.\"\"\"\n",
    "        try:\n",
    "            with self.driver.session() as session:\n",
    "                # Neo4j 5+ syntax for constraints\n",
    "                constraints = [\n",
    "                    \"\"\"CREATE CONSTRAINT person_name_unique IF NOT EXISTS \n",
    "                       FOR (p:Person) REQUIRE p.name IS UNIQUE\"\"\",\n",
    "                    \"\"\"CREATE CONSTRAINT document_id_unique IF NOT EXISTS \n",
    "                       FOR (d:Document) REQUIRE d.id IS UNIQUE\"\"\"\n",
    "                ]\n",
    "                \n",
    "                for constraint in constraints:\n",
    "                    try:\n",
    "                        session.run(constraint)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Could not create constraint: {str(e)}\")\n",
    "                        # Try older Neo4j syntax if newer one fails\n",
    "                        try:\n",
    "                            if \"person_name\" in constraint:\n",
    "                                session.run(\"\"\"CREATE CONSTRAINT ON (p:Person) \n",
    "                                             ASSERT p.name IS UNIQUE\"\"\")\n",
    "                            elif \"document_id\" in constraint:\n",
    "                                session.run(\"\"\"CREATE CONSTRAINT ON (d:Document) \n",
    "                                             ASSERT d.id IS UNIQUE\"\"\")\n",
    "                        except Exception as e2:\n",
    "                            print(f\"Warning: Could not create constraint with old syntax: {str(e2)}\")\n",
    "                            \n",
    "                print(\"Neo4j constraints setup completed\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error during constraint setup: {str(e)}\")\n",
    "            # Continue even if constraints fail - they're helpful but not crit\n",
    "            pass\n",
    "        \n",
    "    def _extract_person_names(self, text: str) -> set:\n",
    "        \"\"\"Extract potential person names from text using Finnish patterns.\"\"\"\n",
    "        names = set()\n",
    "        \n",
    "        # Finnish name patterns\n",
    "        patterns = [\n",
    "            r'\\b[A-ZÄÖÅ][a-zäöå]+\\b',  # Basic capitalized word\n",
    "            r'(?:herra|rouva|neiti)\\s+[A-ZÄÖÅ][a-zäöå]+',  # Titles\n",
    "            r'(?:ystävä|naapuri)\\s+[A-ZÄÖÅ][a-zäöå]+',  # Relationships\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            matches = re.finditer(pattern, text)\n",
    "            for match in matches:\n",
    "                name = match.group().split()[-1]  # Get the actual name part\n",
    "                if len(name) > 2:  # Filter out very short words\n",
    "                    names.add(name)\n",
    "        \n",
    "        return names\n",
    "\n",
    "    def create_document_graph(self, processed_chunks: List[Dict]):\n",
    "        \"\"\"Create graph structure from processed document chunks.\"\"\"\n",
    "        # First pass: collect all person names\n",
    "        for chunk in processed_chunks:\n",
    "            # Add names from metadata\n",
    "            if chunk[\"metadata\"].get(\"person_name\"):\n",
    "                self.person_names.add(chunk[\"metadata\"][\"person_name\"])\n",
    "            \n",
    "            # Add names found in content\n",
    "            self.person_names.update(self._extract_person_names(chunk[\"text\"]))\n",
    "        \n",
    "        with self.driver.session() as session:\n",
    "            for chunk in processed_chunks:\n",
    "                # Create Document and Person nodes\n",
    "                self._create_document_and_person_nodes(session, chunk)\n",
    "                \n",
    "                # Extract and create relationships\n",
    "                self._process_relationships(session, chunk)\n",
    "    \n",
    "    def _create_document_and_person_nodes(self, session, chunk: Dict):\n",
    "        \"\"\"Create document and person nodes with relationships.\"\"\"\n",
    "        session.run(\"\"\"\n",
    "            MERGE (p:Person {name: $person_name})\n",
    "            SET p.age = $age\n",
    "            \n",
    "            MERGE (d:Document {id: $doc_id})\n",
    "            SET d.content = $content,\n",
    "                d.chunk_index = $chunk_index\n",
    "            \n",
    "            MERGE (p)-[:APPEARS_IN]->(d)\n",
    "            \n",
    "            WITH p, d\n",
    "            MATCH (p)-[:APPEARS_IN]->(other:Document)\n",
    "            WHERE other.id <> d.id\n",
    "            MERGE (d)-[:RELATED_TO]->(other)\n",
    "        \"\"\", {\n",
    "            \"person_name\": chunk[\"metadata\"][\"person_name\"],\n",
    "            \"age\": chunk[\"metadata\"][\"person_age\"],\n",
    "            \"doc_id\": f\"{chunk['metadata']['document_id']}_{chunk['metadata']['chunk_index']}\",\n",
    "            \"content\": chunk[\"text\"],\n",
    "            \"chunk_index\": chunk[\"metadata\"][\"chunk_index\"]\n",
    "        })\n",
    "\n",
    "\n",
    "    def _process_relationships(self, session, chunk: Dict):\n",
    "        \"\"\"Create relationships based on the content of each chunk.\"\"\"\n",
    "        # Extract persons mentioned in the text\n",
    "        mentioned_persons = self.find_mentioned_persons(chunk[\"text\"])\n",
    "        \n",
    "        # Get the main person from metadata\n",
    "        main_person = chunk[\"metadata\"].get(\"person_name\")\n",
    "        \n",
    "        # If there is no main person in this chunk, skip relationship creation\n",
    "        if not main_person:\n",
    "            return\n",
    "        \n",
    "        # Loop over each mentioned person to create a relationship in the graph\n",
    "        for mentioned_person in mentioned_persons:\n",
    "            # Define the relationship type - here it's just an example relationship type\n",
    "            relationship_type = \"KNOWS\"\n",
    "            \n",
    "            # Create relationship in Neo4j\n",
    "            session.run(\"\"\"\n",
    "                MATCH (p1:Person {name: $main_person})\n",
    "                MATCH (p2:Person {name: $mentioned_person})\n",
    "                MERGE (p1)-[r:KNOWS]->(p2)\n",
    "                RETURN r\n",
    "            \"\"\", {\n",
    "                \"main_person\": main_person,\n",
    "                \"mentioned_person\": mentioned_person\n",
    "            })   \n",
    "\n",
    "    def find_mentioned_persons(self, text: str) -> List[str]:\n",
    "        \"\"\"Find persons mentioned in text from known person names.\"\"\"\n",
    "        mentioned_persons = []\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Look for known person names in the text\n",
    "        for name in self.person_names:\n",
    "            if name.lower() in text_lower:\n",
    "                mentioned_persons.append(name)\n",
    "                \n",
    "        return mentioned_persons\n",
    "\n",
    "    def query(self, question: str) -> Dict:\n",
    "        \"\"\"Enhanced query processing with dynamic person detection.\"\"\"\n",
    "        # Find mentioned persons in the question\n",
    "        \n",
    "        \n",
    "        with self.driver.session() as session:\n",
    "            # Get person contexts for all mentioned persons\n",
    "            mentioned_persons = self.find_mentioned_persons(question)\n",
    "            related_docs = self.get_related_documents(session, mentioned_persons)\n",
    "            \n",
    "            person_contexts = []\n",
    "            for person_name in mentioned_persons:\n",
    "                context = self._get_person_context(session, person_name)\n",
    "                if context:\n",
    "                    person_contexts.append(context)       \n",
    "            return {\n",
    "                \"person_contexts\": person_contexts,\n",
    "                \"related_documents\": related_docs,\n",
    "                \"mentioned_persons\": mentioned_persons\n",
    "            }\n",
    "    \n",
    "    def _get_person_context(self, session, person_name: str) -> Dict:\n",
    "        \"\"\"Get comprehensive context for a person.\"\"\"\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (p:Person {name: $name})\n",
    "            OPTIONAL MATCH (p)-[:APPEARS_IN]->(d:Document)\n",
    "            OPTIONAL MATCH (d)-[:HAS_RELATIONSHIP]->(r:Relationship)\n",
    "            \n",
    "            WITH p,\n",
    "                 COLLECT(DISTINCT r.type) as relationship_types,\n",
    "                 COLLECT(DISTINCT d.content) as contents,\n",
    "                 COLLECT(DISTINCT d.id) as doc_ids\n",
    "            \n",
    "            RETURN {\n",
    "                name: p.name,\n",
    "                age: p.age,\n",
    "                relationship_types: relationship_types,\n",
    "                document_count: SIZE(contents),\n",
    "                document_ids: doc_ids\n",
    "            } as context\n",
    "        \"\"\", {\"name\": person_name})\n",
    "        \n",
    "        return result.single()[\"context\"] if result.peek() else None\n",
    "\n",
    "class Neo4jEnhancedRAGPipeline:\n",
    "    def __init__(self, base_rag_pipeline, neo4j_uri: str = \"bolt://87.92.59.201:7687\"):\n",
    "        self.base_rag = base_rag_pipeline\n",
    "        self.neo4j_manager = Neo4jDocumentManager(\n",
    "            uri=neo4j_uri,\n",
    "            username=\"neo4j\",\n",
    "            password=\"test\"\n",
    "        )\n",
    "    \n",
    "    def process_documents(self, folder_path: str):\n",
    "        \"\"\"Process documents and create graph structure.\"\"\"\n",
    "        try:\n",
    "            # Process with base RAG first\n",
    "            print(\"Processing documents with base RAG...\")\n",
    "            # Call process_documents and store all chunks\n",
    "            all_chunks = []\n",
    "            \n",
    "            # Get all .docx files in the folder\n",
    "            file_paths = [f for f in os.listdir(folder_path) if f.endswith('.docx')]\n",
    "            if not file_paths:\n",
    "                raise ValueError(f\"No .docx files found in {folder_path}\")\n",
    "            \n",
    "            print(f\"Found {len(file_paths)} documents to process\")\n",
    "            \n",
    "            for file in file_paths:\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                chunks = self.base_rag.doc_processor.process_document(file_path)\n",
    "                all_chunks.extend(chunks)\n",
    "            \n",
    "            if not all_chunks:\n",
    "                raise ValueError(\"No chunks were processed from documents\")\n",
    "            \n",
    "            print(f\"Processed {len(all_chunks)} chunks from documents\")\n",
    "            \n",
    "            # Create graph structure\n",
    "            print(\"Creating Neo4j graph structure...\")\n",
    "            self.neo4j_manager.create_document_graph(all_chunks)\n",
    "            \n",
    "            # Process with base RAG pipeline for vector embeddings\n",
    "            self.base_rag.process_documents(folder_path)\n",
    "            \n",
    "            return all_chunks\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing documents: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def query(self, question: str) -> Dict:\n",
    "        \"\"\"Process a query using both vector and graph capabilities.\"\"\"\n",
    "        try:\n",
    "            # Get base results from vector search\n",
    "            base_response = self.base_rag.query(question)\n",
    "            \n",
    "            # Get graph context from Neo4j\n",
    "            graph_context = self.neo4j_manager.query(question)\n",
    "            \n",
    "            # Combine contexts\n",
    "            combined_context = self._combine_contexts(\n",
    "                base_sources=base_response.get(\"sources\", []),\n",
    "                graph_sources=graph_context.get(\"related_documents\", []),\n",
    "                person_contexts=graph_context.get(\"person_contexts\", [])\n",
    "            )\n",
    "            \n",
    "            # Generate enhanced answer\n",
    "            enhanced_answer = self._generate_enhanced_answer(question, combined_context)\n",
    "            \n",
    "            # Format response\n",
    "            response = {\n",
    "                \"answer\": enhanced_answer,\n",
    "                \"sources\": base_response.get(\"sources\", []),\n",
    "                \"person_context\": graph_context.get(\"person_contexts\", [])[0] if graph_context.get(\"person_contexts\") else None,\n",
    "                \"metadata\": {\n",
    "                    \"question\": question,\n",
    "                    \"mentioned_persons\": graph_context.get(\"mentioned_persons\", []),\n",
    "                    \"base_similarity_score\": base_response.get(\"metadata\", {}).get(\"max_similarity_score\", 0)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in Neo4j enhanced query: {str(e)}\")\n",
    "            # Fallback to base RAG if Neo4j enhancement fails\n",
    "            return self.base_rag.query(question)\n",
    "      \n",
    "    def _combine_contexts(self, base_sources: List[Dict], \n",
    "                         graph_sources: List[Dict], \n",
    "                         person_contexts: List[Dict]) -> Dict:\n",
    "        \"\"\"Combine all sources of context intelligently.\"\"\"\n",
    "        combined = {\n",
    "            \"documents\": [],\n",
    "            \"relationships\": [],\n",
    "            \"persons\": []\n",
    "        }\n",
    "        \n",
    "        # Add base sources\n",
    "        for source in base_sources:\n",
    "            combined[\"documents\"].append({\n",
    "                \"content\": source[\"text\"],\n",
    "                \"source\": source[\"document_id\"],\n",
    "                \"relevance\": source[\"score\"]\n",
    "            })\n",
    "        \n",
    "        # Add graph-enhanced sources\n",
    "        for source in graph_sources:\n",
    "            if source not in combined[\"documents\"]:\n",
    "                combined[\"documents\"].append({\n",
    "                    \"content\": source[\"content\"],\n",
    "                    \"source\": source[\"doc_id\"],\n",
    "                    \"relevance\": source.get(\"relevance\", 0.5)\n",
    "                })\n",
    "        \n",
    "        # Add person contexts\n",
    "        for context in person_contexts:\n",
    "            combined[\"persons\"].append({\n",
    "                \"name\": context[\"name\"],\n",
    "                \"age\": context[\"age\"],\n",
    "                \"relationships\": context[\"relationship_types\"],\n",
    "                \"document_count\": context[\"document_count\"]\n",
    "            })\n",
    "            \n",
    "        return combined\n",
    "\n",
    "    def _generate_enhanced_answer(self, question: str, context: Dict) -> str:\n",
    "        \"\"\"Generate answer using combined context with token management.\"\"\"\n",
    "        # Create enhanced prompt with token management\n",
    "        formatted_context = self._format_context_within_limits(context)\n",
    "        \n",
    "        prompt = f\"\"\"Käytä seuraavaa kontekstia vastataksesi kysymykseen. Huomioi erityisesti henkilöiden väliset suhteet.\n",
    "    \n",
    "        Kysymys: {question}\n",
    "    \n",
    "        {formatted_context}\n",
    "    \n",
    "        Vastausohjeet:\n",
    "        1. Käytä dokumenttien ja suhteiden tietoja yhdessä\n",
    "        2. Mainitse lähteet selkeästi\n",
    "        3. Korosta henkilöiden välisiä suhteita\n",
    "        4. Käytä suoria lainauksia kun mahdollista\n",
    "    \n",
    "        Vastaus:\"\"\"\n",
    "    \n",
    "        # Generate answer using base pipeline\n",
    "        response = self.base_rag.pipeline(\n",
    "            prompt,\n",
    "            max_new_tokens=300,\n",
    "            do_sample=True,\n",
    "            temperature=0.1,\n",
    "            top_p=0.85,\n",
    "            repetition_penalty=1.2\n",
    "        )[0][\"generated_text\"]\n",
    "        \n",
    "        return response.strip()\n",
    "\n",
    "    def _format_document_context(self, documents: List[Dict]) -> str:\n",
    "        \"\"\"Format document context for prompt.\"\"\"\n",
    "        return \"\\n\\n\".join([\n",
    "            f\"Dokumentti {i+1} ({doc['source']}):\\n{doc['content']}\"\n",
    "            for i, doc in enumerate(documents)\n",
    "        ])\n",
    "\n",
    "    def _format_person_context(self, persons: List[Dict]) -> str:\n",
    "        \"\"\"Format person context for prompt.\"\"\"\n",
    "        return \"\\n\\n\".join([\n",
    "            f\"Henkilö: {person['name']}\\n\"\n",
    "            f\"Ikä: {person['age']}\\n\"\n",
    "            f\"Suhteet: {', '.join(person['relationships'])}\"\n",
    "            for person in persons\n",
    "        ])\n",
    "\n",
    "    def _truncate_text(self, text: str, max_tokens: int = 1800) -> str:\n",
    "        \"\"\"Truncate text to stay within token limits.\"\"\"\n",
    "        from transformers import AutoTokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"Finnish-NLP/llama-7b-finnish-instruct-v0.2\")\n",
    "        tokens = tokenizer.encode(text)\n",
    "        \n",
    "        if len(tokens) <= max_tokens:\n",
    "            return text\n",
    "            \n",
    "        # Truncate while trying to keep complete sentences\n",
    "        truncated_tokens = tokens[:max_tokens]\n",
    "        truncated_text = tokenizer.decode(truncated_tokens, skip_special_tokens=True)\n",
    "        \n",
    "        # Try to end at a sentence boundary\n",
    "        last_period = truncated_text.rfind('.')\n",
    "        if last_period > 0:\n",
    "            truncated_text = truncated_text[:last_period + 1]\n",
    "            \n",
    "        return truncated_text\n",
    "\n",
    "    def _format_context_within_limits(self, context: Dict) -> str:\n",
    "        \"\"\"Format context while respecting token limits.\"\"\"\n",
    "        # Allocate tokens for different parts\n",
    "        DOC_TOKENS = 1200  # Reserve tokens for documents\n",
    "        PERSON_TOKENS = 400  # Reserve tokens for person context\n",
    "        INSTRUCTION_TOKENS = 200  # Reserve tokens for instructions\n",
    "        \n",
    "        formatted_docs = self._format_document_context(context['documents'])\n",
    "        formatted_persons = self._format_person_context(context['persons'])\n",
    "        \n",
    "        # Truncate if needed\n",
    "        formatted_docs = self._truncate_text(formatted_docs, DOC_TOKENS)\n",
    "        formatted_persons = self._truncate_text(formatted_persons, PERSON_TOKENS)\n",
    "        \n",
    "        return f\"Dokumenttikonteksti:\\n{formatted_docs}\\n\\nHenkilökonteksti:\\n{formatted_persons}\"\n",
    "\n",
    "def verify_neo4j_components():\n",
    "    \"\"\"Verify Neo4j components are working correctly.\"\"\"\n",
    "    try:\n",
    "        # Create base pipeline\n",
    "        base_pipeline = RAGPipeline()\n",
    "        \n",
    "        # Create enhanced pipeline\n",
    "        neo4j_rag = Neo4jEnhancedRAGPipeline(\n",
    "            base_rag_pipeline=base_pipeline,\n",
    "            neo4j_uri=\"bolt://87.92.59.201:7687\"\n",
    "        )\n",
    "        \n",
    "        # Verify Neo4j connection\n",
    "        with neo4j_rag.neo4j_manager.driver.session() as session:\n",
    "            result = session.run(\"RETURN 1 as test\")\n",
    "            assert result.single()[\"test\"] == 1\n",
    "        \n",
    "        print_status(\"Neo4j Components\", True, \"Connection verified\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print_status(\"Neo4j Components\", False, f\"Error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Add verification call after other verifications\n",
    "verify_section(\"Neo4j Components\", verify_neo4j_components)\n",
    "\n",
    "## section 4: \n",
    "# Testing Infrastructure\n",
    "class RAGTester:\n",
    "    def __init__(self, pipeline: RAGPipeline):\n",
    "        self.pipeline = pipeline\n",
    "        self.test_results = []\n",
    "        \n",
    "    def run_test_suite(self, test_questions: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Run comprehensive tests on the RAG pipeline.\"\"\"\n",
    "        test_results = []\n",
    "        summary_stats = {\n",
    "            \"total_questions\": len(test_questions),\n",
    "            \"successful_responses\": 0,\n",
    "            \"failed_responses\": 0,\n",
    "            \"average_similarity\": 0.0,\n",
    "            \"direct_quote_ratio\": 0.0\n",
    "        }\n",
    "        \n",
    "        for question in test_questions:\n",
    "            try:\n",
    "                # Get response from pipeline\n",
    "                result = self.pipeline.query(question)\n",
    "                \n",
    "                # Validate response\n",
    "                validation = self._validate_response(result[\"answer\"])\n",
    "                \n",
    "                # Calculate response metrics\n",
    "                metrics = {\n",
    "                    \"question\": question,\n",
    "                    \"has_direct_quote\": validation[\"has_direct_quote\"],\n",
    "                    \"source_count\": len(result[\"sources\"]),\n",
    "                    \"max_similarity\": max(s[\"similarity_score\"] for s in result[\"sources\"]),\n",
    "                    \"response_quality\": validation,\n",
    "                    \"response_length\": len(result[\"answer\"]),\n",
    "                }\n",
    "                \n",
    "                test_results.append(metrics)\n",
    "                summary_stats[\"successful_responses\"] += 1\n",
    "                summary_stats[\"average_similarity\"] += metrics[\"max_similarity\"]\n",
    "                summary_stats[\"direct_quote_ratio\"] += int(metrics[\"has_direct_quote\"])\n",
    "                \n",
    "                # Print detailed results\n",
    "                self._print_test_result(question, result, metrics)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error testing question '{question}': {str(e)}\")\n",
    "                summary_stats[\"failed_responses\"] += 1\n",
    "                \n",
    "        # Calculate final statistics\n",
    "        if summary_stats[\"successful_responses\"] > 0:\n",
    "            summary_stats[\"average_similarity\"] /= summary_stats[\"successful_responses\"]\n",
    "            summary_stats[\"direct_quote_ratio\"] /= summary_stats[\"successful_responses\"]\n",
    "        \n",
    "        return {\n",
    "            \"detailed_results\": test_results,\n",
    "            \"summary\": summary_stats\n",
    "        }\n",
    "    \n",
    "    def _validate_response(self, response: str) -> Dict[str, bool]:\n",
    "        \"\"\"Validate Finnish language response with detailed checks.\"\"\"\n",
    "        validation = {\n",
    "            # Basic structural checks\n",
    "            \"has_source_reference\": bool(re.search(r'\\[Dokumentti \\d+\\]', response)),\n",
    "            \"has_direct_quote\": '\"' in response,\n",
    "            \"is_complete_sentence\": response.strip().endswith(('.', '?', '!')),\n",
    "            \"has_confidence\": bool(re.search(r'\\d+\\s*%', response)),\n",
    "            \"reasonable_length\": 10 <= len(response) <= 500,\n",
    "            \n",
    "            # Finnish language specific checks\n",
    "            \"has_finnish_chars\": bool(re.search(r'[äöåÄÖÅ]', response)),\n",
    "            \"proper_finnish_structure\": self._check_finnish_structure(response)\n",
    "        }\n",
    "        return validation\n",
    "    \n",
    "    def _check_finnish_structure(self, text: str) -> bool:\n",
    "        \"\"\"Check if the response follows typical Finnish sentence structure.\"\"\"\n",
    "        finnish_endings = [\n",
    "            'ssa', 'ssä', 'sta', 'stä', 'lla', 'llä', 'lta', 'ltä',\n",
    "            'ksi', 'in', 'en', 'teen', 'seen'\n",
    "        ]\n",
    "        words = text.lower().split()\n",
    "        if not words:\n",
    "            return False\n",
    "            \n",
    "        has_finnish_ending = any(\n",
    "            any(word.endswith(ending) for ending in finnish_endings)\n",
    "            for word in words\n",
    "        )\n",
    "        \n",
    "        return has_finnish_ending\n",
    "    \n",
    "    def _print_test_result(self, question: str, result: Dict, metrics: Dict):\n",
    "        \"\"\"Print formatted test results.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Answer: {result['answer']}\")\n",
    "        print(\"\\nMetrics:\")\n",
    "        print(f\"- Source count: {metrics['source_count']}\")\n",
    "        print(f\"- Max similarity: {metrics['max_similarity']:.2%}\")\n",
    "        print(f\"- Response length: {metrics['response_length']}\")\n",
    "        print(\"\\nValidation:\")\n",
    "        for key, value in metrics['response_quality'].items():\n",
    "            print(f\"- {key}: {'✅' if value else '❌'}\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "\n",
    "def run_rag_tests(pipeline: RAGPipeline, test_questions: List[str] = None):\n",
    "    \"\"\"Execute RAG tests with default or custom test questions.\"\"\"\n",
    "    if test_questions is None:\n",
    "        test_questions = [\n",
    "            \"Kuinka kauan Annikki oli naimisissa miehensä kanssa?\",  # Clear answer: 55 years from Annikki's document\n",
    "            #\"Mitä digitaalisia laitteita Elisa käyttää?\",  # Clear answer from Elisa's document about her devices\n",
    "            #\"Mikä on Jarmon tärkein toive arjessa?\",  # Clear answer about wanting to maintain possibility for solitude\n",
    "            #\"Millä alalla Kosti työskentelee eläkkeellä ollessaan?\",  # Clear answer: kauppa-ala from Kosti's document\n",
    "            #\"Montako lastenlasta Annikilla on?\"\n",
    "        ]\n",
    "    \n",
    "    tester = RAGTester(pipeline)\n",
    "    results = tester.run_test_suite(test_questions)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nTest Summary:\")\n",
    "    print(f\"Total questions: {results['summary']['total_questions']}\")\n",
    "    print(f\"Successful responses: {results['summary']['successful_responses']}\")\n",
    "    print(f\"Failed responses: {results['summary']['failed_responses']}\")\n",
    "    print(f\"Average similarity: {results['summary']['average_similarity']:.2%}\")\n",
    "    print(f\"Direct quote ratio: {results['summary']['direct_quote_ratio']:.2%}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def verify_testing_infrastructure():\n",
    "    pipeline = RAGPipeline()\n",
    "    tester = RAGTester(pipeline)\n",
    "    return bool(tester)\n",
    "\n",
    "verify_section(\"Testing Infrastructure\", verify_testing_infrastructure)\n",
    "\n",
    "## section 5: \n",
    "class TokenManager:\n",
    "    def __init__(self, max_tokens: int = 2048):\n",
    "        self.max_tokens = max_tokens\n",
    "        \n",
    "    def truncate_context(self, context: List[Dict], tokenizer) -> List[Dict]:\n",
    "        \"\"\"Truncate context while preserving most relevant information.\"\"\"\n",
    "        total_tokens = 0\n",
    "        truncated_context = []\n",
    "        \n",
    "        # Sort by relevance\n",
    "        sorted_context = sorted(context, key=lambda x: x.get('score', 0), reverse=True)\n",
    "        \n",
    "        for doc in sorted_context:\n",
    "            # Get tokens for current document\n",
    "            try:\n",
    "                tokens = tokenizer.encode(doc['text'])\n",
    "                token_count = len(tokens)\n",
    "                \n",
    "                # Check if adding this document would exceed limit\n",
    "                if total_tokens + token_count > self.max_tokens:\n",
    "                    # If we can fit a partial document, do so\n",
    "                    remaining_tokens = self.max_tokens - total_tokens\n",
    "                    if remaining_tokens > 100:  # Only if worth including\n",
    "                        partial_tokens = tokens[:remaining_tokens]\n",
    "                        truncated_text = tokenizer.decode(partial_tokens, skip_special_tokens=True)\n",
    "                        # End at a complete sentence if possible\n",
    "                        last_period = truncated_text.rfind('.')\n",
    "                        if last_period > 0:\n",
    "                            truncated_text = truncated_text[:last_period + 1]\n",
    "                        truncated_context.append({\n",
    "                            **doc,\n",
    "                            'text': truncated_text,\n",
    "                            'truncated': True\n",
    "                        })\n",
    "                    break\n",
    "                \n",
    "                total_tokens += token_count\n",
    "                truncated_context.append(doc)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error truncating document: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        return truncated_context\n",
    "\n",
    "    def format_for_llm(self, context: List[Dict], question: str) -> str:\n",
    "        \"\"\"Format truncated context for LLM input.\"\"\"\n",
    "        formatted_parts = [\n",
    "            \"Kysymys: \" + question,\n",
    "            \"\\nKonteksti:\"\n",
    "        ]\n",
    "        \n",
    "        for i, doc in enumerate(context, 1):\n",
    "            doc_text = (\n",
    "                f\"\\nDokumentti {i}:\\n\"\n",
    "                f\"Lähde: {doc.get('source', 'Tuntematon')}\\n\"\n",
    "                f\"Teksti: {doc['text']}\\n\"\n",
    "                f\"{'(Katkaistu)' if doc.get('truncated') else ''}\"\n",
    "            )\n",
    "            formatted_parts.append(doc_text)\n",
    "            \n",
    "        return \"\\n\".join(formatted_parts)\n",
    "\n",
    "## section 6:\n",
    "\n",
    "# Add these imports at the top of your file\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from typing import List, Dict, Any, Optional\n",
    "import re\n",
    "from neo4j import GraphDatabase\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FinnishRAGAgent:\n",
    "    def __init__(self, base_pipeline: RAGPipeline):\n",
    "        self.pipeline = base_pipeline\n",
    "        self.memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "        self.min_similarity_threshold = 0.65\n",
    "        self.max_answer_length = 150\n",
    "        self.setup_tools()\n",
    "        \n",
    "    def setup_tools(self):\n",
    "        \"\"\"Initialize search and analysis tools.\"\"\"\n",
    "        self.tools = {\n",
    "            \"semantic_search\": self._semantic_search,\n",
    "            \"exact_match\": self._exact_match_search,\n",
    "            \"context_analysis\": self._analyze_context\n",
    "        }\n",
    "    \n",
    "    def _semantic_search(self, query: str) -> List[Dict]:\n",
    "        \"\"\"Enhanced semantic search with Finnish preprocessing.\"\"\"\n",
    "        try:\n",
    "            preprocessed_query = self._preprocess_finnish_text(query)\n",
    "            base_results = self.pipeline.query(preprocessed_query, top_k=5)\n",
    "            \n",
    "            # Enhance results with comprehensive scoring\n",
    "            enhanced_results = []\n",
    "            for result in base_results.get('sources', []):  # Use .get() with default\n",
    "                try:\n",
    "                    score = self._calculate_relevance_score(preprocessed_query, result)\n",
    "                    enhanced_results.append({\n",
    "                        'text': result.get('text', ''),\n",
    "                        'score': score,\n",
    "                        'source': result.get('source') or result.get('document_id', 'unknown'),  # Handle both keys\n",
    "                        'metadata': {\n",
    "                            'person_name': result.get('person_name', ''),\n",
    "                            'chunk_index': result.get('chunk_index', 0)\n",
    "                        }\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing result: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            return sorted(enhanced_results, key=lambda x: x['score'], reverse=True)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in semantic search: {str(e)}\")\n",
    "            return []\n",
    "    def _normalize_finnish_text(self, text: str) -> str:\n",
    "        \"\"\"Normalize Finnish text for matching.\"\"\"\n",
    "        try:\n",
    "            # Convert to lowercase\n",
    "            text = text.lower()\n",
    "            \n",
    "            # Remove diacritics but keep ä, ö\n",
    "            special_chars = {\n",
    "                'å': 'a',\n",
    "                'é': 'e',\n",
    "                'è': 'e',\n",
    "                'ü': 'u',\n",
    "                # Keep ä and ö as they are important in Finnish\n",
    "            }\n",
    "            for char, replacement in special_chars.items():\n",
    "                text = text.replace(char, replacement)\n",
    "            \n",
    "            # Remove extra whitespace\n",
    "            text = ' '.join(text.split())\n",
    "            \n",
    "            # Handle common Finnish abbreviations\n",
    "            abbreviations = {\n",
    "                'esim.': 'esimerkiksi',\n",
    "                'ns.': 'niin sanottu',\n",
    "                'jne.': 'ja niin edelleen',\n",
    "                'ym.': 'ynnä muuta',\n",
    "                'mm.': 'muun muassa'\n",
    "            }\n",
    "            for abbr, full in abbreviations.items():\n",
    "                text = text.replace(abbr, full)\n",
    "                \n",
    "            return text\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error normalizing text: {str(e)}\")\n",
    "            return text\n",
    "            \n",
    "    def _exact_match_search(self, query: str) -> List[Dict]:\n",
    "        \"\"\"Direct text matching with Finnish normalization.\"\"\"\n",
    "        try:\n",
    "            normalized_query = self._normalize_finnish_text(query)\n",
    "            \n",
    "            # Use pipeline's collection directly\n",
    "            collection = self.pipeline.collection\n",
    "            results = collection.query(\n",
    "                expr=f'text like \"%{normalized_query}%\"',\n",
    "                output_fields=[\"text\", \"person_name\", \"document_id\", \"chunk_index\"]\n",
    "            )\n",
    "            \n",
    "            # Apply comprehensive scoring to exact matches as well\n",
    "            scored_results = []\n",
    "            for r in results:\n",
    "                try:\n",
    "                    result_dict = {\n",
    "                        'text': r.get('text', ''),\n",
    "                        'source': r.get('document_id', 'unknown'),\n",
    "                        'metadata': {\n",
    "                            'person_name': r.get('person_name', ''),\n",
    "                            'chunk_index': r.get('chunk_index', 0)\n",
    "                        }\n",
    "                    }\n",
    "                    score = self._calculate_relevance_score(normalized_query, result_dict)\n",
    "                    result_dict['score'] = score\n",
    "                    scored_results.append(result_dict)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error scoring result: {str(e)}\")\n",
    "                    continue\n",
    "                    \n",
    "            return scored_results\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in exact match search: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def _analyze_context(self, passages: List[Dict]) -> Dict:\n",
    "        \"\"\"Analyze relationships and context in Finnish text.\"\"\"\n",
    "        context_data = {\n",
    "            'entities': set(),\n",
    "            'relationships': [],\n",
    "            'temporal_refs': [],\n",
    "            'key_topics': set()\n",
    "        }\n",
    "        \n",
    "        for passage in passages:\n",
    "            text = passage['text']\n",
    "            \n",
    "            # Extract Finnish names and entities\n",
    "            entities = self._extract_finnish_entities(text)\n",
    "            context_data['entities'].update(entities)\n",
    "            \n",
    "            # Find relationships\n",
    "            relationships = self._find_relationships(text)\n",
    "            context_data['relationships'].extend(relationships)\n",
    "            \n",
    "            # Extract temporal information\n",
    "            temporal = self._extract_temporal_refs(text)\n",
    "            context_data['temporal_refs'].extend(temporal)\n",
    "            \n",
    "        return context_data\n",
    "\n",
    "    def _preprocess_finnish_text(self, text: str) -> str:\n",
    "        \"\"\"Preprocess Finnish text for better matching.\"\"\"\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text.strip())\n",
    "        \n",
    "        # Handle common Finnish abbreviations\n",
    "        abbreviations = {\n",
    "            'esim.': 'esimerkiksi',\n",
    "            'ns.': 'niin sanottu',\n",
    "            'jne.': 'ja niin edelleen'\n",
    "        }\n",
    "        for abbr, full in abbreviations.items():\n",
    "            text = text.replace(abbr, full)\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def _find_relationships(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Find relationships in Finnish text.\"\"\"\n",
    "        relationships = []\n",
    "        \n",
    "        # Common Finnish relationship patterns\n",
    "        patterns = [\n",
    "            (r'(\\w+)\\s+on\\s+(\\w+)\\s+ystävä', 'ystävyys'),\n",
    "            (r'(\\w+)\\s+asuu\\s+(\\w+)', 'asuminen'),\n",
    "            (r'(\\w+)\\s+tekee\\s+(\\w+)', 'toiminta'),\n",
    "            (r'(\\w+)\\s+pitää\\s+(\\w+)', 'pitäminen'),\n",
    "            (r'(\\w+)\\s+kanssa', 'yhteys'),\n",
    "            (r'(\\w+)\\s+tärkeä', 'tärkeys'),\n",
    "            (r'(\\w+)\\s+auttaa\\s+(\\w+)', 'auttaminen')\n",
    "        ]\n",
    "        \n",
    "        for pattern, rel_type in patterns:\n",
    "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                relationships.append({\n",
    "                    'type': rel_type,\n",
    "                    'entities': match.groups(),\n",
    "                    'text': match.group(0)\n",
    "                })\n",
    "        \n",
    "        return relationships\n",
    "\n",
    "    def _extract_temporal_refs(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Extract temporal references from Finnish text.\"\"\"\n",
    "        temporal_refs = []\n",
    "        \n",
    "        # Common Finnish temporal patterns\n",
    "        patterns = [\n",
    "            (r'\\d+\\s*vuotta', 'duration'),\n",
    "            (r'\\d+\\s*vuotias', 'age'),\n",
    "            (r'(maanantai|tiistai|keskiviikko|torstai|perjantai|lauantai|sunnuntai)', 'weekday'),\n",
    "            (r'(tammikuu|helmikuu|maaliskuu|huhtikuu|toukokuu|kesäkuu|heinäkuu|elokuu|syyskuu|lokakuu|marraskuu|joulukuu)', 'month'),\n",
    "            (r'(aamu|päivä|ilta|yö)', 'time_of_day'),\n",
    "            (r'(eilen|tänään|huomenna)', 'relative_day'),\n",
    "            (r'(viikko|kuukausi|vuosi)', 'time_unit')\n",
    "        ]\n",
    "        \n",
    "        for pattern, ref_type in patterns:\n",
    "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                temporal_refs.append({\n",
    "                    'type': ref_type,\n",
    "                    'text': match.group(0),\n",
    "                    'position': match.span()\n",
    "                })\n",
    "        \n",
    "        return temporal_refs\n",
    "    \n",
    "    def _identify_key_topics(self, text: str) -> set:\n",
    "        \"\"\"Identify key topics in Finnish text.\"\"\"\n",
    "        topics = set()\n",
    "        \n",
    "        # Common Finnish topic indicators\n",
    "        key_patterns = [\n",
    "            (r'tärkeä\\w*\\s+(\\w+)', 'importance'),\n",
    "            (r'harrastaa\\w*\\s+(\\w+)', 'hobby'),\n",
    "            (r'pitää\\w*\\s+(\\w+)', 'preference'),\n",
    "            (r'ongelma\\w*\\s+(\\w+)', 'problem'),\n",
    "            (r'tavoite\\w*\\s+(\\w+)', 'goal')\n",
    "        ]\n",
    "        \n",
    "        for pattern, topic_type in key_patterns:\n",
    "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                if len(match.groups()) > 0:\n",
    "                    topics.add(f\"{topic_type}:{match.group(1)}\")\n",
    "        \n",
    "        return topics\n",
    "\n",
    "    def _extract_finnish_entities(self, text: str) -> set:\n",
    "        \"\"\"Extract Finnish named entities.\"\"\"\n",
    "        entities = set()\n",
    "        \n",
    "        # Match Finnish names (simplified)\n",
    "        name_pattern = r'\\b[A-ZÄÖÅ][a-zäöå]+\\b'\n",
    "        entities.update(re.findall(name_pattern, text))\n",
    "        \n",
    "        return entities\n",
    "\n",
    "    def _calculate_relevance_score(self, query: str, document: Dict) -> float:\n",
    "        \"\"\"Calculate comprehensive relevance score.\"\"\"\n",
    "        base_score = document.get('score', 0.0)\n",
    "        text = document.get('text', '')\n",
    "        \n",
    "        # Initialize boosting factors\n",
    "        boosts = {\n",
    "            'exact_match': 1.0,\n",
    "            'semantic_similarity': 1.0,\n",
    "            'finnish_specific': 1.0,\n",
    "            'context_match': 1.0\n",
    "        }\n",
    "        \n",
    "        # Exact match boost\n",
    "        if any(term.lower() in text.lower() for term in query.split()):\n",
    "            boosts['exact_match'] *= 1.3\n",
    "        \n",
    "        # Finnish-specific patterns boost\n",
    "        finnish_patterns = [\n",
    "            (r'\\b(ssa|ssä|sta|stä|lla|llä|lta|ltä)\\b', 1.1),  # Case suffixes\n",
    "            (r'\\b(ja|tai|eli|sekä)\\b', 1.05),  # Conjunctions\n",
    "            (r'[.!?][^.!?]+\\?', 1.2),  # Question-answer patterns\n",
    "            (r'\\b[A-ZÄÖÅ][a-zäöå]+\\b', 1.15)  # Named entities\n",
    "        ]\n",
    "        \n",
    "        for pattern, boost in finnish_patterns:\n",
    "            if re.search(pattern, text):\n",
    "                boosts['finnish_specific'] *= boost\n",
    "        \n",
    "        # Context match boost\n",
    "        if self.memory:\n",
    "            recent_context = self.memory.load_memory_variables({})\n",
    "            if recent_context.get('chat_history'):\n",
    "                context_terms = set(word.lower() for word in \n",
    "                    ' '.join(recent_context['chat_history']).split())\n",
    "                text_terms = set(word.lower() for word in text.split())\n",
    "                overlap = len(context_terms & text_terms)\n",
    "                if overlap > 0:\n",
    "                    boosts['context_match'] *= (1 + (overlap * 0.1))\n",
    "        \n",
    "        # Calculate final score\n",
    "        final_score = base_score\n",
    "        for boost in boosts.values():\n",
    "            final_score *= boost\n",
    "        \n",
    "        return min(1.0, final_score)\n",
    "    \n",
    "    def _generate_response(self, question: str, results: List[Dict], chat_history: str = None) -> Dict:\n",
    "        \"\"\"Generate focused, concise response with proper citations.\"\"\"\n",
    "        if not results:\n",
    "            return {\n",
    "                \"answer\": \"En löytänyt vastausta kysymykseesi.\",\n",
    "                \"confidence\": 0.0,\n",
    "                \"sources\": []\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            # Build focused prompt\n",
    "            prompt = self._build_focused_prompt(question, results)\n",
    "            \n",
    "            # Generate initial answer\n",
    "            response = self.pipeline.generate_answer(prompt)\n",
    "            \n",
    "            # Process and validate response\n",
    "            answer = self._process_response(response, results)\n",
    "            \n",
    "            # Force concise summary\n",
    "            answer = self._enforce_concise_answer(answer)\n",
    "            \n",
    "            return {\n",
    "                \"answer\": answer,\n",
    "                \"confidence\": max(r['score'] for r in results),\n",
    "                \"sources\": results[:3]\n",
    "            }\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in response generation: {str(e)}\")\n",
    "            return {\n",
    "                \"answer\": \"Virhe vastauksen muodostamisessa.\",\n",
    "                \"confidence\": 0.0,\n",
    "                \"sources\": []\n",
    "            }\n",
    "            \n",
    "    def _enforce_concise_answer(self, answer: str) -> str:\n",
    "        \"\"\"Force answer to be concise by keeping only essential information.\"\"\"\n",
    "        try:\n",
    "            # Split into sentences\n",
    "            sentences = re.split(r'(?<=[.!?])\\s+', answer)\n",
    "            \n",
    "            # Keep only essential parts:\n",
    "            # 1. Document reference\n",
    "            # 2. Direct quote or key information\n",
    "            # 3. Confidence if highly relevant\n",
    "            essential_sentences = []\n",
    "            for sentence in sentences:\n",
    "                if any(x in sentence.lower() for x in ['dokumentin', 'dokumentista', 'lähteen']):\n",
    "                    essential_sentences.append(sentence)\n",
    "                elif '\"' in sentence:  # Contains a quote\n",
    "                    essential_sentences.append(sentence)\n",
    "                    break  # Stop after first quote\n",
    "            \n",
    "            if not essential_sentences:\n",
    "                # If no structured parts found, take first sentence only\n",
    "                return sentences[0] if sentences else answer\n",
    "                \n",
    "            # Join essential parts\n",
    "            concise_answer = ' '.join(essential_sentences)\n",
    "            \n",
    "            # Ensure we have source and quote\n",
    "            if 'dokumentin' not in concise_answer.lower():\n",
    "                concise_answer = f\"Dokumentin mukaan {concise_answer}\"\n",
    "            \n",
    "            return concise_answer\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error enforcing concise answer: {str(e)}\")\n",
    "            # Fallback to simple truncation\n",
    "            return answer[:150] + (\"...\" if len(answer) > 150 else \"\")\n",
    "\n",
    "    def _build_focused_prompt(self, question: str, results: List[Dict]) -> str:\n",
    "        \n",
    "        \"\"\"Build a focused prompt for concise answer generation.\"\"\"\n",
    "        context = f\"\"\"Etsi tarkka ja ytimekäs vastaus seuraavaan kysymykseen käyttäen vain annettua kontekstia.\n",
    "Vastaa mahdollisimman lyhyesti ja selkeästi, keskittyen vain olennaiseen tietoon.\n",
    "\n",
    "Kysymys: {question}\n",
    "\n",
    "Konteksti:\n",
    "\"\"\"\n",
    "        # Add most relevant documents\n",
    "        for i, result in enumerate(results[:3], 1):\n",
    "            context += f\"\\nDokumentti {i}:\\nLähde: {result['document_id']}\\nTeksti: {result['text']}\\n\"\n",
    "        \n",
    "        context += \"\"\"\\nVastausohjeet:\n",
    "1. Jos löydät suoran vastauksen:\n",
    "   - Mainitse dokumentti, josta vastaus löytyy\n",
    "   - Käytä lyhyitä, tarkkoja lainauksia\n",
    "   - Keskity vain kysyttyyn asiaan\n",
    "   - Vältä turhaa toistoa ja ylimääräisiä selityksiä\n",
    "2. Jos et löydä vastausta:\n",
    "   - Ilmoita selkeästi ja lyhyesti, ettei vastausta löydy\n",
    "3. Jos löydät vain osittaisen vastauksen:\n",
    "   - Mainitse lyhyesti löydetty tieto ja mitä puuttuu\n",
    "\n",
    "Vastaus:\"\"\"\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def _summarize_answer(self, answer: str) -> str:\n",
    "        \"\"\"Summarize the answer to be concise and focused.\"\"\"\n",
    "        # If answer is already short enough, return as is\n",
    "        if len(answer) <= self.max_answer_length:\n",
    "            return answer\n",
    "            \n",
    "        try:\n",
    "            # Split into sentences\n",
    "            sentences = re.split(r'(?<=[.!?])\\s+', answer)\n",
    "            \n",
    "            # Start with the first sentence that has document reference\n",
    "            summary_parts = []\n",
    "            doc_ref_found = False\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                if 'Dokumentin' in sentence or 'dokumentin' in sentence:\n",
    "                    summary_parts.append(sentence)\n",
    "                    doc_ref_found = True\n",
    "                    continue\n",
    "                    \n",
    "                if doc_ref_found and '\"' in sentence:\n",
    "                    # Include the quote if it's essential\n",
    "                    summary_parts.append(sentence)\n",
    "                    break\n",
    "            \n",
    "            # If no structured summary was possible, take the first part\n",
    "            if not summary_parts:\n",
    "                return answer[:self.max_answer_length] + \"...\"\n",
    "                \n",
    "            summary = ' '.join(summary_parts)\n",
    "            \n",
    "            # Ensure the summary ends properly\n",
    "            if not any(summary.endswith(end) for end in '.!?'):\n",
    "                summary += '.'\n",
    "                \n",
    "            return summary\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in summarization: {str(e)}\")\n",
    "            # Fallback to simple truncation\n",
    "            return answer[:self.max_answer_length] + \"...\"\n",
    "    \n",
    "    def _analyze_query_type(self, query: str) -> str:\n",
    "        \"\"\"Analyze the type of query for better response structuring.\"\"\"\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        # Extract query characteristics without hardcoding specific names or values\n",
    "        characteristics = {\n",
    "            'personal_info': any(word in query_lower for word in ['kuka', 'kenen', 'kenelle']),\n",
    "            'age_related': any(word in query_lower for word in ['vanha', 'ikä', 'syntymä', 'vuosi']),\n",
    "            'activity_related': any(word in query_lower for word in ['tekee', 'harrastaa', 'pitää', 'tykkää']),\n",
    "            'ability_related': any(word in query_lower for word in ['pystyy', 'osaa', 'liikkuu', 'käyttää']),\n",
    "            'preference_related': any(word in query_lower for word in ['pitää', 'tykkää', 'haluaa']),\n",
    "            'relationship_related': any(word in query_lower for word in ['ystävä', 'tärkeä', 'läheinen'])\n",
    "        }\n",
    "        \n",
    "        # Return the most likely query type\n",
    "        return max(characteristics.items(), key=lambda x: x[1])[0]\n",
    "    \n",
    "    def _get_query_instructions(self, query_type: str) -> str:\n",
    "        \"\"\"Get specific instructions based on query type.\"\"\"\n",
    "        instructions = {\n",
    "            'personal_info': \"\"\"\n",
    "        Ohje: \n",
    "        1. Etsi henkilöön liittyvät suorat maininnat\n",
    "        2. Käytä suoria lainauksia henkilöiden nimistä ja suhteista\n",
    "        3. Mainitse dokumentin lähde\"\"\",\n",
    "                \n",
    "                'age_related': \"\"\"\n",
    "        Ohje:\n",
    "        1. Etsi tarkat ikä- ja syntymävuositiedot\n",
    "        2. Ilmoita sekä ikä että syntymävuosi jos molemmat löytyvät\n",
    "        3. Mainitse dokumentin lähde\"\"\",\n",
    "                \n",
    "                'activity_related': \"\"\"\n",
    "        Ohje:\n",
    "        1. Etsi kaikki mainitut aktiviteetit ja harrastukset\n",
    "        2. Käytä suoria lainauksia aktiviteettien kuvauksista\n",
    "        3. Mainitse dokumentin lähde\"\"\",\n",
    "                \n",
    "                'ability_related': \"\"\"\n",
    "        Ohje:\n",
    "        1. Etsi kuvaukset henkilön kyvyistä ja toiminnasta\n",
    "        2. Käytä suoria lainauksia toimintakyvyn kuvauksista\n",
    "        3. Mainitse dokumentin lähde\"\"\",\n",
    "                \n",
    "                'preference_related': \"\"\"\n",
    "        Ohje:\n",
    "        1. Etsi kaikki mainitut mieltymykset ja kiinnostukset\n",
    "        2. Käytä suoria lainauksia mieltymysten kuvauksista\n",
    "        3. Mainitse dokumentin lähde\"\"\",\n",
    "                \n",
    "                'relationship_related': \"\"\"\n",
    "        Ohje:\n",
    "        1. Etsi kuvaukset ihmissuhteista ja tärkeistä henkilöistä\n",
    "        2. Käytä suoria lainauksia suhteiden kuvauksista\n",
    "        3. Mainitse dokumentin lähde\"\"\"\n",
    "            }\n",
    "            \n",
    "        return instructions.get(query_type, \"\"\"\n",
    "        Ohje:\n",
    "        1. Etsi suora vastaus kysymykseen dokumenteista\n",
    "        2. Käytä suoria lainauksia\n",
    "        3. Mainitse dokumentin lähde\"\"\")\n",
    "    \n",
    "    def _process_response(self, response: str, results: List[Dict]) -> str:\n",
    "        \"\"\"Process and validate the generated response with focus on conciseness.\"\"\"\n",
    "        if not response:\n",
    "            return \"En löytänyt vastausta annetusta kontekstista.\"\n",
    "        \n",
    "        # Clean up the response\n",
    "        answer = response.strip()\n",
    "        \n",
    "        # Remove redundant phrases and metacommentary\n",
    "        redundant_phrases = [\n",
    "            \"On tärkeää huomata, että\",\n",
    "            \"On syytä mainita, että\",\n",
    "            \"Voidaan todeta, että\",\n",
    "            \"Tämän perusteella\",\n",
    "            \"Yhteenvetona voidaan sanoa, että\",\n",
    "            \"Luottamus:\",\n",
    "            \"Selitys:\",\n",
    "            \"Huomaa, että\",\n",
    "            \"On tärkeää muistaa\"\n",
    "        ]\n",
    "        \n",
    "        for phrase in redundant_phrases:\n",
    "            answer = answer.replace(phrase, \"\")\n",
    "        \n",
    "        # Ensure source citations\n",
    "        if not any(f\"Dokumentti\" in answer for result in results):\n",
    "            for result in results:\n",
    "                if result['text'] in answer:\n",
    "                    answer = f\"Dokumentin {result['document_id']} mukaan {answer}\"\n",
    "                    break\n",
    "        \n",
    "        # Ensure proper quotes but keep them minimal\n",
    "        if '\"' not in answer and any(result['text'] in answer for result in results):\n",
    "            # Find the most relevant quote\n",
    "            for result in results:\n",
    "                overlap = SequenceMatcher(None, result['text'], answer).find_longest_match(0, len(result['text']), 0, len(answer))\n",
    "                if overlap.size > 20:\n",
    "                    quoted_text = result['text'][overlap.a:overlap.a + overlap.size]\n",
    "                    answer = f\"Dokumentin {result['document_id']} mukaan \\\"{quoted_text}\\\"\"\n",
    "                    break\n",
    "        \n",
    "        return answer.strip()\n",
    "    \n",
    "    def process_query(self, question: str) -> Dict:\n",
    "        try:\n",
    "            # Get chat history as text\n",
    "            chat_history = []\n",
    "            if self.memory:\n",
    "                memory_vars = self.memory.load_memory_variables({})\n",
    "                if \"chat_history\" in memory_vars:\n",
    "                    # Convert messages to text format\n",
    "                    chat_history = [\n",
    "                        f\"{msg.type}: {msg.content}\"\n",
    "                        for msg in memory_vars[\"chat_history\"]\n",
    "                    ]\n",
    "\n",
    "            # Process query with context\n",
    "            semantic_results = self._semantic_search(question)\n",
    "            \n",
    "            if not semantic_results:\n",
    "                exact_matches = self._exact_match_search(question)\n",
    "                all_results = semantic_results + exact_matches\n",
    "                all_results = sorted(all_results, key=lambda x: x.get('score', 0), reverse=True)\n",
    "            else:\n",
    "                all_results = semantic_results\n",
    "\n",
    "            # Generate response\n",
    "            response = self._generate_response(\n",
    "                question, \n",
    "                all_results, \n",
    "                \"\\n\".join(chat_history)\n",
    "            )\n",
    "\n",
    "            # Store in memory\n",
    "            if response and response.get('answer'):\n",
    "                self.memory.save_context(\n",
    "                    {\"input\": question},\n",
    "                    {\"output\": response['answer']}\n",
    "                )\n",
    "\n",
    "            return response\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing query: {str(e)}\")\n",
    "            return {\n",
    "                \"answer\": \"Virhe kysymyksen käsittelyssä.\",\n",
    "                \"confidence\": 0.0,\n",
    "                \"sources\": []\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing query: {str(e)}\")\n",
    "            # Return a safe default response\n",
    "            return {\n",
    "                \"answer\": \"Virhe kysymyksen käsittelyssä.\",\n",
    "                \"confidence\": 0.0,\n",
    "                \"sources\": [],\n",
    "                \"context_analysis\": {}\n",
    "            }\n",
    "            \n",
    "def run_neo4j_enhanced_tests():\n",
    "    try:\n",
    "        print(\"\\nStarting Neo4j Enhanced RAG test execution...\")\n",
    "        \n",
    "        # Initialize base pipeline\n",
    "        print(\"\\n1. Initializing base pipeline...\")\n",
    "        base_pipeline = RAGPipeline()\n",
    "        print(\"   ✓ Base pipeline initialized\")\n",
    "        \n",
    "        # Initialize Neo4j enhanced pipeline\n",
    "        print(\"\\n2. Creating Neo4j Enhanced Pipeline...\")\n",
    "        neo4j_rag = Neo4jEnhancedRAGPipeline(\n",
    "            base_rag_pipeline=base_pipeline,\n",
    "            neo4j_uri=\"bolt://87.92.59.201:7687\"  # Update with your Neo4j URI\n",
    "        )\n",
    "        print(\"   ✓ Neo4j Enhanced pipeline created\")\n",
    "        \n",
    "        # Process documents\n",
    "        print(\"\\n3. Processing documents...\")\n",
    "        folder_path = '/scratch/project_2011638/notebooks/data/'  # Update with your actual path\n",
    "        neo4j_rag.process_documents(folder_path)\n",
    "        print(\"   ✓ Documents processed with Neo4j enhancement\")\n",
    "        \n",
    "        # Define test questions\n",
    "        test_questions = [\n",
    "            \"Kuinka kauan Annikki oli naimisissa miehensä kanssa?\",  # Clear answer: 55 years from Annikki's document\n",
    "            #\"Mitä digitaalisia laitteita Elisa käyttää?\",  # Clear answer from Elisa's document about her devices\n",
    "            #\"Mikä on Jarmon tärkein toive arjessa?\",  # Clear answer about wanting to maintain possibility for solitude\n",
    "            #\"Millä alalla Kosti työskentelee eläkkeellä ollessaan?\",  # Clear answer: kauppa-ala from Kosti's document\n",
    "            #\"Montako lastenlasta Annikilla on?\"\n",
    "        ]\n",
    "        \n",
    "        # Run tests\n",
    "        print(\"\\n4. Running test questions with Neo4j enhancement...\")\n",
    "        results = []\n",
    "        for i, question in enumerate(test_questions, 1):\n",
    "            try:\n",
    "                print(f\"\\nKysymys {i}/{len(test_questions)}:\")\n",
    "                print(\"-\"*80)\n",
    "                print(f\"{question}\")\n",
    "                print(\"-\"*80)\n",
    "                \n",
    "                # Get results from Neo4j enhanced pipeline\n",
    "                response = neo4j_rag.query(question)\n",
    "                results.append({\n",
    "                    \"question\": question,\n",
    "                    \"response\": response\n",
    "                })\n",
    "                \n",
    "                # Print results\n",
    "                print(\"\\nNeo4j Enhanced Vastaus:\")\n",
    "                print(response.get('answer', 'Ei vastausta'))\n",
    "                print(\"\\nLähteet ja konteksti:\")\n",
    "                print(\"- Dokumenttilähteet:\")\n",
    "                for src in response.get('sources', [])[:2]:\n",
    "                    print(f\"  • {src['document_id']}: {src['text'][:100]}...\")\n",
    "                print(\"- Graafikonteksti:\")\n",
    "                if response.get('person_context'):\n",
    "                    print(f\"  • Henkilö: {response['person_context']['name']}\")\n",
    "                    print(f\"  • Suhteet: {', '.join(response['person_context'].get('relationship_types', []))}\")\n",
    "                print(\"-\"*80)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Virhe kysymyksen käsittelyssä: {str(e)}\")\n",
    "        \n",
    "        print(\"\\n5. Neo4j Enhanced testaus valmis\")\n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nVirhe Neo4j testauksessa: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def run_improved_rag_tests():\n",
    "    try:\n",
    "        print(\"\\nStarting RAG test execution...\")\n",
    "        \n",
    "        # Initialize base pipeline\n",
    "        print(\"\\n1. Initializing base pipeline...\")\n",
    "        base_pipeline = RAGPipeline()\n",
    "        print(\"   ✓ Base pipeline initialized\")\n",
    "        \n",
    "        # Initialize Finnish RAG Agent\n",
    "        print(\"\\n2. Creating Finnish RAG Agent...\")\n",
    "        agent = FinnishRAGAgent(base_pipeline)\n",
    "        print(\"   ✓ Agent created\")\n",
    "        \n",
    "        # Process documents\n",
    "        print(\"\\n3. Processing documents...\")\n",
    "        folder_path = '/scratch/project_2011638/notebooks/data/'\n",
    "        base_pipeline.process_documents(folder_path)\n",
    "        print(\"   ✓ Documents processed\")\n",
    "        \n",
    "        # Define test questions\n",
    "        test_questions = [\n",
    "            \"Kuka on tärkeitä henkilöitä Eilalle?\",\n",
    "            \"Kuinka vanha on Sulo ja mikä on hänen syntymävuotensa?\",\n",
    "            \"Mitä Eila harrastaa?\",\n",
    "            \"Miten Sulo liikkuu?\",\n",
    "            \"Mistä asioista Eila pitää?\"\n",
    "        ]\n",
    "        \n",
    "        # Run tests through the agent\n",
    "        print(\"\\n4. Running test questions...\")\n",
    "        results = []\n",
    "        for i, question in enumerate(test_questions, 1):\n",
    "            try:\n",
    "                print(f\"\\nKysymys {i}/{len(test_questions)}:\")\n",
    "                print(\"-\"*80)\n",
    "                print(f\"{question}\")\n",
    "                print(\"-\"*80)\n",
    "                \n",
    "                response = agent.process_query(question)\n",
    "                results.append({\n",
    "                    \"question\": question,\n",
    "                    \"response\": response\n",
    "                })\n",
    "                \n",
    "                print(\"\\nVastaus:\")\n",
    "                print(response.get('answer', 'Ei vastausta'))\n",
    "                print(\"\\nLähde:\")\n",
    "                for src in response.get('sources', [])[:1]:\n",
    "                    print(f\"- {src['source']}: {src['text'][:150]}...\")\n",
    "                print(\"-\"*80)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Virhe kysymyksen käsittelyssä: {str(e)}\")\n",
    "        \n",
    "        print(\"\\n5. Testaus valmis\")\n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nVirhe testauksessa: {str(e)}\")\n",
    "        raise\n",
    "        \n",
    "class SharedPipelineManager:\n",
    "    def __init__(self):\n",
    "        self._base_pipeline = None\n",
    "        self._neo4j_pipeline = None\n",
    "        self._finnish_agent = None\n",
    "        self.cleanup_gpu_memory()\n",
    "        \n",
    "    @property\n",
    "    def base_pipeline(self):\n",
    "        if self._base_pipeline is None:\n",
    "            self._base_pipeline = RAGPipeline()\n",
    "        return self._base_pipeline\n",
    "    \n",
    "    @property\n",
    "    def neo4j_pipeline(self):\n",
    "        if self._neo4j_pipeline is None:\n",
    "            self._neo4j_pipeline = Neo4jEnhancedRAGPipeline(\n",
    "                base_rag_pipeline=self.base_pipeline,\n",
    "                neo4j_uri=\"bolt://87.92.59.201:7687\"\n",
    "            )\n",
    "        return self._neo4j_pipeline\n",
    "    \n",
    "    @property\n",
    "    def finnish_agent(self):\n",
    "        if self._finnish_agent is None:\n",
    "            self._finnish_agent = FinnishRAGAgent(self.base_pipeline)\n",
    "        return self._finnish_agent\n",
    "\n",
    "    def cleanup_gpu_memory(self):\n",
    "        \"\"\"Clean up GPU memory between pipeline operations\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "    def process_documents(self, folder_path: str):\n",
    "        \"\"\"Process documents once and share across all pipelines\"\"\"\n",
    "        print(\"Processing documents...\")\n",
    "        # Only process documents through Neo4j pipeline as it handles both\n",
    "        self.cleanup_gpu_memory()\n",
    "        self.neo4j_pipeline.process_documents(folder_path)\n",
    "        self.cleanup_gpu_memory()\n",
    "        print(\"Documents processed successfully\")\n",
    "        \n",
    "    def run_comparison_tests(self, test_questions: List[str]):\n",
    "        \"\"\"Run comparison tests using shared resources\"\"\"\n",
    "        results = {\n",
    "            \"standard\": [],\n",
    "            \"neo4j\": []\n",
    "        }\n",
    "        \n",
    "        for question in test_questions:\n",
    "            # Get standard RAG results\n",
    "            self.cleanup_gpu_memory()\n",
    "            standard_response = self.finnish_agent.process_query(question)\n",
    "            results[\"standard\"].append({\n",
    "                \"question\": question,\n",
    "                \"response\": standard_response\n",
    "            })\n",
    "            \n",
    "            # Get Neo4j enhanced results\n",
    "            neo4j_response = self.neo4j_pipeline.query(question)\n",
    "            results[\"neo4j\"].append({\n",
    "                \"question\": question,\n",
    "                \"response\": neo4j_response\n",
    "            })\n",
    "            \n",
    "        return results\n",
    "class SharedEmbeddingManager:\n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "        self.embedding_model = None\n",
    "        self.batch_size = 8\n",
    "\n",
    "    def get_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Get embeddings with caching.\"\"\"\n",
    "        # Create cache keys\n",
    "        cache_keys = [hashlib.md5(text.encode()).hexdigest() for text in texts]\n",
    "        \n",
    "        # Find missing embeddings\n",
    "        missing_indices = [i for i, key in enumerate(cache_keys) if key not in self.cache]\n",
    "        missing_texts = [texts[i] for i in missing_indices]\n",
    "        \n",
    "        if missing_texts:\n",
    "            # Generate embeddings in batches\n",
    "            all_embeddings = []\n",
    "            for i in range(0, len(missing_texts), self.batch_size):\n",
    "                batch = missing_texts[i:i + self.batch_size]\n",
    "                embeddings = self.embedding_model.generate(batch)\n",
    "                all_embeddings.extend(embeddings)\n",
    "                \n",
    "            # Update cache\n",
    "            for i, embedding in zip(missing_indices, all_embeddings):\n",
    "                self.cache[cache_keys[i]] = embedding\n",
    "        \n",
    "        # Return all embeddings in order\n",
    "        return np.array([self.cache[key] for key in cache_keys])   \n",
    "\n",
    "def run_comparison_tests():\n",
    "    \"\"\"Run both standard and Neo4j enhanced tests for comparison.\"\"\"\n",
    "    try:\n",
    "        print(\"\\nStarting comparison test execution...\")\n",
    "        \n",
    "        # Initialize shared pipeline manager\n",
    "        pipeline_manager = SharedPipelineManager()\n",
    "        \n",
    "        # Process documents once\n",
    "        folder_path = '/scratch/project_2011638/notebooks/data/'\n",
    "        pipeline_manager.process_documents(folder_path)\n",
    "        \n",
    "        # Define test questions\n",
    "        test_questions = [\n",
    "            \"Kuinka kauan Annikki oli naimisissa miehensä kanssa?\",  # Clear answer: 55 years from Annikki's document\n",
    "            #\"Mitä digitaalisia laitteita Elisa käyttää?\",  # Clear answer from Elisa's document about her devices\n",
    "            #\"Mikä on Jarmon tärkein toive arjessa?\",  # Clear answer about wanting to maintain possibility for solitude\n",
    "            #\"Millä alalla Kosti työskentelee eläkkeellä ollessaan?\",  # Clear answer: kauppa-ala from Kosti's document\n",
    "            #\"Montako lastenlasta Annikilla on?\"\n",
    "        ]\n",
    "        \n",
    "        # Run tests using shared resources\n",
    "        results = pipeline_manager.run_comparison_tests(test_questions)\n",
    "        \n",
    "        # Print comparison\n",
    "        print(\"\\n5. Results Comparison:\")\n",
    "        print(\"-\" * 80)\n",
    "        for std, neo in zip(results[\"standard\"], results[\"neo4j\"]):\n",
    "            print(f\"\\nQuestion: {std['question']}\")\n",
    "            print(\"\\nStandard RAG Answer:\")\n",
    "            print(std[\"response\"][\"answer\"])\n",
    "            print(\"\\nNeo4j Enhanced Answer:\")\n",
    "            print(neo[\"response\"][\"answer\"])\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in comparison testing: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = run_comparison_tests()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
