{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0365f501-ae24-4ac1-916e-d84f5e9a90bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/opt/conda/lib/python3.11/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 84\u001b[0m\n\u001b[1;32m     81\u001b[0m     print_status(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding Dimension\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing dimension: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEMBEDDING_DIM\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Run initial verification\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m \u001b[43mverify_initial_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Core Pipeline Components\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDocumentProcessor\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[1], line 79\u001b[0m, in \u001b[0;36mverify_initial_setup\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify_initial_setup\u001b[39m():\n\u001b[0;32m---> 79\u001b[0m     \u001b[43mcheck_cuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     ensure_stopwords_downloaded()\n\u001b[1;32m     81\u001b[0m     print_status(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding Dimension\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing dimension: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEMBEDDING_DIM\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 49\u001b[0m, in \u001b[0;36mcheck_cuda\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check CUDA availability and print status.\"\"\"\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     50\u001b[0m         device_name \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mget_device_name(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     51\u001b[0m         print_status(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA Check\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing GPU: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/cuda/__init__.py:107\u001b[0m, in \u001b[0;36mis_available\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m device_count() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# The default availability inspection never throws and returns 0 if the driver is missing or can't\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# be initialized. This uses the CUDA Runtime API `cudaGetDeviceCount` which in turn initializes the CUDA Driver\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# API via `cuInit`\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_getDeviceCount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "import logging\n",
    "from typing import List, Dict, Any\n",
    "from docx import Document\n",
    "from tqdm.notebook import tqdm\n",
    "from pymilvus import connections, Collection, utility, CollectionSchema, FieldSchema, DataType\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "## Env \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "\n",
    "# Utility Functions\n",
    "def print_status(section_name: str, status: bool, message: str = \"\"):\n",
    "    \"\"\"Print status of a section with colored output.\"\"\"\n",
    "    status_str = \"✅ SUCCESS\" if status else \"❌ FAILED\"\n",
    "    print(f\"\\n{status_str} | {section_name}\")\n",
    "    if message:\n",
    "        print(f\"  └─ {message}\")\n",
    "\n",
    "def verify_section(section_number: int, verification_func) -> bool:\n",
    "    \"\"\"Verify if a section was executed successfully.\"\"\"\n",
    "    try:\n",
    "        result = verification_func()\n",
    "        print_status(f\"Section {section_number} Verification\", True, \"Successfully executed\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print_status(f\"Section {section_number} Verification\", False, f\"Error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def get_model_embedding_dim(model_name: str = \"TurkuNLP/bert-base-finnish-cased-v1\") -> int:\n",
    "    \"\"\"Get embedding dimension from model config.\"\"\"\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    return model.config.hidden_size\n",
    "\n",
    "def check_cuda():\n",
    "    \"\"\"Check CUDA availability and print status.\"\"\"\n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            device_name = torch.cuda.get_device_name(0)\n",
    "            print_status(\"CUDA Check\", True, f\"Using GPU: {device_name}\")\n",
    "            return True\n",
    "        else:\n",
    "            print_status(\"CUDA Check\", True, \"Using CPU\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print_status(\"CUDA Check\", False, str(e))\n",
    "        return False\n",
    "\n",
    "def ensure_stopwords_downloaded(language='finnish'):\n",
    "    \"\"\"Download NLTK stopwords and print status.\"\"\"\n",
    "    try:\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "        print_status(\"NLTK Setup\", True, f\"Downloaded {language} stopwords\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print_status(\"NLTK Setup\", False, str(e))\n",
    "        return False\n",
    "\n",
    "# Global Constants\n",
    "#EMBEDDING_DIM = get_model_embedding_dim()\n",
    "EMBEDDING_DIM = 1536\n",
    "MILVUS_HOST = \"milvus-standalone\"\n",
    "MILVUS_PORT = \"19530\"\n",
    "MILVUS_ALIAS = \"default\"\n",
    "\n",
    "# Initial Setup Verification\n",
    "def verify_initial_setup():\n",
    "    check_cuda()\n",
    "    ensure_stopwords_downloaded()\n",
    "    print_status(\"Embedding Dimension\", True, f\"Using dimension: {EMBEDDING_DIM}\")\n",
    "    \n",
    "# Run initial verification\n",
    "verify_initial_setup()\n",
    "\n",
    "# Core Pipeline Components\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, chunk_size=400, chunk_overlap=80):\n",
    "        try:\n",
    "            self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "                separators=[\n",
    "                    \"\\n\\n\",  # First split on double newlines\n",
    "                    \"\\n\",    # Then single newlines\n",
    "                    \".\",     # Then sentence endings\n",
    "                    \":\",     # Then colons (common in Finnish formatting)\n",
    "                    \";\",     # Then semicolons\n",
    "                    \",\",     # Then commas\n",
    "                    \" \",     # Finally, split on spaces if needed\n",
    "                    \"\"\n",
    "                ],\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                length_function=len,\n",
    "                keep_separator=True,\n",
    "                add_start_index=True\n",
    "            )\n",
    "            \n",
    "            # Add Finnish-specific cleaning patterns\n",
    "            self.clean_patterns = [\n",
    "                (r'\\s+', ' '),  # Normalize whitespace\n",
    "                (r'[\\(\\{\\[\\]\\}\\)]', ''),  # Remove brackets\n",
    "                (r'[^\\w\\s\\.\\,\\?\\!\\-\\:\\;äöåÄÖÅ]', ''),  # Keep Finnish characters\n",
    "                (r'\\s+\\.', '.'),  # Fix spacing around periods\n",
    "                (r'\\.+', '.'),  # Normalize multiple periods\n",
    "            ]\n",
    "            print_status(\"Document Processor\", True, \"Initialized with Finnish-optimized settings\")\n",
    "        except Exception as e:\n",
    "            print_status(\"Document Processor\", False, str(e))\n",
    "            raise\n",
    "\n",
    "    def extract_metadata_from_filename(self, filename: str) -> tuple:\n",
    "        \"\"\"Extract metadata from filename.\"\"\"\n",
    "        title = os.path.splitext(filename)[0]\n",
    "        match = re.match(r'([A-Za-z]+)\\s+(\\d{1,3})v\\s+([A-Za-z0-9\\-]+)', title)\n",
    "        if match:\n",
    "            return match.group(1), int(match.group(2)), match.group(3)\n",
    "        return None, None, None\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize Finnish text with improved handling.\"\"\"\n",
    "        try:\n",
    "            # Apply all cleaning patterns\n",
    "            for pattern, replacement in self.clean_patterns:\n",
    "                text = re.sub(pattern, replacement, text)\n",
    "            \n",
    "            # Ensure proper sentence boundaries\n",
    "            text = re.sub(r'([.!?])\\s*([A-ZÄÖÅ])', r'\\1\\n\\2', text)\n",
    "            \n",
    "            # Remove extra whitespace while preserving paragraph breaks\n",
    "            text = '\\n'.join(line.strip() for line in text.split('\\n'))\n",
    "            return text.strip()\n",
    "        except Exception as e:\n",
    "            print_status(\"Text Preprocessing\", False, str(e))\n",
    "            raise\n",
    "\n",
    "    def process_document(self, file_path: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process document with improved metadata and chunking.\"\"\"\n",
    "        try:\n",
    "            # Read document\n",
    "            doc = Document(file_path)\n",
    "            text = \"\\n\".join([para.text for para in doc.paragraphs if para.text.strip()])\n",
    "            \n",
    "            # Extract metadata\n",
    "            filename = os.path.basename(file_path)\n",
    "            name, age, doc_id = self.extract_metadata_from_filename(filename)\n",
    "            \n",
    "            # Preprocess and split text\n",
    "            clean_text = self.preprocess_text(text)\n",
    "            chunks = self.text_splitter.split_text(clean_text)\n",
    "            \n",
    "            # Create chunks with enhanced metadata\n",
    "            processed_chunks = []\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                # Calculate semantic importance score\n",
    "                importance_score = self._calculate_chunk_importance(chunk)\n",
    "                \n",
    "                processed_chunks.append({\n",
    "                    \"text\": chunk,\n",
    "                    \"metadata\": {\n",
    "                        \"source\": filename,\n",
    "                        \"person_name\": name,\n",
    "                        \"person_age\": age,\n",
    "                        \"document_id\": doc_id,\n",
    "                        \"chunk_index\": i,\n",
    "                        \"importance_score\": importance_score,\n",
    "                        \"chunk_length\": len(chunk),\n",
    "                        \"contains_question\": \"?\" in chunk,\n",
    "                    }\n",
    "                })\n",
    "            \n",
    "            print_status(\"Document Processing\", True, \n",
    "                        f\"Processed {filename} into {len(processed_chunks)} chunks\")\n",
    "            return processed_chunks\n",
    "            \n",
    "        except Exception as e:\n",
    "            print_status(\"Document Processing\", False, f\"Error processing {file_path}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _calculate_chunk_importance(self, chunk: str) -> float:\n",
    "        \"\"\"Calculate importance score for chunk based on Finnish text patterns.\"\"\"\n",
    "        score = 1.0\n",
    "        \n",
    "        # Key phrase indicators (common in Finnish documentation)\n",
    "        key_phrases = [\n",
    "            \"tärkeä\", \"merkittävä\", \"olennainen\", \"keskeinen\",\n",
    "            \"huomattava\", \"erityinen\", \"tärkein\", \"ensisijainen\"\n",
    "        ]\n",
    "        \n",
    "        # Increase score for chunks with key phrases\n",
    "        if any(phrase in chunk.lower() for phrase in key_phrases):\n",
    "            score *= 1.2\n",
    "            \n",
    "        # Prefer chunks with complete sentences\n",
    "        if chunk.count('.') > 0:\n",
    "            score *= 1.1\n",
    "            \n",
    "        # Prefer chunks with personal pronouns (common in Finnish personal documents)\n",
    "        if any(pronoun in chunk.lower() for pronoun in [\"minä\", \"minun\", \"minua\", \"minulla\"]):\n",
    "            score *= 1.15\n",
    "            \n",
    "        return score\n",
    "\n",
    "\n",
    "class MilvusManager:\n",
    "    def __init__(self, host: str = \"milvus-standalone\", port: str = \"19530\", alias: str = \"default\"):\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.alias = alias\n",
    "        self.connected = False\n",
    "        self.connect()\n",
    "        \n",
    "    def connect(self):\n",
    "        \"\"\"Establish connection to Milvus.\"\"\"\n",
    "        try:\n",
    "            try:\n",
    "                connections.remove_connection(alias=self.alias)\n",
    "                print_status(\"Milvus Connection\", True, \"Cleaned up existing connection\")\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            connections.connect(\n",
    "                alias=self.alias,\n",
    "                host=self.host,\n",
    "                port=self.port,\n",
    "                timeout=10.0\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                utility.get_server_version()\n",
    "                self.connected = True\n",
    "                print_status(\"Milvus Connection\", True, f\"Connected to {self.host}:{self.port}\")\n",
    "            except Exception as ve:\n",
    "                raise Exception(f\"Connection verification failed: {str(ve)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.connected = False\n",
    "            print_status(\"Milvus Connection\", False, str(e))\n",
    "            raise\n",
    "\n",
    "    def create_collection(self, collection_name: str = \"document_embeddings\"):\n",
    "        \"\"\"Create Milvus collection with appropriate schema.\"\"\"\n",
    "        try:\n",
    "            if utility.has_collection(collection_name):\n",
    "                Collection(name=collection_name).drop()\n",
    "                print_status(\"Milvus Collection\", True, f\"Dropped existing collection: {collection_name}\")\n",
    "                \n",
    "            fields = [\n",
    "                FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "                FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "                FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=EMBEDDING_DIM),\n",
    "                FieldSchema(name=\"person_name\", dtype=DataType.VARCHAR, max_length=100),\n",
    "                FieldSchema(name=\"person_age\", dtype=DataType.INT64),\n",
    "                FieldSchema(name=\"document_id\", dtype=DataType.VARCHAR, max_length=100),\n",
    "                FieldSchema(name=\"chunk_index\", dtype=DataType.INT64)\n",
    "            ]\n",
    "            \n",
    "            schema = CollectionSchema(\n",
    "                fields=fields,\n",
    "                description=\"Document embeddings collection\",\n",
    "                enable_dynamic_field=False\n",
    "            )\n",
    "            collection = Collection(name=collection_name, schema=schema)\n",
    "            \n",
    "            self.create_and_load_index(collection)\n",
    "            \n",
    "            print_status(\"Milvus Collection\", True, f\"Created new collection: {collection_name} with dim={EMBEDDING_DIM}\")\n",
    "            return collection\n",
    "        except Exception as e:\n",
    "            print_status(\"Milvus Collection\", False, str(e))\n",
    "            raise\n",
    "\n",
    "    def create_and_load_index(self, collection):\n",
    "        try:\n",
    "            index_params = {\n",
    "                \"metric_type\": \"IP\",\n",
    "                \"index_type\": \"IVF_FLAT\",\n",
    "                \"params\": {\"nlist\": 1024}\n",
    "            }\n",
    "            collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "            print_status(\"Index Creation\", True, \"Created IVF_FLAT index\")\n",
    "            \n",
    "            collection.load()\n",
    "            print_status(\"Collection Load\", True, \"Loaded collection into memory\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print_status(\"Index Creation\", False, str(e))\n",
    "            raise\n",
    "\n",
    "    def reload_collection(self, collection_name: str = \"document_embeddings\"):\n",
    "        try:\n",
    "            if utility.has_collection(collection_name):\n",
    "                collection = Collection(name=collection_name)\n",
    "                collection.load()\n",
    "                print_status(\"Collection Reload\", True, f\"Reloaded collection: {collection_name}\")\n",
    "                return collection\n",
    "            else:\n",
    "                raise Exception(f\"Collection {collection_name} does not exist\")\n",
    "        except Exception as e:\n",
    "            print_status(\"Collection Reload\", False, str(e))\n",
    "            raise\n",
    "class EmbeddingGenerator:\n",
    "    def __init__(self, model_name: str = \"TurkuNLP/gpt3-finnish-large\"):\n",
    "        try:\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "            self.embedding_dim = self.model.config.hidden_size\n",
    "            print_status(\"Embedding Model\", True, f\"Loaded {model_name} (dim={self.embedding_dim})\")\n",
    "        except Exception as e:\n",
    "            print_status(\"Embedding Model\", False, str(e))\n",
    "            raise\n",
    "        \n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        \n",
    "    def generate(self, texts: List[str], batch_size: int = 8) -> np.ndarray:\n",
    "        try:\n",
    "            all_embeddings = []\n",
    "            \n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                batch_texts = texts[i:i + batch_size]\n",
    "                \n",
    "                encoded_input = self.tokenizer(\n",
    "                    batch_texts,\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=512,\n",
    "                    return_tensors='pt'\n",
    "                ).to(self.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    model_output = self.model(**encoded_input)\n",
    "                \n",
    "                sentence_embeddings = self.mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "                sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "                \n",
    "                all_embeddings.append(sentence_embeddings.cpu().numpy())\n",
    "            \n",
    "            result = np.concatenate(all_embeddings)\n",
    "            \n",
    "            if result.shape[1] != EMBEDDING_DIM:\n",
    "                raise ValueError(f\"Embedding dimension mismatch. Expected {EMBEDDING_DIM}, got {result.shape[1]}\")\n",
    "            \n",
    "            print_status(\"Embedding Generation\", True, \n",
    "                    f\"Generated {len(texts)} embeddings with dimension {result.shape[1]}\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print_status(\"Embedding Generation\", False, str(e))\n",
    "            raise\n",
    "\n",
    "    def get_embedding_dim(self) -> int:\n",
    "        return self.embedding_dim\n",
    "\n",
    "\n",
    "class RAGPipeline:\n",
    "    def __init__(self, model_id: str = \"Finnish-NLP/llama-7b-finnish-instruct-v0.2\"):\n",
    "        try:\n",
    "            self.setup_llm(model_id)\n",
    "            print_status(\"LLM Setup\", True, f\"Loaded {model_id}\")\n",
    "            \n",
    "            self.doc_processor = DocumentProcessor()\n",
    "            self.embedding_generator = EmbeddingGenerator()\n",
    "            \n",
    "            if self.embedding_generator.get_embedding_dim() != EMBEDDING_DIM:\n",
    "                raise ValueError(f\"Embedding dimension mismatch. Global: {EMBEDDING_DIM}, \" \n",
    "                               f\"Generator: {self.embedding_generator.get_embedding_dim()}\")\n",
    "            \n",
    "            self.milvus_manager = MilvusManager(\n",
    "                host=MILVUS_HOST,\n",
    "                port=MILVUS_PORT,\n",
    "                alias=MILVUS_ALIAS\n",
    "            )\n",
    "            self.collection = self.milvus_manager.create_collection()\n",
    "            print_status(\"RAG Pipeline\", True, \"All components initialized\")\n",
    "        except Exception as e:\n",
    "            print_status(\"RAG Pipeline\", False, str(e))\n",
    "            raise\n",
    "    def process_documents(self, folder_path: str):\n",
    "        \"\"\"Process all documents in the specified folder.\"\"\"\n",
    "        try:\n",
    "            # Get all .docx files in the folder\n",
    "            file_paths = [f for f in os.listdir(folder_path) if f.endswith('.docx')]\n",
    "            if not file_paths:\n",
    "                raise ValueError(f\"No .docx files found in {folder_path}\")\n",
    "                \n",
    "            print_status(\"Document Loading\", True, f\"Found {len(file_paths)} documents\")\n",
    "            all_chunks = []\n",
    "            \n",
    "            # Process each document\n",
    "            for file in tqdm(file_paths, desc=\"Processing documents\"):\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                chunks = self.doc_processor.process_document(file_path)\n",
    "                all_chunks.extend(chunks)\n",
    "                \n",
    "            # Generate embeddings for all chunks\n",
    "            texts = [chunk[\"text\"] for chunk in all_chunks]\n",
    "            embeddings = self.embedding_generator.generate(texts)\n",
    "            \n",
    "            # Prepare entities for Milvus\n",
    "            entities = []\n",
    "            for i, (text, embedding, chunk) in enumerate(zip(texts, embeddings, all_chunks)):\n",
    "                entity = {\n",
    "                    \"text\": text,\n",
    "                    \"embedding\": embedding.tolist(),\n",
    "                    \"person_name\": chunk[\"metadata\"][\"person_name\"],\n",
    "                    \"person_age\": chunk[\"metadata\"][\"person_age\"],\n",
    "                    \"document_id\": chunk[\"metadata\"][\"document_id\"],\n",
    "                    \"chunk_index\": chunk[\"metadata\"][\"chunk_index\"]\n",
    "                }\n",
    "                entities.append(entity)\n",
    "            \n",
    "            # Insert into Milvus in batches\n",
    "            batch_size = 100\n",
    "            for i in range(0, len(entities), batch_size):\n",
    "                batch = entities[i:i + batch_size]\n",
    "                self.collection.insert(batch)\n",
    "            \n",
    "            # Ensure data is persisted\n",
    "            self.collection.flush()\n",
    "            \n",
    "            # Create index and load collection\n",
    "            self.milvus_manager.create_and_load_index(self.collection)\n",
    "            \n",
    "            print_status(\"Document Processing\", True, \n",
    "                        f\"Processed {len(texts)} chunks from {len(file_paths)} documents\")\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print_status(\"Document Processing\", False, f\"Error: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "    def setup_llm(self, model_id: str):\n",
    "        try:\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\"\n",
    "            )\n",
    "            \n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                quantization_config=bnb_config,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                max_memory={0: \"6GiB\"},\n",
    "                offload_folder=\"offload\"\n",
    "            )\n",
    "            \n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "            self.pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                max_new_tokens=512,\n",
    "                do_sample=True,\n",
    "                temperature=0.3,\n",
    "                top_p=0.95,\n",
    "                repetition_penalty=1.15\n",
    "            )\n",
    "            print_status(\"LLM Pipeline\", True, \"Pipeline configured successfully\")\n",
    "        except Exception as e:\n",
    "            print_status(\"LLM Pipeline\", False, str(e))\n",
    "            raise\n",
    "\n",
    "    def query(self, question: str, top_k: int = 10):\n",
    "        try:\n",
    "            self.collection = self.milvus_manager.reload_collection()\n",
    "            \n",
    "            # Preprocess question\n",
    "            question = question.strip()\n",
    "            if not question.endswith('?'):\n",
    "                question += '?'\n",
    "            \n",
    "            # Generate question embedding and search\n",
    "            question_embedding = self.embedding_generator.generate([question])[0]\n",
    "            \n",
    "            search_params = {\n",
    "                \"metric_type\": \"IP\",\n",
    "                \"params\": {\"nprobe\": 50}\n",
    "            }\n",
    "            \n",
    "            # Get search results\n",
    "            search_results = self.collection.search(\n",
    "                data=[question_embedding.tolist()],\n",
    "                anns_field=\"embedding\",\n",
    "                param=search_params,\n",
    "                limit=top_k * 2,\n",
    "                output_fields=[\"text\", \"person_name\", \"document_id\", \"chunk_index\"]\n",
    "            )\n",
    "    \n",
    "            # Convert search results to a format we can work with\n",
    "            initial_results = []\n",
    "            for hit in search_results[0]:  # Fix: Properly handle Milvus Hit objects\n",
    "                initial_results.append({\n",
    "                    'text': hit.entity.get('text'),\n",
    "                    'person_name': hit.entity.get('person_name'),\n",
    "                    'document_id': hit.entity.get('document_id'),\n",
    "                    'chunk_index': hit.entity.get('chunk_index'),\n",
    "                    'score': float(hit.score)\n",
    "                })\n",
    "            \n",
    "            # Rerank results\n",
    "            reranked_results = self._rerank_results(question, initial_results)\n",
    "            \n",
    "            # Take top_k after reranking\n",
    "            results = reranked_results[:top_k]\n",
    "            \n",
    "            context_parts = []\n",
    "            for i, hit in enumerate(results):\n",
    "                context_parts.append(\n",
    "                    f\"[Dokumentti {i+1}]\\n\"\n",
    "                    f\"Lähde: {hit['document_id']}\\n\"\n",
    "                    f\"Henkilö: {hit['person_name']}\\n\"\n",
    "                    f\"Luotettavuus: {hit['score']:.2%}\\n\"\n",
    "                    f\"Tekstikatkelma:\\n{hit['text']}\\n\"\n",
    "                    f\"{'-' * 40}\\n\"\n",
    "                )\n",
    "            \n",
    "            context = \"\\n\".join(context_parts)\n",
    "            \n",
    "            prompt = f\"\"\"Tehtävä: Etsi tarkka vastaus annettuun kysymykseen käyttäen vain alla olevaa kontekstia.\n",
    "    \n",
    "            Kysymys: {question}\n",
    "            \n",
    "            Konteksti:\n",
    "            {context}\n",
    "            \n",
    "            Tärkeät ohjeet:\n",
    "            1. Jos löydät suoran vastauksen:\n",
    "               - Mainitse AINA ensin dokumentti, josta vastaus löytyy (esim. \"Dokumentti 1:\")\n",
    "               - Lainaa TARKASTI alkuperäistä tekstiä käyttäen lainausmerkkejä\n",
    "               - Perustele vastauksen luotettavuus yhdellä lyhyellä lauseella\n",
    "            \n",
    "            2. Jos löydät vain osittaisen vastauksen:\n",
    "               - Kerro selkeästi mikä osa vastauksesta löytyi ja mikä puuttuu\n",
    "               - Käytä silti suoria lainauksia löytyneestä osasta\n",
    "            \n",
    "            3. Jos et löydä minkäänlaista vastausta:\n",
    "               - Vastaa vain: \"En löydä suoraa vastausta annetusta kontekstista\"\n",
    "            \n",
    "            Vastaus:\"\"\"\n",
    "            \n",
    "            response = self.pipeline(\n",
    "                prompt,\n",
    "                max_new_tokens=300,\n",
    "                do_sample=True,\n",
    "                temperature=0.1,\n",
    "                top_p=0.85,\n",
    "                repetition_penalty=1.2\n",
    "            )[0][\"generated_text\"]\n",
    "    \n",
    "            response = self._clean_response(response)\n",
    "            \n",
    "            return {\n",
    "                \"answer\": response,\n",
    "                \"sources\": results,\n",
    "                \"metadata\": {\n",
    "                    \"question\": question,\n",
    "                    \"num_chunks_retrieved\": len(results),\n",
    "                    \"max_similarity_score\": max(hit['score'] for hit in results)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print_status(\"Query\", False, str(e))\n",
    "            raise\n",
    "    \n",
    "    def _rerank_results(self, question: str, results: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Rerank results using both semantic similarity and context relevance.\"\"\"\n",
    "        reranked = []\n",
    "        question_embedding = self.embedding_generator.generate([question])[0]\n",
    "        \n",
    "        for hit in results:\n",
    "            text = hit['text']\n",
    "            score = hit['score']\n",
    "            \n",
    "            # Compute semantic similarity\n",
    "            chunk_embedding = self.embedding_generator.generate([text])[0]\n",
    "            semantic_similarity = np.dot(question_embedding, chunk_embedding)\n",
    "            \n",
    "            # Adjust score based on semantic similarity\n",
    "            score *= (1 + semantic_similarity)\n",
    "            \n",
    "            # Further adjustments based on context relevance\n",
    "            if '?' in text:\n",
    "                score *= 1.05  # Slight boost if chunk contains a question\n",
    "            if any(entity in text.lower() for entity in self._extract_entities(question)):\n",
    "                score *= 1.1  # Boost if entities match\n",
    "            \n",
    "            hit['score'] = score\n",
    "            reranked.append(hit)\n",
    "        \n",
    "        # Sort the reranked results\n",
    "        reranked.sort(key=lambda x: x['score'], reverse=True)\n",
    "        return reranked\n",
    "    \n",
    "    def _extract_entities(self, text: str) -> List[str]:\n",
    "        \"\"\"Simple entity extraction based on capitalized words.\"\"\"\n",
    "        return re.findall(r'\\b[A-Z][a-zäöå]*\\b', text)\n",
    "\n",
    "    def _clean_response(self, response: str) -> str:\n",
    "        response = re.sub(r'(\\[Dokumentti \\d+\\])\\s*\\1', r'\\1', response)\n",
    "        response = re.sub(r'Luottamus: \\d+%\\s*Selitys:', '', response)\n",
    "        if len(response) > 500:\n",
    "            response = response[:497] + '...'\n",
    "        return response.strip()\n",
    "\n",
    "    def _create_prompt(self, question: str, context: str) -> str:\n",
    "        return f\"\"\"Tehtävä: Vastaa annettuun kysymykseen käyttäen vain alla olevaa kontekstia.\n",
    "\n",
    "        Kysymys: {question}\n",
    "        \n",
    "        Konteksti:\n",
    "        {context}\n",
    "        \n",
    "        Vastausohjeet:\n",
    "        1. Jos löydät suoran vastauksen:\n",
    "           - Mainitse ensin dokumentti, josta vastaus löytyy\n",
    "           - Lainaa tekstiä tarkasti käyttäen lainausmerkkejä\n",
    "           - Mainitse vastauksen luotettavuus prosentteina\n",
    "        \n",
    "        2. Jos löydät osittaisen vastauksen:\n",
    "           - Kerro, mitä tietoa löysit ja mistä\n",
    "           - Mainitse selkeästi, mitä tietoa puuttuu\n",
    "        \n",
    "        3. Jos et löydä vastausta:\n",
    "           - Vastaa vain: \"En löydä suoraa vastausta annetusta kontekstista\"\n",
    "        \n",
    "        Vastaus:\"\"\"\n",
    "\n",
    "## validating \n",
    "def verify_pipeline_components():\n",
    "    doc_processor = DocumentProcessor()\n",
    "    milvus_manager = MilvusManager()\n",
    "    embedding_generator = EmbeddingGenerator()\n",
    "    pipeline = RAGPipeline()\n",
    "    return all([doc_processor, milvus_manager, embedding_generator, pipeline])\n",
    "\n",
    "verify_section(\"Pipeline Components\", verify_pipeline_components)\n",
    "\n",
    "class Neo4jDocumentManager:\n",
    "    def __init__(self, uri: str = \"neo4j://neo4j:7687\", \n",
    "                 username: str = \"neo4j\", \n",
    "                 password: str = \"test\"):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "        self.setup_constraints()\n",
    "        self.person_names = set()  # Store discovered person names\n",
    "        \n",
    "    def _extract_person_names(self, text: str) -> set:\n",
    "        \"\"\"Extract potential person names from text using Finnish patterns.\"\"\"\n",
    "        names = set()\n",
    "        \n",
    "        # Finnish name patterns\n",
    "        patterns = [\n",
    "            r'\\b[A-ZÄÖÅ][a-zäöå]+\\b',  # Basic capitalized word\n",
    "            r'(?:herra|rouva|neiti)\\s+[A-ZÄÖÅ][a-zäöå]+',  # Titles\n",
    "            r'(?:ystävä|naapuri)\\s+[A-ZÄÖÅ][a-zäöå]+',  # Relationships\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            matches = re.finditer(pattern, text)\n",
    "            for match in matches:\n",
    "                name = match.group().split()[-1]  # Get the actual name part\n",
    "                if len(name) > 2:  # Filter out very short words\n",
    "                    names.add(name)\n",
    "        \n",
    "        return names\n",
    "\n",
    "    def create_document_graph(self, processed_chunks: List[Dict]):\n",
    "        \"\"\"Create graph structure from processed document chunks.\"\"\"\n",
    "        # First pass: collect all person names\n",
    "        for chunk in processed_chunks:\n",
    "            # Add names from metadata\n",
    "            if chunk[\"metadata\"].get(\"person_name\"):\n",
    "                self.person_names.add(chunk[\"metadata\"][\"person_name\"])\n",
    "            \n",
    "            # Add names found in content\n",
    "            self.person_names.update(self._extract_person_names(chunk[\"text\"]))\n",
    "        \n",
    "        with self.driver.session() as session:\n",
    "            for chunk in processed_chunks:\n",
    "                # Create Document and Person nodes\n",
    "                self._create_document_and_person_nodes(session, chunk)\n",
    "                \n",
    "                # Extract and create relationships\n",
    "                self._process_relationships(session, chunk)\n",
    "    \n",
    "    def _create_document_and_person_nodes(self, session, chunk: Dict):\n",
    "        \"\"\"Create document and person nodes with relationships.\"\"\"\n",
    "        session.run(\"\"\"\n",
    "            MERGE (p:Person {name: $person_name})\n",
    "            SET p.age = $age\n",
    "            \n",
    "            MERGE (d:Document {id: $doc_id})\n",
    "            SET d.content = $content,\n",
    "                d.chunk_index = $chunk_index\n",
    "            \n",
    "            MERGE (p)-[:APPEARS_IN]->(d)\n",
    "            \n",
    "            WITH p, d\n",
    "            MATCH (p)-[:APPEARS_IN]->(other:Document)\n",
    "            WHERE other.id <> d.id\n",
    "            MERGE (d)-[:RELATED_TO]->(other)\n",
    "        \"\"\", {\n",
    "            \"person_name\": chunk[\"metadata\"][\"person_name\"],\n",
    "            \"age\": chunk[\"metadata\"][\"person_age\"],\n",
    "            \"doc_id\": f\"{chunk['metadata']['document_id']}_{chunk['metadata']['chunk_index']}\",\n",
    "            \"content\": chunk[\"text\"],\n",
    "            \"chunk_index\": chunk[\"metadata\"][\"chunk_index\"]\n",
    "        })\n",
    "\n",
    "    def find_mentioned_persons(self, text: str) -> List[str]:\n",
    "        \"\"\"Find persons mentioned in text from known person names.\"\"\"\n",
    "        mentioned_persons = []\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Look for known person names in the text\n",
    "        for name in self.person_names:\n",
    "            if name.lower() in text_lower:\n",
    "                mentioned_persons.append(name)\n",
    "                \n",
    "        return mentioned_persons\n",
    "\n",
    "    def query(self, question: str) -> Dict:\n",
    "        \"\"\"Enhanced query processing with dynamic person detection.\"\"\"\n",
    "        # Find mentioned persons in the question\n",
    "        mentioned_persons = self.find_mentioned_persons(question)\n",
    "        \n",
    "        with self.driver.session() as session:\n",
    "            # Get person contexts for all mentioned persons\n",
    "            person_contexts = []\n",
    "            for person_name in mentioned_persons:\n",
    "                context = self._get_person_context(session, person_name)\n",
    "                if context:\n",
    "                    person_contexts.append(context)\n",
    "            \n",
    "            # Get related documents\n",
    "            documents = self._get_related_documents(session, mentioned_persons)\n",
    "            \n",
    "            return {\n",
    "                \"person_contexts\": person_contexts,\n",
    "                \"related_documents\": documents,\n",
    "                \"mentioned_persons\": mentioned_persons\n",
    "            }\n",
    "    \n",
    "    def _get_person_context(self, session, person_name: str) -> Dict:\n",
    "        \"\"\"Get comprehensive context for a person.\"\"\"\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (p:Person {name: $name})\n",
    "            OPTIONAL MATCH (p)-[:APPEARS_IN]->(d:Document)\n",
    "            OPTIONAL MATCH (d)-[:HAS_RELATIONSHIP]->(r:Relationship)\n",
    "            \n",
    "            WITH p,\n",
    "                 COLLECT(DISTINCT r.type) as relationship_types,\n",
    "                 COLLECT(DISTINCT d.content) as contents,\n",
    "                 COLLECT(DISTINCT d.id) as doc_ids\n",
    "            \n",
    "            RETURN {\n",
    "                name: p.name,\n",
    "                age: p.age,\n",
    "                relationship_types: relationship_types,\n",
    "                document_count: SIZE(contents),\n",
    "                document_ids: doc_ids\n",
    "            } as context\n",
    "        \"\"\", {\"name\": person_name})\n",
    "        \n",
    "        return result.single()[\"context\"] if result.peek() else None\n",
    "\n",
    "class Neo4jEnhancedRAGPipeline:\n",
    "    def __init__(self, base_rag_pipeline, neo4j_uri: str = \"neo4j://neo4j:7687\"):\n",
    "        self.base_rag = base_rag_pipeline\n",
    "        self.neo4j_manager = Neo4jDocumentManager(\n",
    "            uri=neo4j_uri,\n",
    "            username=\"neo4j\",\n",
    "            password=\"test\"\n",
    "        )\n",
    "        \n",
    "    def query(self, question: str) -> Dict:\n",
    "        \"\"\"Enhanced query processing with dynamic person detection.\"\"\"\n",
    "        # Get initial results from base RAG\n",
    "        base_results = self.base_rag.query(question)\n",
    "        \n",
    "        # Get Neo4j enhanced results with dynamic person detection\n",
    "        neo4j_results = self.neo4j_manager.query(question)\n",
    "        \n",
    "        # Combine contexts\n",
    "        enhanced_context = self._combine_contexts(\n",
    "            base_results[\"sources\"],\n",
    "            neo4j_results[\"related_documents\"],\n",
    "            neo4j_results[\"person_contexts\"]\n",
    "        )\n",
    "        \n",
    "        # Generate final answer\n",
    "        final_answer = self._generate_enhanced_answer(\n",
    "            question,\n",
    "            enhanced_context\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"answer\": final_answer,\n",
    "            \"sources\": base_results[\"sources\"],\n",
    "            \"graph_enhanced_sources\": neo4j_results[\"related_documents\"],\n",
    "            \"person_contexts\": neo4j_results[\"person_contexts\"],\n",
    "            \"mentioned_persons\": neo4j_results[\"mentioned_persons\"]\n",
    "        }\n",
    "\n",
    "# Testing Infrastructure\n",
    "class RAGTester:\n",
    "    def __init__(self, pipeline: RAGPipeline):\n",
    "        self.pipeline = pipeline\n",
    "        self.test_results = []\n",
    "        \n",
    "    def run_test_suite(self, test_questions: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Run comprehensive tests on the RAG pipeline.\"\"\"\n",
    "        test_results = []\n",
    "        summary_stats = {\n",
    "            \"total_questions\": len(test_questions),\n",
    "            \"successful_responses\": 0,\n",
    "            \"failed_responses\": 0,\n",
    "            \"average_similarity\": 0.0,\n",
    "            \"direct_quote_ratio\": 0.0\n",
    "        }\n",
    "        \n",
    "        for question in test_questions:\n",
    "            try:\n",
    "                # Get response from pipeline\n",
    "                result = self.pipeline.query(question)\n",
    "                \n",
    "                # Validate response\n",
    "                validation = self._validate_response(result[\"answer\"])\n",
    "                \n",
    "                # Calculate response metrics\n",
    "                metrics = {\n",
    "                    \"question\": question,\n",
    "                    \"has_direct_quote\": validation[\"has_direct_quote\"],\n",
    "                    \"source_count\": len(result[\"sources\"]),\n",
    "                    \"max_similarity\": max(s[\"similarity_score\"] for s in result[\"sources\"]),\n",
    "                    \"response_quality\": validation,\n",
    "                    \"response_length\": len(result[\"answer\"]),\n",
    "                }\n",
    "                \n",
    "                test_results.append(metrics)\n",
    "                summary_stats[\"successful_responses\"] += 1\n",
    "                summary_stats[\"average_similarity\"] += metrics[\"max_similarity\"]\n",
    "                summary_stats[\"direct_quote_ratio\"] += int(metrics[\"has_direct_quote\"])\n",
    "                \n",
    "                # Print detailed results\n",
    "                self._print_test_result(question, result, metrics)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error testing question '{question}': {str(e)}\")\n",
    "                summary_stats[\"failed_responses\"] += 1\n",
    "                \n",
    "        # Calculate final statistics\n",
    "        if summary_stats[\"successful_responses\"] > 0:\n",
    "            summary_stats[\"average_similarity\"] /= summary_stats[\"successful_responses\"]\n",
    "            summary_stats[\"direct_quote_ratio\"] /= summary_stats[\"successful_responses\"]\n",
    "        \n",
    "        return {\n",
    "            \"detailed_results\": test_results,\n",
    "            \"summary\": summary_stats\n",
    "        }\n",
    "    \n",
    "    def _validate_response(self, response: str) -> Dict[str, bool]:\n",
    "        \"\"\"Validate Finnish language response with detailed checks.\"\"\"\n",
    "        validation = {\n",
    "            # Basic structural checks\n",
    "            \"has_source_reference\": bool(re.search(r'\\[Dokumentti \\d+\\]', response)),\n",
    "            \"has_direct_quote\": '\"' in response,\n",
    "            \"is_complete_sentence\": response.strip().endswith(('.', '?', '!')),\n",
    "            \"has_confidence\": bool(re.search(r'\\d+\\s*%', response)),\n",
    "            \"reasonable_length\": 10 <= len(response) <= 500,\n",
    "            \n",
    "            # Finnish language specific checks\n",
    "            \"has_finnish_chars\": bool(re.search(r'[äöåÄÖÅ]', response)),\n",
    "            \"proper_finnish_structure\": self._check_finnish_structure(response)\n",
    "        }\n",
    "        return validation\n",
    "    \n",
    "    def _check_finnish_structure(self, text: str) -> bool:\n",
    "        \"\"\"Check if the response follows typical Finnish sentence structure.\"\"\"\n",
    "        finnish_endings = [\n",
    "            'ssa', 'ssä', 'sta', 'stä', 'lla', 'llä', 'lta', 'ltä',\n",
    "            'ksi', 'in', 'en', 'teen', 'seen'\n",
    "        ]\n",
    "        words = text.lower().split()\n",
    "        if not words:\n",
    "            return False\n",
    "            \n",
    "        has_finnish_ending = any(\n",
    "            any(word.endswith(ending) for ending in finnish_endings)\n",
    "            for word in words\n",
    "        )\n",
    "        \n",
    "        return has_finnish_ending\n",
    "    \n",
    "    def _print_test_result(self, question: str, result: Dict, metrics: Dict):\n",
    "        \"\"\"Print formatted test results.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Answer: {result['answer']}\")\n",
    "        print(\"\\nMetrics:\")\n",
    "        print(f\"- Source count: {metrics['source_count']}\")\n",
    "        print(f\"- Max similarity: {metrics['max_similarity']:.2%}\")\n",
    "        print(f\"- Response length: {metrics['response_length']}\")\n",
    "        print(\"\\nValidation:\")\n",
    "        for key, value in metrics['response_quality'].items():\n",
    "            print(f\"- {key}: {'✅' if value else '❌'}\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "\n",
    "def run_rag_tests(pipeline: RAGPipeline, test_questions: List[str] = None):\n",
    "    \"\"\"Execute RAG tests with default or custom test questions.\"\"\"\n",
    "    if test_questions is None:\n",
    "        test_questions = [\n",
    "            \"Onko Marjatta Eilan ystävä?\",\n",
    "            \"Miten Sulo kokee sosiaalisen kanssakäymisen merkityksen?\",\n",
    "            \"Montako sisarusta Sulolla on?\",\n",
    "            \"Millainen on Eilan arki?\",\n",
    "            \"Mikä on Sulolle tärkeää?\"\n",
    "        ]\n",
    "    \n",
    "    tester = RAGTester(pipeline)\n",
    "    results = tester.run_test_suite(test_questions)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nTest Summary:\")\n",
    "    print(f\"Total questions: {results['summary']['total_questions']}\")\n",
    "    print(f\"Successful responses: {results['summary']['successful_responses']}\")\n",
    "    print(f\"Failed responses: {results['summary']['failed_responses']}\")\n",
    "    print(f\"Average similarity: {results['summary']['average_similarity']:.2%}\")\n",
    "    print(f\"Direct quote ratio: {results['summary']['direct_quote_ratio']:.2%}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def verify_testing_infrastructure():\n",
    "    pipeline = RAGPipeline()\n",
    "    tester = RAGTester(pipeline)\n",
    "    return bool(tester)\n",
    "\n",
    "verify_section(\"Testing Infrastructure\", verify_testing_infrastructure)\n",
    "\n",
    "# Add these imports at the top of your file\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from typing import List, Dict, Any, Optional\n",
    "import re\n",
    "\n",
    "class FinnishRAGAgent:\n",
    "    def __init__(self, base_pipeline: RAGPipeline):\n",
    "        self.pipeline = base_pipeline\n",
    "        self.memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "        self.setup_tools()\n",
    "        \n",
    "    def setup_tools(self):\n",
    "        \"\"\"Initialize search and analysis tools.\"\"\"\n",
    "        self.tools = {\n",
    "            \"semantic_search\": self._semantic_search,\n",
    "            \"exact_match\": self._exact_match_search,\n",
    "            \"context_analysis\": self._analyze_context\n",
    "        }\n",
    "    \n",
    "    def _semantic_search(self, query: str) -> List[Dict]:\n",
    "        \"\"\"Enhanced semantic search with Finnish preprocessing.\"\"\"\n",
    "        preprocessed_query = self._preprocess_finnish_text(query)\n",
    "        base_results = self.pipeline.query(preprocessed_query, top_k=5)\n",
    "        \n",
    "        # Enhance results with Finnish-specific scoring\n",
    "        enhanced_results = []\n",
    "        for result in base_results['sources']:\n",
    "            score = self._calculate_finnish_relevance(preprocessed_query, result['text'])\n",
    "            enhanced_results.append({\n",
    "                'text': result['text'],\n",
    "                'score': score * result['score'],  # Combine scores\n",
    "                'source': result['document_id'],\n",
    "                'metadata': {\n",
    "                    'person_name': result['person_name'],\n",
    "                    'chunk_index': result['chunk_index']\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        return sorted(enhanced_results, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "    def _exact_match_search(self, query: str) -> List[Dict]:\n",
    "        \"\"\"Direct text matching with Finnish normalization.\"\"\"\n",
    "        normalized_query = self._normalize_finnish_text(query)\n",
    "        \n",
    "        # Use pipeline's collection directly\n",
    "        collection = self.pipeline.collection\n",
    "        results = collection.query(\n",
    "            expr=f'text like \"%{normalized_query}%\"',\n",
    "            output_fields=[\"text\", \"person_name\", \"document_id\", \"chunk_index\"]\n",
    "        )\n",
    "        \n",
    "        return [\n",
    "            {\n",
    "                'text': r['text'],\n",
    "                'score': 1.0, \n",
    "                'source': r['document_id'],\n",
    "                'metadata': {'person_name': r['person_name']}\n",
    "            }\n",
    "            for r in results\n",
    "        ]\n",
    "\n",
    "    def _analyze_context(self, passages: List[Dict]) -> Dict:\n",
    "        \"\"\"Analyze relationships and context in Finnish text.\"\"\"\n",
    "        context_data = {\n",
    "            'entities': set(),\n",
    "            'relationships': [],\n",
    "            'temporal_refs': [],\n",
    "            'key_topics': set()\n",
    "        }\n",
    "        \n",
    "        for passage in passages:\n",
    "            text = passage['text']\n",
    "            \n",
    "            # Extract Finnish names and entities\n",
    "            entities = self._extract_finnish_entities(text)\n",
    "            context_data['entities'].update(entities)\n",
    "            \n",
    "            # Find relationships\n",
    "            relationships = self._find_relationships(text)\n",
    "            context_data['relationships'].extend(relationships)\n",
    "            \n",
    "            # Extract temporal information\n",
    "            temporal = self._extract_temporal_refs(text)\n",
    "            context_data['temporal_refs'].extend(temporal)\n",
    "            \n",
    "        return context_data\n",
    "\n",
    "    def _preprocess_finnish_text(self, text: str) -> str:\n",
    "        \"\"\"Preprocess Finnish text for better matching.\"\"\"\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text.strip())\n",
    "        \n",
    "        # Handle common Finnish abbreviations\n",
    "        abbreviations = {\n",
    "            'esim.': 'esimerkiksi',\n",
    "            'ns.': 'niin sanottu',\n",
    "            'jne.': 'ja niin edelleen'\n",
    "        }\n",
    "        for abbr, full in abbreviations.items():\n",
    "            text = text.replace(abbr, full)\n",
    "        \n",
    "        return text\n",
    "\n",
    "    # Add these methods to your FinnishRAGAgent class\n",
    "\n",
    "    def _find_relationships(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Find relationships in Finnish text.\"\"\"\n",
    "        relationships = []\n",
    "        \n",
    "        # Common Finnish relationship patterns\n",
    "        patterns = [\n",
    "            (r'(\\w+)\\s+on\\s+(\\w+)\\s+ystävä', 'ystävyys'),\n",
    "            (r'(\\w+)\\s+asuu\\s+(\\w+)', 'asuminen'),\n",
    "            (r'(\\w+)\\s+tekee\\s+(\\w+)', 'toiminta'),\n",
    "            (r'(\\w+)\\s+pitää\\s+(\\w+)', 'pitäminen'),\n",
    "            (r'(\\w+)\\s+kanssa', 'yhteys'),\n",
    "            (r'(\\w+)\\s+tärkeä', 'tärkeys'),\n",
    "            (r'(\\w+)\\s+auttaa\\s+(\\w+)', 'auttaminen')\n",
    "        ]\n",
    "        \n",
    "        for pattern, rel_type in patterns:\n",
    "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                relationships.append({\n",
    "                    'type': rel_type,\n",
    "                    'entities': match.groups(),\n",
    "                    'text': match.group(0)\n",
    "                })\n",
    "        \n",
    "        return relationships\n",
    "\n",
    "    def _extract_temporal_refs(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Extract temporal references from Finnish text.\"\"\"\n",
    "        temporal_refs = []\n",
    "        \n",
    "        # Common Finnish temporal patterns\n",
    "        patterns = [\n",
    "            (r'\\d+\\s*vuotta', 'duration'),\n",
    "            (r'\\d+\\s*vuotias', 'age'),\n",
    "            (r'(maanantai|tiistai|keskiviikko|torstai|perjantai|lauantai|sunnuntai)', 'weekday'),\n",
    "            (r'(tammikuu|helmikuu|maaliskuu|huhtikuu|toukokuu|kesäkuu|heinäkuu|elokuu|syyskuu|lokakuu|marraskuu|joulukuu)', 'month'),\n",
    "            (r'(aamu|päivä|ilta|yö)', 'time_of_day'),\n",
    "            (r'(eilen|tänään|huomenna)', 'relative_day'),\n",
    "            (r'(viikko|kuukausi|vuosi)', 'time_unit')\n",
    "        ]\n",
    "        \n",
    "        for pattern, ref_type in patterns:\n",
    "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                temporal_refs.append({\n",
    "                    'type': ref_type,\n",
    "                    'text': match.group(0),\n",
    "                    'position': match.span()\n",
    "                })\n",
    "        \n",
    "        return temporal_refs\n",
    "    \n",
    "    def _identify_key_topics(self, text: str) -> set:\n",
    "        \"\"\"Identify key topics in Finnish text.\"\"\"\n",
    "        topics = set()\n",
    "        \n",
    "        # Common Finnish topic indicators\n",
    "        key_patterns = [\n",
    "            (r'tärkeä\\w*\\s+(\\w+)', 'importance'),\n",
    "            (r'harrastaa\\w*\\s+(\\w+)', 'hobby'),\n",
    "            (r'pitää\\w*\\s+(\\w+)', 'preference'),\n",
    "            (r'ongelma\\w*\\s+(\\w+)', 'problem'),\n",
    "            (r'tavoite\\w*\\s+(\\w+)', 'goal')\n",
    "        ]\n",
    "        \n",
    "        for pattern, topic_type in key_patterns:\n",
    "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                if len(match.groups()) > 0:\n",
    "                    topics.add(f\"{topic_type}:{match.group(1)}\")\n",
    "        \n",
    "        return topics\n",
    "\n",
    "    def _normalize_finnish_text(self, text: str) -> str:\n",
    "        \"\"\"Normalize Finnish text for comparison.\"\"\"\n",
    "        text = text.lower().strip()\n",
    "        text = re.sub(r'[^\\w\\s\\äöåÄÖÅ]', '', text)\n",
    "        return text\n",
    "\n",
    "    def _calculate_finnish_relevance(self, query: str, text: str) -> float:\n",
    "        \"\"\"Calculate relevance score for Finnish text.\"\"\"\n",
    "        score = 1.0\n",
    "        \n",
    "        # Boost score for Finnish grammar patterns\n",
    "        if re.search(r'\\b(ssa|ssä|sta|stä|lla|llä|lta|ltä)\\b', text):\n",
    "            score *= 1.1\n",
    "            \n",
    "        # Boost for question-answer pairs\n",
    "        if '?' in query and '.' in text[:100]:\n",
    "            score *= 1.2\n",
    "            \n",
    "        # Check for named entity matches\n",
    "        query_entities = self._extract_finnish_entities(query)\n",
    "        text_entities = self._extract_finnish_entities(text)\n",
    "        if query_entities & text_entities:\n",
    "            score *= 1.3\n",
    "            \n",
    "        return min(1.0, score)\n",
    "\n",
    "    def _extract_finnish_entities(self, text: str) -> set:\n",
    "        \"\"\"Extract Finnish named entities.\"\"\"\n",
    "        entities = set()\n",
    "        \n",
    "        # Match Finnish names (simplified)\n",
    "        name_pattern = r'\\b[A-ZÄÖÅ][a-zäöå]+\\b'\n",
    "        entities.update(re.findall(name_pattern, text))\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def _generate_response(self, query: str, results: List[Dict], context: Dict) -> Dict:\n",
    "        \"\"\"Generate an enhanced response with proper citations.\"\"\"\n",
    "        if not results:\n",
    "            return {\n",
    "                \"answer\": \"En löytänyt vastausta kysymykseesi saatavilla olevista dokumenteista.\",\n",
    "                \"confidence\": 0.0,\n",
    "                \"sources\": []\n",
    "            }\n",
    "    \n",
    "        try:\n",
    "            # Build context-rich query\n",
    "            enhanced_query = self._build_enhanced_query(query, results)\n",
    "            \n",
    "            # Get response from base pipeline\n",
    "            response = self.pipeline.query(enhanced_query)\n",
    "            \n",
    "            # Process and validate response\n",
    "            answer = self._process_response(response, results, query)\n",
    "            \n",
    "            return {\n",
    "                \"answer\": answer,\n",
    "                \"confidence\": max(r['score'] for r in results),\n",
    "                \"sources\": results[:3],\n",
    "                \"context_analysis\": context\n",
    "            }\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in response generation: {str(e)}\")\n",
    "            return {\n",
    "                \"answer\": \"Virhe vastauksen muodostamisessa.\",\n",
    "                \"confidence\": 0.0,\n",
    "                \"sources\": results[:3],\n",
    "                \"context_analysis\": context\n",
    "            }\n",
    "\n",
    "    def _build_enhanced_query(self, query: str, results: List[Dict]) -> str:\n",
    "        \"\"\"Build a context-rich query with relevant information.\"\"\"\n",
    "        # Analyze query intent\n",
    "        query_type = self._analyze_query_type(query)\n",
    "        \n",
    "        # Build appropriate context structure based on query type\n",
    "        context = f\"\"\"Etsi tarkka vastaus seuraavaan kysymykseen käyttäen alla olevia dokumentteja.\n",
    "    \n",
    "    Kysymys: {query}\n",
    "    \n",
    "    Dokumentit:\n",
    "    \"\"\"\n",
    "        \n",
    "        # Add relevant documents with source information\n",
    "        for i, result in enumerate(results[:3], 1):\n",
    "            context += f\"\\nDokumentti {i} ({result['source']}):\\n{result['text'].strip()}\\n\"\n",
    "    \n",
    "        # Add query-specific instructions\n",
    "        context += self._get_query_instructions(query_type)\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def _analyze_query_type(self, query: str) -> str:\n",
    "        \"\"\"Analyze the type of query for better response structuring.\"\"\"\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        # Extract query characteristics without hardcoding specific names or values\n",
    "        characteristics = {\n",
    "            'personal_info': any(word in query_lower for word in ['kuka', 'kenen', 'kenelle']),\n",
    "            'age_related': any(word in query_lower for word in ['vanha', 'ikä', 'syntymä', 'vuosi']),\n",
    "            'activity_related': any(word in query_lower for word in ['tekee', 'harrastaa', 'pitää', 'tykkää']),\n",
    "            'ability_related': any(word in query_lower for word in ['pystyy', 'osaa', 'liikkuu', 'käyttää']),\n",
    "            'preference_related': any(word in query_lower for word in ['pitää', 'tykkää', 'haluaa']),\n",
    "            'relationship_related': any(word in query_lower for word in ['ystävä', 'tärkeä', 'läheinen'])\n",
    "        }\n",
    "        \n",
    "        # Return the most likely query type\n",
    "        return max(characteristics.items(), key=lambda x: x[1])[0]\n",
    "    \n",
    "    def _get_query_instructions(self, query_type: str) -> str:\n",
    "        \"\"\"Get specific instructions based on query type.\"\"\"\n",
    "        instructions = {\n",
    "            'personal_info': \"\"\"\n",
    "        Ohje: \n",
    "        1. Etsi henkilöön liittyvät suorat maininnat\n",
    "        2. Käytä suoria lainauksia henkilöiden nimistä ja suhteista\n",
    "        3. Mainitse dokumentin lähde\"\"\",\n",
    "                \n",
    "                'age_related': \"\"\"\n",
    "        Ohje:\n",
    "        1. Etsi tarkat ikä- ja syntymävuositiedot\n",
    "        2. Ilmoita sekä ikä että syntymävuosi jos molemmat löytyvät\n",
    "        3. Mainitse dokumentin lähde\"\"\",\n",
    "                \n",
    "                'activity_related': \"\"\"\n",
    "        Ohje:\n",
    "        1. Etsi kaikki mainitut aktiviteetit ja harrastukset\n",
    "        2. Käytä suoria lainauksia aktiviteettien kuvauksista\n",
    "        3. Mainitse dokumentin lähde\"\"\",\n",
    "                \n",
    "                'ability_related': \"\"\"\n",
    "        Ohje:\n",
    "        1. Etsi kuvaukset henkilön kyvyistä ja toiminnasta\n",
    "        2. Käytä suoria lainauksia toimintakyvyn kuvauksista\n",
    "        3. Mainitse dokumentin lähde\"\"\",\n",
    "                \n",
    "                'preference_related': \"\"\"\n",
    "        Ohje:\n",
    "        1. Etsi kaikki mainitut mieltymykset ja kiinnostukset\n",
    "        2. Käytä suoria lainauksia mieltymysten kuvauksista\n",
    "        3. Mainitse dokumentin lähde\"\"\",\n",
    "                \n",
    "                'relationship_related': \"\"\"\n",
    "        Ohje:\n",
    "        1. Etsi kuvaukset ihmissuhteista ja tärkeistä henkilöistä\n",
    "        2. Käytä suoria lainauksia suhteiden kuvauksista\n",
    "        3. Mainitse dokumentin lähde\"\"\"\n",
    "            }\n",
    "            \n",
    "        return instructions.get(query_type, \"\"\"\n",
    "        Ohje:\n",
    "        1. Etsi suora vastaus kysymykseen dokumenteista\n",
    "        2. Käytä suoria lainauksia\n",
    "        3. Mainitse dokumentin lähde\"\"\")\n",
    "    \n",
    "    def _process_response(self, response: Dict, results: List[Dict], query: str) -> str:\n",
    "        \"\"\"Process and validate the response.\"\"\"\n",
    "        if not response or 'answer' not in response:\n",
    "            return \"En löytänyt vastausta annetusta kontekstista.\"\n",
    "        \n",
    "        answer = response['answer'].strip()\n",
    "        \n",
    "        # Validate the answer has proper citations\n",
    "        if not any(f\"Dokumentin {result['source']}\" in answer for result in results):\n",
    "            # Try to add source information if missing\n",
    "            for result in results:\n",
    "                if any(quote in result['text'] for quote in re.findall(r'\"([^\"]*)\"', answer)):\n",
    "                    answer = f\"Dokumentin {result['source']} mukaan {answer}\"\n",
    "                    break\n",
    "        \n",
    "        # Validate answer has quotes\n",
    "        if '\"' not in answer and any(result['text'] in answer for result in results):\n",
    "            answer = re.sub(r'(Dokumentin [^\\s]+ mukaan) (.*)', r'\\1 \"\\2\"', answer)\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def process_query(self, query: str) -> Dict:\n",
    "        \"\"\"Process a query using all available tools.\"\"\"\n",
    "        try:\n",
    "            # Step 1: Get semantic search results\n",
    "            semantic_results = self._semantic_search(query)\n",
    "            \n",
    "            # Step 2: Check for exact matches if needed\n",
    "            if not semantic_results or max(r['score'] for r in semantic_results) < 0.5:\n",
    "                exact_matches = self._exact_match_search(query)\n",
    "                all_results = semantic_results + exact_matches\n",
    "            else:\n",
    "                all_results = semantic_results\n",
    "            \n",
    "            # Step 3: Analyze context\n",
    "            context_data = self._analyze_context(all_results[:3])\n",
    "            \n",
    "            # Step 4: Generate enhanced response\n",
    "            response = self._generate_response(query, all_results, context_data)\n",
    "            \n",
    "            # Store in memory for future context\n",
    "            self.memory.save_context(\n",
    "                {\"input\": query},\n",
    "                {\"output\": response['answer']}\n",
    "            )\n",
    "            \n",
    "            # Print formatted response for immediate feedback\n",
    "            print(\"\\nVASTAUS:\")\n",
    "            print(\"-\" * 40)\n",
    "            print(response['answer'])\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {str(e)}\")\n",
    "            raise\n",
    "def run_neo4j_enhanced_tests():\n",
    "    try:\n",
    "        print(\"\\nStarting Neo4j Enhanced RAG test execution...\")\n",
    "        \n",
    "        # Initialize base pipeline\n",
    "        print(\"\\n1. Initializing base pipeline...\")\n",
    "        base_pipeline = RAGPipeline()\n",
    "        print(\"   ✓ Base pipeline initialized\")\n",
    "        \n",
    "        # Initialize Neo4j enhanced pipeline\n",
    "        print(\"\\n2. Creating Neo4j Enhanced Pipeline...\")\n",
    "        neo4j_rag = Neo4jEnhancedRAGPipeline(\n",
    "            base_rag_pipeline=base_pipeline,\n",
    "            neo4j_uri=\"neo4j://neo4j:7687\"  # Update with your Neo4j URI\n",
    "        )\n",
    "        print(\"   ✓ Neo4j Enhanced pipeline created\")\n",
    "        \n",
    "        # Process documents\n",
    "        print(\"\\n3. Processing documents...\")\n",
    "        folder_path = '/home/jovyan/work/notebooks/data/'  # Update with your actual path\n",
    "        neo4j_rag.process_documents(folder_path)\n",
    "        print(\"   ✓ Documents processed with Neo4j enhancement\")\n",
    "        \n",
    "        # Define test questions\n",
    "        test_questions = [\n",
    "            \"Onko Marjatta Eilan ystävä?\",\n",
    "            \"Miten Sulo kokee sosiaalisen kanssakäymisen merkityksen?\",\n",
    "            \"Montako sisarusta Sulolla on?\",\n",
    "            \"Millainen on Eilan arki?\",\n",
    "            \"Mikä on Sulolle tärkeää?\"\n",
    "        ]\n",
    "        \n",
    "        # Run tests\n",
    "        print(\"\\n4. Running test questions with Neo4j enhancement...\")\n",
    "        results = []\n",
    "        for i, question in enumerate(test_questions, 1):\n",
    "            try:\n",
    "                print(f\"\\nKysymys {i}/{len(test_questions)}:\")\n",
    "                print(\"-\"*80)\n",
    "                print(f\"{question}\")\n",
    "                print(\"-\"*80)\n",
    "                \n",
    "                # Get results from Neo4j enhanced pipeline\n",
    "                response = neo4j_rag.query(question)\n",
    "                results.append({\n",
    "                    \"question\": question,\n",
    "                    \"response\": response\n",
    "                })\n",
    "                \n",
    "                # Print results\n",
    "                print(\"\\nNeo4j Enhanced Vastaus:\")\n",
    "                print(response.get('answer', 'Ei vastausta'))\n",
    "                print(\"\\nLähteet ja konteksti:\")\n",
    "                print(\"- Dokumenttilähteet:\")\n",
    "                for src in response.get('sources', [])[:2]:\n",
    "                    print(f\"  • {src['document_id']}: {src['text'][:100]}...\")\n",
    "                print(\"- Graafikonteksti:\")\n",
    "                if response.get('person_context'):\n",
    "                    print(f\"  • Henkilö: {response['person_context']['name']}\")\n",
    "                    print(f\"  • Suhteet: {', '.join(response['person_context'].get('relationship_types', []))}\")\n",
    "                print(\"-\"*80)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Virhe kysymyksen käsittelyssä: {str(e)}\")\n",
    "        \n",
    "        print(\"\\n5. Neo4j Enhanced testaus valmis\")\n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nVirhe Neo4j testauksessa: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Add this after your existing test runs\n",
    "if __name__ == \"__main__\":\n",
    "    # Run both standard and Neo4j enhanced tests for comparison\n",
    "    print(\"\\nRunning standard RAG tests...\")\n",
    "    standard_results = run_improved_rag_tests()\n",
    "    \n",
    "    print(\"\\nRunning Neo4j enhanced RAG tests...\")\n",
    "    neo4j_results = run_neo4j_enhanced_tests()\n",
    "    \n",
    "    # Compare results\n",
    "    print(\"\\nComparison of results:\")\n",
    "    for std, neo in zip(standard_results, neo4j_results):\n",
    "        print(\"\\nQuestion:\", std[\"question\"])\n",
    "        print(\"\\nStandard RAG Answer:\")\n",
    "        print(std[\"response\"][\"answer\"])\n",
    "        print(\"\\nNeo4j Enhanced Answer:\")\n",
    "        print(neo[\"response\"][\"answer\"])\n",
    "        print(\"-\"*80)\n",
    "    \n",
    "def run_improved_rag_tests():\n",
    "    try:\n",
    "        print(\"\\nStarting RAG test execution...\")\n",
    "        \n",
    "        # Initialize base pipeline\n",
    "        print(\"\\n1. Initializing base pipeline...\")\n",
    "        base_pipeline = RAGPipeline()\n",
    "        print(\"   ✓ Base pipeline initialized\")\n",
    "        \n",
    "        # Initialize Finnish RAG Agent\n",
    "        print(\"\\n2. Creating Finnish RAG Agent...\")\n",
    "        agent = FinnishRAGAgent(base_pipeline)\n",
    "        print(\"   ✓ Agent created\")\n",
    "        \n",
    "        # Process documents\n",
    "        print(\"\\n3. Processing documents...\")\n",
    "        folder_path = '/home/jovyan/work/notebooks/data/'\n",
    "        base_pipeline.process_documents(folder_path)\n",
    "        print(\"   ✓ Documents processed\")\n",
    "        \n",
    "        # Define test questions\n",
    "        test_questions = [\n",
    "            \"Kuka on tärkeitä henkilöitä Eilalle?\",\n",
    "            \"Kuinka vanha on Sulo ja mikä on hänen syntymävuotensa?\",\n",
    "            \"Mitä Eila harrastaa?\",\n",
    "            \"Miten Sulo liikkuu?\",\n",
    "            \"Mistä asioista Eila pitää?\"\n",
    "        ]\n",
    "        \n",
    "        # Run tests through the agent\n",
    "        print(\"\\n4. Running test questions...\")\n",
    "        results = []\n",
    "        for i, question in enumerate(test_questions, 1):\n",
    "            try:\n",
    "                print(f\"\\nKysymys {i}/{len(test_questions)}:\")\n",
    "                print(\"-\"*80)\n",
    "                print(f\"{question}\")\n",
    "                print(\"-\"*80)\n",
    "                \n",
    "                response = agent.process_query(question)\n",
    "                results.append({\n",
    "                    \"question\": question,\n",
    "                    \"response\": response\n",
    "                })\n",
    "                \n",
    "                print(\"\\nVastaus:\")\n",
    "                print(response.get('answer', 'Ei vastausta'))\n",
    "                print(\"\\nLähde:\")\n",
    "                for src in response.get('sources', [])[:1]:\n",
    "                    print(f\"- {src['source']}: {src['text'][:150]}...\")\n",
    "                print(\"-\"*80)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Virhe kysymyksen käsittelyssä: {str(e)}\")\n",
    "        \n",
    "        print(\"\\n5. Testaus valmis\")\n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nVirhe testauksessa: {str(e)}\")\n",
    "        raise\n",
    "## call the function so that it runs\n",
    "run_improved_rag_tests()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5b2e2d-68c7-45e1-8f8a-3631978d68f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
