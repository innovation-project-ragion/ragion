{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0365f501-ae24-4ac1-916e-d84f5e9a90bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "import logging\n",
    "from typing import List, Dict, Any\n",
    "from docx import Document\n",
    "from tqdm.notebook import tqdm\n",
    "from pymilvus import connections, Collection, utility, CollectionSchema, FieldSchema, DataType\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def print_status(section_name: str, status: bool, message: str = \"\"):\n",
    "    \"\"\"Print status of a section with colored output.\"\"\"\n",
    "    status_str = \"✅ SUCCESS\" if status else \"❌ FAILED\"\n",
    "    print(f\"\\n{status_str} | {section_name}\")\n",
    "    if message:\n",
    "        print(f\"  └─ {message}\")\n",
    "\n",
    "def check_cuda():\n",
    "    \"\"\"Check CUDA availability and print status.\"\"\"\n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            device_name = torch.cuda.get_device_name(0)\n",
    "            print_status(\"CUDA Check\", True, f\"Using GPU: {device_name}\")\n",
    "            return True\n",
    "        else:\n",
    "            print_status(\"CUDA Check\", True, \"Using CPU\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print_status(\"CUDA Check\", False, str(e))\n",
    "        return False\n",
    "\n",
    "def ensure_stopwords_downloaded(language='finnish'):\n",
    "    \"\"\"Download NLTK stopwords and print status.\"\"\"\n",
    "    try:\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "        print_status(\"NLTK Setup\", True, f\"Downloaded {language} stopwords\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print_status(\"NLTK Setup\", False, str(e))\n",
    "        return False\n",
    "\n",
    "# Milvus Connection Settings\n",
    "MILVUS_HOST = \"localhost\"\n",
    "MILVUS_PORT = \"19530\"\n",
    "MILVUS_ALIAS = \"default\"\n",
    "EMBEDDING_DIM = 384\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, chunk_size=400, chunk_overlap=50):\n",
    "        try:\n",
    "            self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "                separators=[\"\\n\\n\", \"\\n\", \". \", \", \", \" \"],\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                length_function=len,\n",
    "                keep_separator=True,\n",
    "                add_start_index=True\n",
    "            )\n",
    "            print_status(\"Document Processor\", True, \"Initialized successfully\")\n",
    "        except Exception as e:\n",
    "            print_status(\"Document Processor\", False, str(e))\n",
    "            raise\n",
    "        \n",
    "    def extract_metadata_from_filename(self, filename: str) -> tuple:\n",
    "        \"\"\"Extract metadata from filename.\"\"\"\n",
    "        title = os.path.splitext(filename)[0]\n",
    "        match = re.match(r'([A-Za-z]+)\\s+(\\d{1,3})v\\s+([A-Za-z0-9\\-]+)', title)\n",
    "        if match:\n",
    "            return match.group(1), int(match.group(2)), match.group(3)\n",
    "        return None, None, None\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize Finnish text.\"\"\"\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'[^\\w\\s\\.\\,\\?\\!\\-\\:\\;äöåÄÖÅ]', '', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def process_document(self, file_path: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process a single document and return chunks with metadata.\"\"\"\n",
    "        try:\n",
    "            # Read document\n",
    "            doc = Document(file_path)\n",
    "            text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "            \n",
    "            # Extract metadata\n",
    "            filename = os.path.basename(file_path)\n",
    "            name, age, doc_id = self.extract_metadata_from_filename(filename)\n",
    "            \n",
    "            # Preprocess and split text\n",
    "            clean_text = self.preprocess_text(text)\n",
    "            chunks = self.text_splitter.split_text(clean_text)\n",
    "            \n",
    "            # Create chunks with metadata\n",
    "            processed_chunks = []\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                processed_chunks.append({\n",
    "                    \"text\": chunk,\n",
    "                    \"metadata\": {\n",
    "                        \"source\": filename,\n",
    "                        \"person_name\": name,\n",
    "                        \"person_age\": age,\n",
    "                        \"document_id\": doc_id,\n",
    "                        \"chunk_index\": i\n",
    "                    }\n",
    "                })\n",
    "            \n",
    "            print_status(\"Document Processing\", True, f\"Processed {filename}\")\n",
    "            return processed_chunks\n",
    "        except Exception as e:\n",
    "            print_status(\"Document Processing\", False, f\"Error processing {file_path}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "class MilvusManager:\n",
    "    def __init__(self, host: str, port: str, alias: str = \"default\"):\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.alias = alias\n",
    "        self.connected = False\n",
    "        self.connect()\n",
    "        \n",
    "    def connect(self):\n",
    "        \"\"\"Establish connection to Milvus.\"\"\"\n",
    "        try:\n",
    "            connections.connect(\n",
    "                alias=self.alias,\n",
    "                host=self.host,\n",
    "                port=self.port\n",
    "            )\n",
    "            self.connected = True\n",
    "            print_status(\"Milvus Connection\", True, f\"Connected to {self.host}:{self.port}\")\n",
    "        except Exception as e:\n",
    "            print_status(\"Milvus Connection\", False, str(e))\n",
    "            raise\n",
    "            \n",
    "    def create_collection(self, collection_name: str = \"document_embeddings\"):\n",
    "        \"\"\"Create Milvus collection with appropriate schema.\"\"\"\n",
    "        try:\n",
    "            if utility.has_collection(collection_name):\n",
    "                collection = Collection(name=collection_name)\n",
    "                print_status(\"Milvus Collection\", True, f\"Using existing collection: {collection_name}\")\n",
    "                return collection\n",
    "                \n",
    "            fields = [\n",
    "                FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "                FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "                FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=EMBEDDING_DIM),\n",
    "                FieldSchema(name=\"person_name\", dtype=DataType.VARCHAR, max_length=100),\n",
    "                FieldSchema(name=\"person_age\", dtype=DataType.INT64),\n",
    "                FieldSchema(name=\"document_id\", dtype=DataType.VARCHAR, max_length=100),\n",
    "                FieldSchema(name=\"chunk_index\", dtype=DataType.INT64)\n",
    "            ]\n",
    "            \n",
    "            schema = CollectionSchema(fields=fields, description=\"Document embeddings collection\")\n",
    "            collection = Collection(name=collection_name, schema=schema)\n",
    "            \n",
    "            # Create IVF_FLAT index\n",
    "            index_params = {\n",
    "                \"metric_type\": \"IP\",\n",
    "                \"index_type\": \"IVF_FLAT\",\n",
    "                \"params\": {\"nlist\": 1024}\n",
    "            }\n",
    "            collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "            print_status(\"Milvus Collection\", True, f\"Created new collection: {collection_name}\")\n",
    "            return collection\n",
    "        except Exception as e:\n",
    "            print_status(\"Milvus Collection\", False, str(e))\n",
    "            raise\n",
    "\n",
    "class EmbeddingGenerator:\n",
    "    def __init__(self, model_name: str = \"TurkuNLP/bert-base-finnish-cased-v1\"):\n",
    "        try:\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "            print_status(\"Embedding Model\", True, f\"Loaded {model_name}\")\n",
    "        except Exception as e:\n",
    "            print_status(\"Embedding Model\", False, str(e))\n",
    "            raise\n",
    "        \n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        \n",
    "    def generate(self, texts: List[str], batch_size: int = 32) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings for a list of texts.\"\"\"\n",
    "        try:\n",
    "            all_embeddings = []\n",
    "            \n",
    "            # Process in batches\n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                batch_texts = texts[i:i + batch_size]\n",
    "                \n",
    "                # Tokenize texts\n",
    "                encoded_input = self.tokenizer(\n",
    "                    batch_texts,\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=512,\n",
    "                    return_tensors='pt'\n",
    "                ).to(self.device)\n",
    "                \n",
    "                # Compute token embeddings\n",
    "                with torch.no_grad():\n",
    "                    model_output = self.model(**encoded_input)\n",
    "                \n",
    "                # Perform pooling\n",
    "                sentence_embeddings = self.mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "                \n",
    "                # Normalize embeddings\n",
    "                sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "                \n",
    "                all_embeddings.append(sentence_embeddings.cpu().numpy())\n",
    "            \n",
    "            result = np.concatenate(all_embeddings)\n",
    "            print_status(\"Embedding Generation\", True, f\"Generated {len(texts)} embeddings\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print_status(\"Embedding Generation\", False, str(e))\n",
    "            raise\n",
    "\n",
    "class RAGPipeline:\n",
    "    def __init__(self, model_id: str = \"Finnish-NLP/llama-7b-finnish-instruct-v0.2\"):\n",
    "        try:\n",
    "            self.setup_llm(model_id)\n",
    "            print_status(\"LLM Setup\", True, f\"Loaded {model_id}\")\n",
    "            \n",
    "            self.doc_processor = DocumentProcessor()\n",
    "            self.embedding_generator = EmbeddingGenerator()\n",
    "            self.milvus_manager = MilvusManager(\n",
    "                host=MILVUS_HOST,\n",
    "                port=MILVUS_PORT,\n",
    "                alias=MILVUS_ALIAS\n",
    "            )\n",
    "            self.collection = self.milvus_manager.create_collection()\n",
    "            print_status(\"RAG Pipeline\", True, \"All components initialized\")\n",
    "        except Exception as e:\n",
    "            print_status(\"RAG Pipeline\", False, str(e))\n",
    "            raise\n",
    "        \n",
    "    def setup_llm(self, model_id: str):\n",
    "        \"\"\"Initialize the LLM with optimized settings.\"\"\"\n",
    "        try:\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\"\n",
    "            )\n",
    "            \n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                quantization_config=bnb_config,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                max_memory={0: \"6GiB\"},\n",
    "                offload_folder=\"offload\"\n",
    "            )\n",
    "            \n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "            self.pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                max_new_tokens=512,\n",
    "                do_sample=True,\n",
    "                temperature=0.3,\n",
    "                top_p=0.95,\n",
    "                repetition_penalty=1.15\n",
    "            )\n",
    "            print_status(\"LLM Pipeline\", True, \"Pipeline configured successfully\")\n",
    "        except Exception as e:\n",
    "            print_status(\"LLM Pipeline\", False, str(e))\n",
    "            raise\n",
    "        \n",
    "    def process_documents(self, folder_path: str):\n",
    "        \"\"\"Process all documents in the specified folder.\"\"\"\n",
    "        try:\n",
    "            file_paths = [f for f in os.listdir(folder_path) if f.endswith('.docx')]\n",
    "            all_chunks = []\n",
    "            \n",
    "            for file in tqdm(file_paths, desc=\"Processing documents\"):\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                chunks = self.doc_processor.process_document(file_path)\n",
    "                all_chunks.extend(chunks)\n",
    "                \n",
    "            # Generate embeddings\n",
    "            texts = [chunk[\"text\"] for chunk in all_chunks]\n",
    "            embeddings = self.embedding_generator.generate(texts)\n",
    "            \n",
    "            # Insert into Milvus\n",
    "            data = {\n",
    "                \"text\": texts,\n",
    "                \"embedding\": embeddings.tolist(),\n",
    "                \"person_name\": [chunk[\"metadata\"][\"person_name\"] for chunk in all_chunks],\n",
    "                \"person_age\": [chunk[\"metadata\"][\"person_age\"] for chunk in all_chunks],\n",
    "                \"document_id\": [chunk[\"metadata\"][\"document_id\"] for chunk in all_chunks],\n",
    "                \"chunk_index\": [chunk[\"metadata\"][\"chunk_index\"] for chunk in all_chunks]\n",
    "            }\n",
    "            \n",
    "            self.collection.insert(data)\n",
    "            self.collection.flush()\n",
    "            print_status(\"Document Processing\", True, f\"Inserted {len(texts)} chunks into Milvus\")\n",
    "        except Exception as e:\n",
    "            print_status(\"Document Processing\", False, str(e))\n",
    "            raise\n",
    "        \n",
    "    def query(self, question: str, top_k: int = 3):\n",
    "        \"\"\"Query the system with a question.\"\"\"\n",
    "        try:\n",
    "            # Generate question embedding\n",
    "            question_embedding = self.embedding_generator.generate([question])[0]\n",
    "            \n",
    "            # Search in Milvus\n",
    "            search_params = {\"metric_type\": \"IP\", \"params\": {\"nprobe\": 10}}\n",
    "            results = self.collection.search(\n",
    "                data=[question_embedding.tolist()],\n",
    "                anns_field=\"embedding\",\n",
    "                param=search_params,\n",
    "                limit=top_k,\n",
    "                output_fields=[\"text\", \"person_name\", \"document_id\"]\n",
    "            )\n",
    "            \n",
    "            # Format context\n",
    "            context = \"\\n\".join([\n",
    "                f\"Dokumentti {i+1}:\\n{hit.entity.get('text')}\\n\"\n",
    "                for i, hit in enumerate(results[0])\n",
    "            ])\n",
    "            \n",
    "            # Generate answer\n",
    "            prompt = f\"\"\"Käytä seuraavaa kontekstia vastataksesi kysymykseen.\n",
    "            Vastaa vain kysyttyyn kysymykseen ja käytä vain annettua kontekstia.\n",
    "            Jos et löydä vastausta kontekstista, kerro se rehellisesti.\n",
    "\n",
    "            Konteksti:\n",
    "            {context}\n",
    "\n",
    "            Kysymys: {question}\n",
    "\n",
    "            Vastaus:\"\"\"\n",
    "            \n",
    "            response = self.pipeline(prompt)[0][\"generated_text\"]\n",
    "            answer = response.split(\"Vastaus:\")[-1].strip()\n",
    "            \n",
    "            print_status(\"Query\", True, \"Generated response successfully\")\n",
    "            return {\n",
    "                \"answer\": answer,\n",
    "                \"sources\": [\n",
    "                    {\n",
    "                        \"text\": hit.entity.get('text'),\n",
    "                        \"person_name\": hit.entity.get('person_name'),\n",
    "                        \"document_id\": hit.entity.get('document_id')\n",
    "                    }\n",
    "                    for hit in results[0]\n",
    "                ]\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print_status(\"Query\", False, str(e))\n",
    "            raise\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Check CUDA\n",
    "        check_cuda()\n",
    "        \n",
    "        # Download stopwords\n",
    "        ensure_stopwords_downloaded()\n",
    "        \n",
    "        # Initialize the RAG pipeline\n",
    "        rag = RAGPipeline()\n",
    "        \n",
    "        # Process documents\n",
    "        folder_path = '/home/jovyan/work/notebooks/data/'  # Update this path\n",
    "        print_status(\"Document Path\", True, f\"Using folder: {folder_path}\")\n",
    "        rag.process_documents(folder_path)\n",
    "        \n",
    "        # Example queries\n",
    "        questions = [\n",
    "            \"Onko Marjatta Eilan ystävä?\",\n",
    "            \"Miten Sulo kokee sosiaalisen kanssakäymisen merkityksen?\",\n",
    "            \"Montako sisarusta Sulolla on?\"\n",
    "        ]\n",
    "        \n",
    "        for i, question in enumerate(questions, 1):\n",
    "            print(f\"\\nProcessing Query {i}/{len(questions)}\")\n",
    "            try:\n",
    "                result = rag.query(question)\n",
    "                print_status(f\"Query {i}\", True, f\"Question: {question}\")\n",
    "                print(f\"Answer: {result['answer']}\")\n",
    "                print(\"\\nSources:\")\n",
    "                for source in result['sources']:\n",
    "                    print(f\"- {source['document_id']}: {source['text'][:100]}...\")\n",
    "            except Exception as e:\n",
    "                print_status(f\"Query {i}\", False, f\"Failed to process question: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        print_status(\"Main Execution\", True, \"All operations completed successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print_status(\"Main Execution\", False, str(e))\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5b2e2d-68c7-45e1-8f8a-3631978d68f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
