import requests
import json
import logging
from src.config import config

logger = logging.getLogger(__name__)

OLLAMA_URL = "http://ollama:11434/api/generate"  # Update if using a different URL

def query_ollama(prompt, model="mistral:7b"):
    """
    Send a prompt to the Ollama LLM and return the generated response.
    
    Args:
        prompt (str): The prompt/question to ask the LLM.
        model (str): The model to use for generating responses (default: mistral:7b).
    
    Returns:
        str: The response generated by the LLM.
    """
    headers = {
        "Content-Type": "application/json"
    }
    data = {
        "model": model,
        "prompt": prompt
    }

    try:
        response = requests.post(OLLAMA_URL, headers=headers, json=data, stream=True)
        if response.status_code == 200:
            result_text = ""
            for chunk in response.iter_lines(decode_unicode=True):
                if chunk:
                    try:
                        chunk_data = json.loads(chunk)
                        result_text += chunk_data.get("response", "")
                    except json.JSONDecodeError:
                        logger.error(f"Failed to parse chunk: {chunk}")
            return result_text
        else:
            logger.error(f"Error querying Ollama: {response.status_code}, {response.text}")
            return None
    except Exception as e:
        logger.error(f"Failed to connect to Ollama: {str(e)}")
        return None
    
def query_ollama_with_context(query, context):
    """
    Create a prompt with the retrieved context and ask the Ollama LLM the final question.
    
    Args:
        query (str): The user's question.
        context (str): The relevant text/context retrieved from Milvus.
    
    Returns:
        str: The response from the LLM.
    """
    prompt_template = """
    Sinulle on annettu seuraava teksti:

    {context}

    Vastaa seuraavaan kysymykseen perustuen yll√§ olevaan tekstiin:

    {question}
    """
    
    prompt = prompt_template.format(context=context, question=query)
    logger.info(f"Prompt sent to LLM: {prompt}")
    
    # Query Ollama and return the response
    response = query_ollama(prompt)
    
    if response:
        logger.info(f"Ollama response for '{query}' received successfully.")
        return response
    else:
        logger.warning(f"Failed to get response from Ollama for '{query}', returning fallback.")
        return "I don't know, the information is not provided in the documents."